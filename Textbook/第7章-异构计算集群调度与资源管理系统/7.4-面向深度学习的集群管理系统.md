<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 7.4 面向深度学习的集群管理系统

在之前的章节，我们已经介绍经典的调度算法在深度学习作业的集群调度中的原理和应用。但是这些算法本身没有深入的考虑深度学习作业自身的特点，和更加深入的考虑GPU服务器中GPU的拓扑结构。本章将围绕经典且前沿的针对深度学习负载和GPU服务器特点而设计的平台调度算法进行介绍，期望让读者了解新负载和硬件的特点和调度管理需求，并通过更加适合的方式予以解决，并启发新的工作。本章节的很多工作目前没有严格定论，还属于研究的前沿，本章中更多的是总结当前负载，硬件和平台本身的新问题，启发读者未来可以思考新的研究和工程工作。

- [7.4 面向深度学习的集群管理系统](#74-面向深度学习的集群管理系统)
  - [7.4.1 深度学习工作负载的需求](#741-深度学习工作负载的需求)
      - [思考题](#思考题)
  - [7.4.2 异构硬件的多样性](#742-异构硬件的多样性)
  - [7.4.3 深度学习平台的管理与运维需求](#743-深度学习平台的管理与运维需求)
    - [思考与实验设计](#思考与实验设计)
  - [7.4.4 深度学习负载与异构硬件下的调度设计](#744-深度学习负载与异构硬件下的调度设计)
        - [经典系统设计回顾](#经典系统设计回顾)
        - [经典系统设计回顾](#经典系统设计回顾-1)
        - [知识点总结与展望](#知识点总结与展望)
  - [7.4.5 代表性异构集群管理系统](#745-代表性异构集群管理系统)
  - [参考文献](#参考文献)

## 7.4.1 深度学习工作负载的需求

深度学习作业相比传统的大数据作业有一些新的特点和不同。
我们使用的集群管理系统大多数支持的作业是深度学习的训练作业。

- 批量深度学习训练作业特点：自动化机器学习场景下，用户会一次性提交大量的深度学习作业。自动机器学习训练作业的一个关键特征是反馈驱动探索。由于深度学习实验固有的反复试验方法，用户通常会尝试多个作业配置（多项工作），并使用这些工作的早期反馈来
决定是否优先考虑或杀死其中的某些子集。这种有条件的探索，称为超参数搜索，可以是手动的，也可以是系统自动触发。所以我们经常可以看到集群中有大量类似作业和被提前终止的作业。
- 单个深度学习训练作业特点：
  - 训练时间持续数小时甚至几天。
  - 作业主干部分是迭代的计算，每轮迭代可以切分为小时间窗口的任务。这样让作业本身有机会以更小的粒度触发调度，抢占策略。 
  - 在训练过程中不同的时间点做检查点有不同的数据量，这给做检查点提供了优化机会，如果有检查点的支持，可以让平台或框架本身支持更加激进的调度策略，例如，动态迁移作业，装箱作业到同一块GPU，时分复用等。
  - 资源消耗可预测性，可以通过运行时监控获取。由于其可预测性，让调度器有机会可以根据资源消耗做更加优化的作业放置和装箱。
- 分布式深度学习训练作业特点：
  - 对GPU拓扑结构敏感，例如：数据并行策略通信传递梯度，模型并行策略通信传递中间结果张量，GPU与GPU之间传输带宽容易形成瓶颈。所以考虑GPU亲和性的合并防止策略对降低分布式训练作业的完工时间有帮助。但会引发新的问题，由于调度是一批GPU，容易造成亲和性约束不满足，产生资源碎片。
  
根据深度学习作业特点和软件栈和硬件栈的底层机制支持，可以框架或平台可以协同设计启发的优化策略，提升资源利用率等指标。

#### 思考题

总结思考深度学习作业和传统操作系统作业以及大数据平台作业的异同点？

## 7.4.2 异构硬件的多样性

深度学习作业使用的服务器一般会挂载多块GPU，而且训练时主要的计算单元是GPU。相比传统的大数据作业和服务器硬件有一些新的特点和不同。多GPU集群运行深度学习问题与挑战：由于多块GPU之间的互联方式多样，造成作业的不同放置方式受到GPU拓扑结构影响，进而影响总线容易产生争用，影响性能。同时，作业本身由于可能共享服务器，数据总线等资源也受到服务器上同时运行作业的干扰。

所以针对硬件特点可以设计启发优化策略：考虑集群和服务器节点的GPU拓扑结构的亲和性(affinity)调度。这点和传统NUMA架构中考虑[处理器亲和性(Processor affinity)](https://en.wikipedia.org/wiki/Processor_affinity)的问题与优化有异曲同工之处。我们可以看到，对系统问题，我们可以从传统的操作系统的经典设计中找到设计原则，对新工作形成指导和借鉴。

## 7.4.3 深度学习平台的管理与运维需求

深度学习平台对上管理深度学习模型训练负载，对下管理以GPU，InfiniBand为代表的异构硬件，平台管理与运维遇到了相比之前大数据平台等平台新挑战。

平台管理者相比数据科学家等使用平台的用户，相比单纯考虑作业执行速度等性能指标，更加关注以下更加全面的设计目标：

- 稳定性
  - 由于深度学习框架的设计者在初始没有像大数据社区一样把容错当成第一要义，框架提供的容错机制过于简单，同时没有自动备份与恢复的支持，造成在之后的设计版本中才有弹性等功能的支持。这样对底层平台来说造成比较大的运维负担。
  - 由于节点上的异构硬件也有一定概率产生硬件问题（例如，[GPU故障](https://ieeexplore.ieee.org/abstract/document/7056044)），造成平台稳定性另一方面的挑战。如何高效，敏捷的发现和修复故障，是平台方需要不断沉淀的，这需要系统化的系统设计，开发流程设计与管理策略设计共同作用。
- 效率
  - GPU集群价格昂贵，更新换代频繁，如何规划好集群，提升投入产出比，如何在现有集群中，减少资源碎片，提升利用率也有很大的挑战。调度算法在一定程度上能优化和提升集群的资源利用率。
- 公平性
  - 目前使用深度学习平台的用户在训练生产环境的同时，也有一些是研究投稿赶论文截止的需求，造成相比传统批处理调度场景，用户有了类似特定时段的峰值资源使用需求。平台需要保证各组资源使用的公平性，同时提前规划好用户的资源使用同时兼顾峰值利用需求，需要管理员设计好相应的策略。
- 可维护性
  - 平台团队同时在开发和运维平台，可维护性也是平时减少运维负担的一个重要考虑的因素。通过微服务等手段(回顾操作系统[微内核](https://en.wikipedia.org/wiki/Microkernel)的设计思想)将功能模块尽可能的拆分，能够让故障的定位与修复最小化，同时良好的DevOps流程搭建，敏捷的开发与项目管理也为平台的可维护性提升起到关键的作用。
- 用户体验
  - 用户体验良好并统一的作业提交，作业管理与调试工具，能大幅提升用户的开发生产力，同时也能减轻平台运维工程师的负担。

除了以上指标，平台也会关注性能（吞吐，完工时间等）指标。综上所述我们看到，平台本身模块众多，涉及的外部交互的软硬件多样，使用和维护的用户也很多，所以其面对的问题场景较为复杂，作为平台设计者和使用者需要通盘考量，性能只其中一个环节，我们还要以系统化的视角去设计和管理整个异构资源，为上层应用负载与用户提供更加透明与便捷的用户体验。

### 思考与实验设计

思考，如果需要兼顾以上指标，新一代深度学习调度与平台设计应该朝着哪个方向设计与发展？

请读者设计算法或策略，保证公平性的同时，最大限度提升集群效率，可以在深度学习调度模拟器中进行实验设计与算法验证。

## 7.4.4 深度学习负载与异构硬件下的调度设计

接下来，我们将介绍不同侧重点，考虑不同设计目标的调度器设计，这些调度器由于设计目标不同，基于能获取信息的假设也不同，进而造成实现和对作业的入侵性也不同，读者在选用和设计调度器时，需要考虑不同算法的优劣势根据需求酌情选择。

- ***利用领域知识(深度学习作业特点)和运行时数据驱动，提升深度学习作业延迟和效率的框架与平台协同设计的调度器设计***
  
[Gandiva](https://dl.acm.org/doi/10.5555/3291168.3291212)根据深度学习作业特点（早反馈，分布式作业对拓扑敏感等），以及硬件和操作系统对GPU计算的进程上下文切换的不足进行弥补，通过协同设计进而提升整体集群资源利用率。

Gandiva设计了两种模式：

一种是反应模式(reactive Mode)：

类似传统调度器事件驱动的设计，根据不同事件和状态（作业到达(arrivals), 离开(departures), 失效(failures)）触发调度策略。

<center> <img src="./img/4/affinity.png" ch="500" width="700" height="160" /></center>
<center>图7-4-1. 分布式作业调度受局部性影响 (<a href="https://www.usenix.org/sites/default/files/conference/protected-files/osdi20_slides_zhao.pdf">图片来源</a>)</center>

通过图中可以看到，将同样需要2块GPU卡的作业分别调度在，相同PCI-e交换机(switch)，跨交换机和跨节点下进行部署运行，会产生40%~5x的降速。所以对于多卡作业，考虑部署的局部性，通过亲和性调度，可以让作业执行更快，让出更多的资源执行其他作业，进而对整体完工时间受益，进而提升资源利用率。

考虑亲和性(affinity)的调度策略：在调度过程中按以下优先级考虑和排序节点进行作业分配，这样能够更多的考虑GPU的亲和性和拓扑结构，让深度学习作业减少数据I/O的瓶颈，提升性能。
其优先考虑的节点优先级为。
1. 拥有相同亲和性的节点。
2. 还未标注亲和性的节点。
3. 有不同亲和性的节点。
4. 进行超额订阅(Oversubscription)，在有相同亲和性的节点暂停和恢复其他作业。
5. 不满足之前条件，则作业排队等待。

另一种是内省模式(introspective Mode)：

在作业执行后，持续监控并定期优化当前作业的放置 (placement)，同时通过扩展框架支持细粒度的检查点和恢复功能，对后续策略提供基础原语的支持。通过不断监控作业利用率和节点资源利用率，不断进行作业的，装箱(bin packing)，迁移(migration)，增长收缩(grow-shrink)，超额定于和时间切片(time slicing)，进而提升整体资源利用率，降低作业的完工时间(makespan)。
这些底层机制有以下好处：
- 装箱 (bin packing)：在保证GPU显存约束的情况下，根据浮点运算量，将更多的作业装箱到相同GPU，提升资源利用率。
- 时分复用(time slicing):利用框架层或底层实现的检查点和恢复机制，多个作业可以通过时分复用，共享单块GPU。
- 迁移 (migration)：利用框架层或底层实现的检查点和恢复机制，当有空闲资源或奖励资源，动态迁移作业使用免费资源，加速训练，当被抢占迁移作业保证作业之前训练不失效。
  
Gandiva的设计方法能大幅度降低作业执行延迟和提升资源利用率，但需要结合其他策略才能兼顾公平性等目标，同时需要框架和平台提供功能支持。

同时也存在很多其他挑战，例如：需要能够遥测深度学习作业负载信息，同时越来越多样的资源使用状况的深度学习负载来说挑战较大。

Gandiva本身需要框架或硬件提供检查点和实时迁移 (live migrtation)的支持，这点假设对现有平台挑战较大，同时需要框架和平台协同设计，对一些公司或平台两者为分离团队维护挑战较大。

同时拥有基于在线运行时性能信息反馈和框架平台协同设计的经典调度器还有：
- 基于在线(online)作业信息反馈，进行调度算法优化设计的调度器[Optimus](https://dl.acm.org/doi/10.1145/3190508.3190517)等。
- 需要框架和平台协同设计提供支持的调度器还有[AntMan](https://dl.acm.org/doi/abs/10.5555/3488766.3488796)等。

##### 经典系统设计回顾

同时我们也可以看到数据驱动(data-driven)的设计思路和检查点与迁移等经典机制再次在新的场景下的应用与创新，数据驱动的内省模式是经典的系统优化策略，和JIT（Just-In-Time）优化利用运行时信息进行优化有异曲同工之妙，同时检查点和实时迁移是操作系统中进程，线程上下文切换的经典设计，其在提升资源复用的设计目标上是基础机制。

- ***基于经典集群调度器并根据新负载特点（例如，GPU亲和性）进行策略扩展的调度器设计***

深度学习系统社区有很多有大数据系统和平台背景的工程师，同时深度学习训练负载从作业性质上可以归纳为[批处理作业(batch job)](https://research.google/pubs/pub43438/)，那么以往成熟的大数据批处理作业集群管理系统[YARN](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html)中的调度器，无疑在平台设计初期是一个较好的入手点和起点，因为起经过大规模的生产环境检验，提供容量调度，虚拟集群等机制的支持，社区活跃，应用范围广。所以有些工作([YARN-CS](https://dl.acm.org/doi/10.5555/3358807.3358888))和开源系统[OpenPAI](https://github.com/microsoft/pai)的初期采用从经典调度器的基础上进行扩展深度学习负载调度特点功能支持的思路进行设计。[YARN-CS](https://dl.acm.org/doi/10.5555/3358807.3358888)研究作业在不同GPU拓扑下的性能影响，进而通过分析作业调度日志，证明分布式作业尽可能考虑局部性(Locality)的调度到“离得近”且通信代价小的一批GPU节点上，能够降低作业的完工时间。

- ***考虑历史作业执行时间（历史数据驱动）和亲和性合并放置，降低完工时间和提升资源利用率的调度器设计***
  
调度器[Tiresias](https://dl.acm.org/doi/10.5555/3323234.3323274)中对集群历史作业的资源分配和完工时间日志跟踪(trace)进行统计分析，通过数据驱动的设计思路，预估作业执行时间，并利用这些信息指导调度，优先调度短时作业，降低作业的完工时间。并借鉴经典的基廷斯系数([Gittins index](https://en.wikipedia.org/wiki/Gittins_index))策略进行优化问题的抽象与策略设计，优先调度大量短作业，提升整体完工时间。另外同时对分布式深度学习作业进行基准测试，对亲和性敏感的负载进行考虑合并防止策略，提升利用率，同时也抛出亲和性影响排队时间的伏笔，为后续的研究工作铺垫新的挑战问题。

- ***多租环境下，降低由于亲和性调度造成排队延迟问题和资源分配碎片问题的调度器设计***


<center> <img src="./img/4/queuedelay.png" ch="500" width="600" height="200" /></center>
<center>图7-4-1. 排队延迟问题 (<a href="https://www.usenix.org/sites/default/files/conference/protected-files/osdi20_slides_zhao.pdf">图片来源</a>)</center>
如图所示，此图展示2个月的日志数据，2232个GPU的集群，11个租户的情况下的私有(Private)，共享多租(Shared）和HiveD优化后的情况下，作业的排队时间延迟问题，其中红色的线中我们观察到，平均有7倍的延迟由于多租环境又考虑局部性造成作业延迟调度。

[HiveD](https://dl.acm.org/doi/10.5555/3488766.3488795)提出如果假设调度深度学习作业同时考虑多租环境，不同租户(tenants)之间会配置不同的资源配额，以配额被分配相应的资源，但是如果考虑GPU亲和性的调度策略，会尽可能将亲和性高的资源整体分配给作业，这就会造成集群调度容易出现排队延迟较高的异常。HiveD通过设计多级细胞(Cell)结构，设计了伙伴细胞分配(buddy cell allocation)算法去保证在之前的约束下，资源能够高效分配，降低排队时间和资源碎片。同时HiveD能够和现有的调度器进行较好的集成。

HiveD本身的兼容性较好，能够与其他调度器有较好的集成。读者可以参考其文献[HiveD](https://dl.acm.org/doi/10.5555/3488766.3488795)，或使用和测试其已[开源的HiveD调度器](https://github.com/microsoft/hivedscheduler)。

HiveD需要假设平台已经提供多租虚拟集群机制，以及已经获取到GPU拓扑会考虑作业的GPU亲和性问题。


##### 经典系统设计回顾

同时我们也可以看到经典的系统算法再次在新的场景下的应用与创新，伙伴系统是资源分配算法中常常使用的机制用于减少碎片的发生，常常用于内存分配器或调度器中。

##### 知识点总结与展望

通过以上调度经典算法的脉络，我们可以看到，在深度学习集群调度问题中充满了不同的设计目标的权衡和需要满足的约束，之前我们已经总结过调度算法的设计目标，接下来我们可以总结调度问题设计过程中常常可以追加和需要满足的硬约束(hard constraint)和软约束(soft constraint)。

- 空闲资源约束：保证作业的GPU，GPU内存，CPU，主存等资源需要有空闲资源保证能分配给作业使用。此类约束是硬约束(hard constraint)，需要必须满足。
- 资源局部性(Locality)：GPU亲和性约束有助于降低分布式或多卡训练作业的通信开销，降低完工时间。此类约束一般可以设计为软约束(soft constraint)，需要必须满足。
- 配额(Quota)约束：虚拟集群等多租环境有严格的配额约束保证公平性。此类约束一般可以设计为硬约束(hard constraint)，需要必须满足。但是可以通过强占等底层机制的支持，适当放松。
- 最小资源保证约束：容量调度等场景有虚拟集群的最小资源佩配额约束保证公平性。此类约束一般可以设计为硬约束(hard constraint)，需要必须满足。
- 完工时间约束：例如，保证排队时间低于一定条件，对排队较久或执行时间更短的作业优先调度。此类约束一般可以认为是软约束(soft constraint)。

随着新技术的发展，还会有新的调度算法设计和产生，最新的深度学习集群调度算法研究工作，读者可以关注OSDI，SOSP，ATC，NSDI，SoCC等会议的最新进展。

总结起来，调度器的算法设计是基于不同可获取（例如，在线，离线获取）的作业（时间）和硬件（拓扑）上下文信息假设，最优化的优化目标（例如，完工时间，平均完工时间等），满足特定的约束（例如，局部性）的作业资源分配的优化问题。在历史的长河中，涌现出非常多的经典算法设计，同时针对深度学习平台，还会不断涌现新的算法设计，感兴趣的读者可以思考，应用和研究相关工作，为未来平台管理打下坚实基础。

## 7.4.5 代表性异构集群管理系统

本小节我们将介绍代表性的开源和企业内部的大规模异构集群管理系统。基于代表性开源系统，企业可以部署和构建自己的平台，提升资源管理能力，资源利用率和开发效率。基于内部大规模异构集群管理系统所发布的参考文献，企业可以较早的规划和采取最佳实践策略，将未来可预见的问题较早规避，并持续扩容。

- [OpenPAI](https://github.com/microsoft/pai)

<center> <img src="./img/4/pai.png" ch="500" width="500" height="600" /></center>
<center>图7-4-1. OpenPAI架构图 (<a href="https://github.com/microsoft/pai">图片来源</a>)</center>

OpenPAI是由微软亚洲研究院系统组和微软（亚洲）互联网工程院联合研发的，支持多种深度学习、机器学习及大数据任务，可提供大规模GPU集群调度、集群监控、任务监控、分布式存储等功能，且用户界面友好，易于操作。OpenPAI正在转向更健壮、更强大和更轻量级的架构。OpenPAI 也变得越来越模块化，以便平台可以轻松定制和扩展以满足新的需求。OpenPAI 还提供了许多 AI 用户友好的功能，使最终用户和管理员更容易完成日常的 AI 任务。OpenPAI通过统一的框架控制器(framework controller)，提供对各个深度学习或大数据框架的自动部署，监控，重试(retry)的支持，类似YARN一样提供统一的AppMaster。同时提供丰富的运行时信息监控，服务状态监控，调试(远程SSH)等功能支持，功能和组件较为丰富。同时还提供应用市场，插件化支持和扩展平台上可以运行的应用。


- [Kubeflow](https://www.kubeflow.org/)

Kubeflow是由Google开源的平台项目，该项目致力于使机器学习工作流在 Kubernetes上的部署变得简单、便携和可扩展。由于Kubeflow是Kubernetes社区原生支持，一经推出，社区发展就非常迅速。Kubeflow的设计目标不是重新创建其他服务，而是提供一种直接的方法，将用于机器学习和深度学习的同类最佳开源系统部署到不同的基础设施。无论用户在何处运行Kubernetes，都可以运行Kubeflow。 Kubeflow通过定制化各个框架的Operator，提供对各个深度学习或大数据框架的自动部署，监控，重试(retry)的支持，但是区别是，没有像YARN一样提供统一的AppMaster，需要各个框架依赖社区构建，为之后的维护和功能支持会增加额外负担。

- [Philly](https://dl.acm.org/doi/10.5555/3358807.3358888)

Philly是微软内部使用的大规模AI训练平台，Philly 旨在支持执行有监督式机器学习的训练工作负载。这包括培训来自开发产品的生产团队的工作，这些产品使用用于图像分类、语音识别等的模型。有相关研究工作对Philly上的[资源分配，深度学习作业性能特点](https://dl.acm.org/doi/10.5555/3358807.3358888)和[深度学习程序Bug](https://dl.acm.org/doi/10.1145/3377811.3380362)进行研究，从中我们可以观察和了解大规模生产环境中作业资源争用，资源利用率，调度策略设计和程序Bug等问题及启发相关新的研究工作。  

- [Singularity](https://arxiv.org/abs/2202.07848)

Singularity是微软Azure提供的大规模云端AI训练平台服务，Singularity旨在支持调度并执行跨数据中心，抢占式和弹性调度的深度学习作业。Singularity的核心是一种工作负载感知的调度程序，它可以透明地抢占和弹性扩展深度学习工作负载，以在不影响其正确性或性能的情况下在全球范围内的AI加速器（例如 GPU、FPGA）中提高利用率。默认情况下，Singularity 中的所有作业都是可抢占、可迁移和动态调整大小（弹性）。作业可以动态且透明地 (a) 抢占并迁移到一组不同的节点、集群、数据中心或区域，并准确地从执行被抢占的点，以及 (b) 在给定类型的一组不同的加速器上调整大小（即弹性地放大/缩小）。Singlularity通过底层实现设备代理(device proxy)机制，通过在驱动层实现透明的检查点（checkpointing）功能支持，进而支持作业的抢占，迁移与弹性调整。从这里我们可以看到很多高层应用的功能十分依赖底层基础机制的支持。相较于Gandiva对GPU备份在框架层设计检查点，Singularity在内核驱动层面设计检查点，可以做到透明模型检查点和弹性作业的支持。

在工业界，有大规模人工智能应用场景的公司，都需要购买并部署大规模异构资源管理平台，并进行定制化的功能开发与支持，感兴趣的读者可以关注开源和业界的公开分享，并不断设计和重构公司的平台系统，让平台更加稳定与高效。

面向深度学习的集群管理系统目前仍是学术界和工业界的前沿和重要的研究和工程实践的方向，仍在不断发展和迭代，以上内容很多经典问题场景已经有很多共识，但解决方法和新的问题仍在高速发展，请感兴趣的读者或者从业者密切关注社区的发展与前沿，一起推动人工智能领域的集群资源管理不断朝着更加成熟，稳定与高效发展。

## 参考文献
- https://github.com/microsoft/pai
- https://www.kubeflow.org/
- [Myeongjae Jeon, Shivaram Venkataraman, Amar Phanishayee, unjie Qian, Wencong Xiao, and Fan Yang. 2019. Analysis of large-scale multi-tenant GPU clusters for DNN training workloads. In Proceedings of the 2019 USENIX Conference on Usenix Annual Technical Conference (USENIX ATC '19). USENIX Association, USA, 947–960.](https://dl.acm.org/doi/10.5555/3358807.3358888)
- [Ru Zhang, Wencong Xiao, Hongyu Zhang, Yu Liu, Haoxiang Lin, and Mao Yang. 2020. An empirical study on program failures of deep learning jobs. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering (ICSE '20). Association for Computing Machinery, New York, NY, USA, 1159–1170. DOI:https://doi.org/10.1145/3377811.3380362](https://dl.acm.org/doi/10.1145/3377811.3380362)
- [Wencong Xiao, Romil Bhardwaj, Ramachandran Ramjee, Muthian Sivathanu, Nipun Kwatra, Zhenhua Han, Pratyush Patel, Xuan Peng, Hanyu Zhao, Quanlu Zhang, Fan Yang, and Lidong Zhou. 2018. Gandiva: introspective cluster scheduling for deep learning. In Proceedings of the 13th USENIX conference on Operating Systems Design and Implementation (OSDI'18). USENIX Association, USA, 595–610.](https://dl.acm.org/doi/10.5555/3291168.3291212)
- [Hanyu Zhao, Zhenhua Han, Zhi Yang, Quanlu Zhang, Fan Yang, Lidong Zhou, Mao Yang, Francis C.M. Lau, Yuqi Wang, Yifan Xiong, and Bin Wang. 2020. HiveD: sharing a GPU cluster for deep learning with guarantees. Proceedings of the 14th USENIX Conference on Operating Systems Design and Implementation. Article 29, 515–532.](https://dl.acm.org/doi/10.5555/3488766.3488795)
- https://www.msra.cn/zh-cn/news/features/openpai
- [Singularity: Planet-Scale, Preemptive and Elastic Scheduling of AI Workloads](https://arxiv.org/abs/2202.07848)
- [D. Tiwari et al., "Understanding GPU errors on large-scale HPC systems and the implications for system design and operation," 2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA), 2015, pp. 331-342, doi: 10.1109/HPCA.2015.7056044.](https://ieeexplore.ieee.org/abstract/document/7056044)
- [Wencong Xiao, Shiru Ren, Yong Li, Yang Zhang, Pengyang Hou, Zhi Li, Yihui Feng, Wei Lin, and Yangqing Jia. 2020. AntMan: dynamic scaling on GPU clusters for deep learning. Proceedings of the 14th USENIX Conference on Operating Systems Design and Implementation. USENIX Association, USA, Article 30, 533–548.](https://www.usenix.org/conference/osdi20/presentation/xiao)
- [Juncheng Gu, Mosharaf Chowdhury, Kang G. Shin, Yibo Zhu, Myeongjae Jeon, Junjie Qian, Hongqiang Liu, and Chuanxiong Guo. 2019. Tiresias: a GPU cluster manager for distributed deep learning. In Proceedings of the 16th USENIX Conference on Networked Systems Design and Implementation (NSDI'19). USENIX Association, USA, 485–500.](https://dl.acm.org/doi/10.5555/3323234.3323274)
- [Shukla, D., Sivathanu, M., Viswanatha, S., Gulavani, B.S., Nehme, R.V., Agrawal, A., Chen, C., Kwatra, N., Ramjee, R., Sharma, P., Katiyar, A., Modi, V., Sharma, V., Singh, A., Singhal, S., Welankar, K., Xun, L., Anupindi, R., Elangovan, K., Rahman, H., Lin, Z., Seetharaman, R., Xu, C., Ailijiang, E., Krishnappa, S., & Russinovich, M. (2022). Singularity: Planet-Scale, Preemptive and Elastic Scheduling of AI Workloads. ArXiv, abs/2202.07848.](https://arxiv.org/abs/2202.07848)
- https://techcommunity.microsoft.com/t5/azure-compute-blog/singularity-on-azure-containers-for-hpc/ba-p/464174 
- [Yanghua Peng, Yixin Bao, Yangrui Chen, Chuan Wu, and Chuanxiong Guo. 2018. Optimus: an efficient dynamic resource scheduler for deep learning clusters. In Proceedings of the Thirteenth EuroSys Conference (EuroSys '18). Association for Computing Machinery, New York, NY, USA, Article 3, 1–14.](https://doi.org/10.1145/3190508.3190517)

