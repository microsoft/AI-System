<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 7.3 调度（Scheduling）

在之前的章节，我们已经介绍集群管理中的运行时，但是作业在启动前，需要平台本身进行决策进而进行调度。本章将围绕经典平台调度算法进行介绍，期望让读者了解作业调度的经典问题和算法。

- [7.3 调度（Scheduling）](#73-调度scheduling)
  - [7.3.1 调度问题优化目标](#731-调度问题优化目标)
  - [7.3.2 群调度](#732-群调度)
  - [7.3.3 主导资源公平DRF(Dominant Resource Fairness)调度](#733-主导资源公平drfdominant-resource-fairness调度)
  - [7.3.4 容量(Capacity)调度](#734-容量capacity调度)
  - [7.3.5 虚拟集群(Virtual Cluster)机制](#735-虚拟集群virtual-cluster机制)
  - [7.3.6 抢占式调度(Preemptive Scheduling)](#736-抢占式调度preemptive-scheduling)
  - [7.3.7 深度学习调度算法实验与模拟研究](#737-深度学习调度算法实验与模拟研究)
    - [7.3.7.1 数据读取](#7371-数据读取)
    - [7.3.7.2 评测指标设定](#7372-评测指标设定)
    - [7.3.7.3 算法实现与评测](#7373-算法实现与评测)
  - [参考文献](#参考文献)


## 7.3.1 调度问题优化目标

针对一批作业调度，常常考虑以下指标 :

- 吞吐(Throughput)：单位时间能完成的作业数量。平台希望吞吐量越大越好。 
- 完工时间(Makespan)：作业从启动到终止的时间，希望其越小越好，有些调度算法也考虑所有作业的整体完工时间作为优化目标。
- 平均响应时间(Average Response Time):平均响应时间是请求者在授予对全局资源的请求之前必须等待的平均时间。平台希望平均响应时间越短越好。
- 公平性(Fairness)：资源使用在平台用户或组之间平均分配，而不是在作业之间平均分配。
- 资源利用率(Utilization) ：描述用于作业的资源占总资源的百分比。平台希望利用率越高越好。
- 服务水平协议(SLA)：服务级别协议 (SLA-service-level agreement) 是服务提供商和客户之间的承诺。例如：服务的特定方面——质量、可用性、责任——在服务提供商和服务用户之间达成一致。 

接下来，我们将通过经典的调度算法，看平台常用算法是如何解决遇到的问题的。

## 7.3.2 群调度

深度学习作业通常会持续数小时，有些甚至会持续数周。深度学习作业通常需要群调度，直到所有必需的加速设备都被授予后才能开始训练过程。 

如果不使用群调度会产生什么问题? 深度学习作业可以同时执行多个任务，如果有依赖任务没启动，已启动任务会在同步点忙于等待或者频繁上下文切换 (如下图所示)。首先会造成训练任务无法训练，由于等待不能启动的任务，如下图所示两个作业都申请了部分资源，但是还需要其他资源才能启动，产生了死锁现象。同时已启动的任务不释放资源，造成资源浪费。


<center> <img src="./img/3/7-3-2-gangscheduleproblem.png" ch="500" width="400" height="400" /></center>
<center>图7-3-1. 并行启动执行作业可能产生的问题</center>

群调度(Gang Scheduling) 的Wiki定义是：一种用于并行系统的调度算法，用于调度相关线程或进程,在不同处理器上同时启动并运行。 

接下来，对上面的问题实例，我们使用群调度(Gang Scheduling)同时启动深度学习任务进程。图中的A，B，C作业就可以交替执行，保证任务能顺利执行完。

<center> <img src="./img/3/7-3-3-gangschedule.png" ch="500" width="400" height="260" /></center>
<center>图7-3-2. 并行执行作业可能产生的问题</center>

当然群调度自身也有一定的局限性，群调度会增加资源碎片化的风险，并且在共享集群中利用率低。如图中t1,t2时间段，GPU 7和8就是空闲浪费的。

## 7.3.3 主导资源公平DRF(Dominant Resource Fairness)调度 

目前深度学习平台其实包含多种异构资源（CPU，GPU，memory等）以及被多用户使用是一个多租的环境。在调度过程中用户会细粒度的申请不同资源的用量，我们在满足用户异构资源需求的同时，也希望在多租的环境下兼顾一定的公平。

- 问题：包含异构资源类型的系统中如何进行多作业公平(Fairness)的资源调度？
- 挑战：相比传统单资源公平调度，深度学习作业也需要使用多种异构资源 (CPU, Host memory, etc.)，并且需要调度GPU及GPU memory
  
DRF 使用优势资源的概念来比较多维（CPU, GPU, memory等）资源。这个想法是在多资源环境中，资源分配应该由作业（用户或队列）的主导份额决定，这是作业已分配的任何资源（内存或 CPU）的最大份额。

本质上，DRF 优化目标是寻求最大化所有实体的最小主导份额(smallest dominant share)。

DRF调度策略：
1. 通过同类型资源在集群整体资源中的份额确定主导资源 (dominant resource) 
2. 基于最大最小公平（max-min fairness）的针对多资源类型（e.g. GPU, CPU）的调度算法

如下图所示实例，有Job 1和Job 2都在启动多个任务并申请多个资源。第一步先计算每个Job的主导资源。Job 1主导资源为Memory，Job 2主导资源是GPU。Job 1的优先级高于Job 2，因为Job 1份额0.4小于Job 2份额0.5。 

<center> <img src="./img/3/7-3-1-drf.png" ch="500" width="300" height="400" /></center>
<center>图7-3-3. 2个作业的DRF调度实例</center>


$$Cluster \  Resources: [10 \  GPU, 20GB \ RAM] $$

下面的资源申请需求下，主导资源是内存。
$$
Job \ 1: \\
$$
$$
Total \ GPU \ 1 + 1 = 2 \ GPU \\
$$
$$
GPU \ Share \ \frac{2}{10} = 0.2 \\
$$
$$
Total \ Memory \ 4 + 4 = 8GB \\
$$
$$
Memory \ Share \ \frac{8}{20} = 0.4 \\
$$
$$
Dominant \ resource \ is \ Memory
$$

下面的资源申请需求下，主导资源是GPU。
$$
Job \ 2:  \\
$$
$$
Total \ GPU \ 2 + 3 = 5 \ GPU \\
$$
$$
GPU \ Share \ \frac{5}{10} = 0.5 \\
$$
$$
Total \ Memory \ 2 + 2 = 4GB \\
$$
$$
Memory \ Share \ \frac{4}{20} = 0.2 \\
$$
$$
Dominant \ resource \ is \ GPU
$$


## 7.3.4 容量(Capacity)调度

除了多个作业能够兼顾公平的分配，平台管理员还需要考虑如果是多个组共享平台，也需要兼顾组与组之间的公平性。如何让多个小组共享集群？
  - 能否为多个组织共享集群资源？
  - 共享集群资源的同时，如何同时为每个组织提供最小容量保证?
  - 空闲资源能否弹性为其他组织利用？
- 挑战：相比传统容量调度调度，深度学习作业也需要考虑调度GPU及GPU memory
  
容量调度器在大数据平台常常作为主流调度器使用。它允许多租户安全地共享一个大型集群，以便在分配容量的限制下及时为他们的应用程序分配资源。 

我们看下图实例，图中Team A，B，C共享集群，每个组有多个用户，每个用户都会提交作业使用集群资源。如果不考虑组间公平性，Team A再申请了45\%的资源后，如果没有使用造成浪费的同时，也会让Team C对资源无法申请，产生[饥饿(Starvation)](https://en.wikipedia.org/wiki/Starvation_(computer_science))现象
<center> <img src="./img/3/7-3-4-capaproblem.png" ch="500" width="400" height="300" /></center>
<center>图7-3-4. 资源占用过多造成其他组无法分配资源问题</center>

所以，为了支持支持多租(Multi-tenant）资源共享，容量调度被设计出来。

容量调度策略集合：
- 提升利用率（Utilization）: 
  - 虚拟集群(Virtual Cluster):组能看到视图是虚拟资源，并不绑定具体机器，等作业启动后分配相应的资源。
  - 层级队列(Hierarchical Queues): 支持队列分层结构，以确保在允许其他队列使用空闲资源之前在组织的子队列之间共享资源，从而提供更多的控制和可预测性。 
- 多租与公平性(Fairness):
  - 多租与用户限制因素(User Limit Factor)：
    - 从某种意义上说，队列将分配到网格容量的一小部分，因为它们可以使用一定容量的资源。 提交到队列的所有应用程序都可以访问分配给队列的容量。 管理员可以对分配给每个队列的容量配置软限制和可选的硬限制。 
    - 允许多用户多组以多租形式使用集群。控制单用户的可以消耗的最大资源，放止占用资源过多，造成其他进程无法申请资源。
  - Dominant Resource Fairness (DRF)：对异构计算场景，扔可以采用适合多维资源调度或其他自定义调度器。满足以上约束情况下，仍旧可以采用DRF等调度策略进行具体作业之间的调度与资源分配。
- 弹性(Elasitcity)和SLA 
  - 奖励资源(Bonus Resource)：对其他组没有使用的资源可以临时免费出让给有需要的团队，但是当资源持有者需要，则需要抢占资源归还给持有者。
  - 抢占(Preemption)：配合奖励资源使用，保证对用户提供的服务协议。

如下图所示，当管理员配置了最小和最大的组使用资源限额，这样保证组与组之间都有资源可用。

<center> <img src="./img/3/7-3-5-capascheduling.png" ch="500" width="400" height="300" /></center>
<center>图7-3-5. 容量调度</center>

## 7.3.5 虚拟集群(Virtual Cluster)机制

在这些集群内，一般组和用户所看到的的资源并没有绑定到具体的物理机器，而是在调度后决定作业启动的物理机器。这背后是通过虚拟集群 (Virtual Cluster) 映射所实现的。而虚拟集群和我们之前介绍的cgroups的目的和设计较为类似。我们会看到很多集群产生的问题，在传统的操作系统中都能找到类似的设计问题与原则。

如下图所示，虚拟集群会在配置用户组的配额和视图，物理集群是在调度后在运行时绑定的。这样可以大幅提升资源利用率。

<center> <img src="./img/3/7-3-6-vc-bind.png" ch="500" width="400" height="200" /></center>
<center>图7-3-6. 虚拟集群和物理集群映射与绑定</center>

针对深度学习的细粒度虚拟集群策略：
- 虚拟集群根据小组的配额进行定义
- 每个租户(Tenant)构成了一个虚拟集群(VC)
- 资源被分配给租户(Tenants)
- 将虚拟集群映射到物理集群

<center> <img src="./img/3/7-3-7-vc-define.png" ch="500" width="400" height="300" /></center>
<center>图7-3-6. 虚拟集群资源分配</center>

## 7.3.6 抢占式调度(Preemptive Scheduling)

一些集群管理员为了减少组空闲资源造成的浪费，想共享空闲资源，但是单纯出让资源但是不能保证原有用户随时要回会产生相应新的问题，不能保证对原用户的SLA （service level agreement）。所以一般通过抢占调度解决。抢占调度也适合，需要保证其他作业相应时间的场景。



如下图所示，如过APP2长时间无法得到资源，则无法执行，而其执行时间实际很短。这就需要通过抢占机制进行调度。让APP2获取一定资源执行，保证降低平均响应时间。

<center> <img src="./img/3/7-3-8-preemptive.png" ch="500" width="400" height="300" /></center>
<center>图7-3-7. 作业等待时间过长问题</center>

下图所示，我们可以看到，A队列中配置可用为10资源，但是由于集群有空闲资源，多实用了20奖励资源给C6和C7，这时C队列需要使用20资源，而且集群应该保证，这时会触发抢占。当APP1的C6和C7使用的资源被标记为可以被抢占后，其资源可以通过以下步骤被抢占：
1. 从过度使用的队列中获取需要被抢占的容器(队列A的C6和C7)。
2. 通知作业(队列A)控制器即将触发抢占。
3. 等待直到被抢占终止运行。

<center> <img src="./img/3/7-3-9-preemptive.png" ch="500" width="400" height="260" /></center>
<center>图7-3-8. 抢占调度</center>

抢占式调度对深度学习作业的挑战：在深度学习作业下，被抢占的作业当前只能失败，无法像传统OS上下文切换。未来可以设计更好的深度学习检查点和恢复技术进行减少抢占后作业失效造成之前作业的训练资源无效使用的问题。

## 7.3.7 深度学习调度算法实验与模拟研究

此开源数据集[philly-traces](https://github.com/msr-fiddle/philly-traces) 包含 Microsoft 内部 Philly集群上第一方 (first-party) DNN 训练工作负载的代表性子集。 数据是 ATC’19 中“Analysis of large-scale multi-tenant GPU clusters for DNN training workloads”中描述的工作负载的一个脱敏数据子集。 这项工作是作为 Microsoft Research 的 Project Fiddle 的一部分完成的。

### 7.3.7.1 数据读取
读者可以参考库中提供的脚本读取数据并了解数据模式。
### 7.3.7.2 评测指标设定
读者可以根据本章开始介绍的指标设计优化目标。
### 7.3.7.3 算法实现与评测
读者可以选用以上介绍的经典算法作为基准测试，设计新的算法，并通过真实平台数据模拟，看能否提升当前目标，超越基准算法，并进行结果分析，形成分析报告或论文。

## 参考文献

- [Analysis of Large-Scale Multi-Tenant GPU Clusters for DNN Training Workloads](https://dl.acm.org/doi/10.5555/3358807.3358888)
- [Multi-tenant GPU Clusters for Deep Learning Workloads: Analysis and Implications](https://www.microsoft.com/en-us/research/uploads/prod/2018/05/gpu_sched_tr.pdf)
- [Gandiva: Introspective Cluster Scheduling  for Deep Learning](https://dl.acm.org/doi/10.5555/3291168.3291212)
- [Dominant Resource Fairness: Fair Allocation of Multiple Resource Types](https://cs.stanford.edu/~matei/papers/2011/nsdi_drf.pdf) 
- [All You Need to Know about Scheduling Deep Learning Jobs](https://www.sigops.org/src/srcsosp2017/sosp17src-final35.pdf) 
- https://github.com/microsoft/pai
- https://www.kubeflow.org/ 
- [Kubeflow Pipelines With GPUs](https://betterprogramming.pub/kubeflow-pipelines-with-gpus-1af6a74ec2a)
- [YARN – The Capacity Scheduler](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html)
- [Better SLAs via Resource-preemption in YARN’s Capacity Scheduler](https://blog.cloudera.com/better-slas-via-resource-preemption-in-yarns-capacityscheduler/)
- [Kube-Batch: Dominant Resource Fairness (DRF)](https://github.com/kubernetes-sigs/kube-batch/blob/master/doc/design/plugin/drf.md)
- https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.4.3/bk_yarn_resource_mgt/content/ref-4c095638-c9ce-487a-b42d-eab26fd58b9c.1.html

