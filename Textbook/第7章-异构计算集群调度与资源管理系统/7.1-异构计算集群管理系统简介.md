<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 7.1 异构计算集群管理系统简介

本章介绍异构集群管理系统的设计初衷，需要解决的问题及挑战，并通过启发式实例以更为具象的方式展开。之后我们会交替使用集群管理系统与平台代表当前的异构计算集群管理系统。

- [7.1 异构计算集群管理系统简介](#71-异构计算集群管理系统简介)
  - [7.1.1 多租环境运行的训练作业](#711-多租环境运行的训练作业)
  - [7.1.2 作业生命周期](#712-作业生命周期)
  - [7.1.3 集群管理系统架构](#713-集群管理系统架构)
  - [小结与讨论](#小结与讨论)
  - [参考文献](#参考文献)

## 7.1.1 多租环境运行的训练作业

<center><img src="./img/1/7-1-1-jobsubmission.png"  /></center>
<center>图 7.1.1 多租环境提交运行作业</center>

如图 7.1.1 所示，企业级环境下，不同用户会提交不同框架的作业，有不同作业的资源需求，共享一个物理集群才能让组织减少硬件资源浪费。
在企业级深度学习场景下，大型企业有很多机器学习科学家与工程师，组织有大量的 GPU 服务器，为了组织效率的提升与资源共享，就诞生了针对深度学习场景下设计的多租户的平台系统。多租户（Multi-Tenancy）技术是一种软件架构技术，它实现如何于多用户的环境下共用相同的系统或程序组件，并且仍可确保各用户间数据的隔离性。又由于平台系统管理的资源是异构（例如, CPU，GPU 等）的，所以本章主要介绍的是管理异构资源，调度深度学习作业的，多租户的平台系统（Platform System）。

在企业级场景下，多用户共享多 GPU 服务器相比原来深度学习开发者独占使用服务器进行模型训练，有很大的不同，如图 7-1-1 所示。这也为之后的异构计算集群管理系统（简称，平台，深度学习平台）的设计产生了相应的需求。主要体现在以下几点:

- 多作业（Job），多用户
  - 每个用户需要不断改进模型，超参数调优，调试与优化作业，这样会提交大量的作业到平台。
  - 多用户，多样的（例如，CV，NLP等）人工智能团队。团队有多名工程师，他们会在同一时段向平台申请资源执行作业。
- 作业环境需求多样
  - 目前深度学习的技术栈并不统一，有的用户使用 TensorFlow，有的用户使用 PyTorch。使用的框架版本也可能不一样，造成底层依赖，例如 NVIDIA CUDA 等也可能版本不同。用户如果共享机器，需要对依赖的环境互不干扰。
- 作业资源需求多样
  - 用户提交的作业有些是分布式训练作业，对资源需求较多，有些是单机的调试任务，对资源的需求较少。平台需要按需求做好资源的分配，减少资源碎片。
  - 同时对用户来说，不希望作业本身受到其他作业的硬件资源与命名空间内软件的干扰，理想是像使用独占资源一样运行自己的作业。平台需要做好运行期资源隔离。
- 服务器软件环境单一
  - 平台方在采购资源，安装底层操作系统和驱动时，是无法规划和指定未来用户的软件和版本，同时为了运维和部署减少软件兼容性问题，一般将服务器安装统一的操作系统，驱动，并让其版本保持一致，减少运维负担。这与之前用户的多样环境需求产生了矛盾。
- 服务器空闲资源多样
  - 虽然平台一般批量购买同型号大量机器。但由于用户的作业申请资源多样，作业生命周期多样，造成资源释放后，平台上的空闲资源的组合比较多样，需要设计好调度策略，尽可能提升资源的利用率。

从以上的问题，我们可以看到调度与资源管理系统重要性。它底层抽象并管理计算资源，对上层应用提供隔离并且易用的作业运行时环境，整理来说我们可以理解其为支持深度学习应用的管理分布式GPU集群的操作系统。我们可以总结以下几点来概括平台的使命和重要性：

- 提供人工智能基础架构支持：
  - 高效地深度学习作业调度与管理：根据作业资源需求，分配和回收计算资源，提升利用率的同时，保持一定的公平性等。
  - 稳定地异构硬件管理：高效运维，动态扩容，节点问题修复等。管理员和用户可以监控节点及硬件资源状态和利用率等。
- 提升用户的研发生产力：
  - 用户专注于模型创新，无需关注系统部署，管理。通过镜像等技术，让用户打包软件依赖，简化部署与安装。
  - 运行时隔离资源与软件依赖，让用户像独占一样使用运行时资源，执行作业。
  - 模型，代码和数据共享，加速研究与创新。团队与组织内共享与提供插件化的功能支持，加速创新。

## 7.1.2 作业生命周期

接下来，让我们先了解一下，一个深度学习作业，在平台上是如何提交并执行的，也就是作业的生命周期。

<center><img src="./img/1/7-1-2-cluster.png" /></center>
<center>图 7.1.2 GPU 集群</center>

平台上作业生命周期：

1. 作业提交与排队：用户先将作业的依赖环境打包为镜像，并上传到公共镜像中心。之后用户将代码，数据等运行作业需要的素材，上传到平台的文件系统。之后用户可以通过提交工具（例如，Web，命令行，API 等），填写资源申请（例如，几块 GPU，内存需求），作业启动命令，部署方式，以及镜像，代码和数据的路径。之后点击提交即可。
2. 作业资源分配与调度：平台收到用户的资源申请后，先进行排队，调度器轮询到作业时，根据目前集群中空闲资源状况，根据一定的调度算法，决定作业是在哪些服务器节点启动，如果不满足条件，则继续排队。
3. 作业执行完成与释放：当作业被调度器调度启动，平台会在有空闲资源的节点启动作业，下载镜像，挂载代码和数据所在的文件系统到节点本地，运行时做好资源限制与隔离，启动作业，执行作业。在作业运行中，平台监控不断收集运行时性能指标和日志，方便用户调试。作业执行完成后，平台会释放申请的资源，并继续分配给其他作业使用。

在这样一个生命周期中，请大家思考集群环境的新问题与挑战可以通过什么技术解决：

1. 如何提交作业与解决环境依赖问题？
2. 如何高效调度作业并分配资源？
3. 如何将启动的作业运行时资源与命名空间隔离？ 
4. 如何面向深度学习作业和异构资源设计集群管理系统？
5. 如何高效存取数据？
6. 如何不断开发平台新功能与运维平台保证稳定性？

## 7.1.3 集群管理系统架构

<center><img src="./img/1/7-1-3-archcluster.png" /></center>
<center>图 7.1.3. 异构集群管理系统架构</center>

如图 7.1.3 所示，异构集群管理中通常包含以下组件进行资源与作业管理。

***平台中的主要组件***：
1. 集群调度与资源管理模块：管理集群资源和状态，调度作业到集群空闲资源，回收运行完作业的资源。一般控制平面（Control Plane）可以选择使用 Kubernetes，YARN，Mesos 等系统。同时定制化调度器或者选择开源调度器组件，例如 HiveD 等。
2. 镜像中心：存储 Docker 镜像，用户提交镜像，作业下载加载镜像。一般可以选用 Docker Hub，或者私有的镜像中心，或者云上镜像中心 Azure Containter Registry 等。
3. 存储模块：在平台中扮演数据平面（Data Plane）角色，存储数据，模型与代码。用户上传数据，作业下载数据和上传结果。存储系统一般可以选用：NFS，Lustre，HDFS 等，或者选用云存储 AWS S3，Azure Blob 等。
4. 作业生命周期管理：部署作业，监控作业，重试作业，作业错误诊断，类型属于单作业的控制平面，一般不涉及其他作业情况，除非自动化机器学习系统构建在平台接口之上进行部署。生命周期管理一般可以选择使用 K8s Operator，Framework Controller，YARN AppMaster 等。
5. 集群监控与报警：负责集群硬件，服务与作业的监控与报警。监控系统一般可以选择使用 Promethus + Grafana + Alert Manage 等开源系统搭建。
6. 开发接口：Web门户，REST服务与集成开发环境（例如，VS Code 和 Jupyter Notebook）被用户使用进行作业与数据资源提交与作业管理监控与调试。
7. 测试集群：为了和生产环境隔离，一般开发工程师可以在测试平台进行开发测试，之后再上线到生产环境。


***经典回顾***

关注点分离（[Separation of Concerns](https://en.wikipedia.org/wiki/Separation_of_concerns)）简称 (SoC) 是将计算机系统分成不同部分的设计原则。每个部分都解决了一个单独的问题（Concern），即一组影响计算机程序代码的信息。能够很好地体现 SoC 的系统称为模块化（Modular）系统。例如，平台中的关注点有：调度，监控，存储，运行时等。

***平台中的角色***：
1. 用户：用户打包作业镜像，上传数据和代码到存储，并书写作业规格（Specification），进而提交作业，观察作业性能和错误，如果有问题重新修改提交，如果成功则获取训练完成的模型或者处理完的数据。
2. 运维工程师：运维工程师负责监控运维和管理集群健康状况和错误，处理和应对突发时间，进行错误修复，增加和配置租户资源等。
3. 平台开发工程师：平台工程师负责不断开发平台服务的新组件与功能，持续集成，持续部署。

机构一般可以根据，成本，数据合规与安全，弹性资源需求等多个维度衡量和考虑是本地还是采用云平台进行平台部署。平台的部署模式当前一般支持以下几种***部署模式***：

1. 本地（On-Premises）部署方式：有些公司选择使用开源（例如，OpenPAI，Kubeflow 等）或者自研平台（例如，基于 Kuberenetes，YARN 等二次开发）进行本地部署，保证数据，镜像等在自有数据中心维护，这种方式对运维与开发工程师要求较高。
2. 公有云部署方式：有些公司可以采购公有云的 IaaS 或者 PaaS 服务进行平台搭建，好处是减轻运维压力并可以利用公有云平台最先进的技术，能够弹性伸缩资源，但是数据和代码需要上云。
3. 混合云：目前有些公司采用敏感数据放在本地数据中心，非敏感数据或弹性资源需求上公有云的方案，一些公有云提供商也提供了混合云机器学习平台，例如，微软提供 [Azure Arc-enabled machine learning](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-attach-arc-kubernetes?tabs=studio) 服务，在一套集群管理系统中管理混合云深度学习平台资源。
4. 多云方式：目前有些公司出于防止锁死一家公司或者综合选取性价比最高的方案会选择多云方案，例如有些公司如 HashCorp 等提供多云运维工具与服务。在 Hotos '21 上 UCB 的 Ion Stoica 和 Scott Shenker 发表“[From Cloud Computing to Sky Computing](https://sigops.org/s/conferences/hotos/2021/papers/hotos21-s02-stoica.pdf)”，这篇文章简而言之，它应该很容易让开发人员构建多云应用程序，不同的基础设施模块和服务可以来源不同的云服务提供商，也称此为天空计算（Sky Computing）。但是当前商业化的提供商受限于性能安全等因素，常用的资源服务组合或者粒度，更多的是在供应商间提供无缝切换整体基础设施，例如，[HashCorp](https://www.hashicorp.com/) 等。

如图 7.1.4 所示，一般本地部署初始一次性投入较大，在初始的几年成本高于云计算按需付费的方式，云计算一般在一定年份之后，成本会逐渐超过本地部署。所以云适合公司规模较小和发展初期，等业务稳定和体量大后，机构可以选择自建本地集群或者采用混合云方式，降本增效。

<center><img src="./img/1/7-1-4-publicvspremisecloud.png" /></center>
<center>图 7.1.4 本地与云部署成本趋势 (<a href="https://www.scirp.org/journal/paperinformation.aspx?paperid=87661">图片来源</a>) </center>

## 小结与讨论

本章我们主要介绍异构计算集群管理系统的应用场景和其中的面对的问题与挑战，启发读者展开后续章节的阅读，并从中理解为何会涉及到相关技术点，其解决了何种问题。

请读者思考，多租的场景相比独占使用资源的场景让系统面临何种问题和挑战？

## 参考文献

- https://en.wikipedia.org/wiki/Multitenancy
- https://en.wikipedia.org/wiki/Docker_(software)
- [Ru Zhang, Wencong Xiao, Hongyu Zhang, Yu Liu, Haoxiang Lin, and Mao Yang. 2020. An empirical study on program failures of deep learning jobs. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering (ICSE '20). Association for Computing Machinery, New York, NY, USA, 1159–1170. DOI:https://doi.org/10.1145/3377811.3380362](https://dl.acm.org/doi/10.1145/3377811.3380362)
- https://www.scirp.org/journal/paperinformation.aspx?paperid=87661
