<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 1.4 影响深度学习系统设计的理论，原则与假设

人工智能系统内容保罗万象，涵盖传统计算机体系结构，编译器，操作系统，计算机网络的经典的应用与拓展。同时我们也可以观察到经典的计算机系统相关理论和系统优化方法在深度学习中依然在发挥巨大的作用，我们依然可以将当前很多问题映射和抽象并通过经典理论所解决。那么在展开后面内容之前，我们通过几个代表性理论和在深度学习系统中的应用，以及深度学习作业的独有特点所产生的新问题展开，启发之后我们在学习具体系统问题时形成理论指导。

- [1.4 影响深度学习系统设计的理论，原则与假设](#14-影响深度学习系统设计的理论原则与假设)
  - [1.3.1 抽象-层次化表示与解释](#131-抽象-层次化表示与解释)
  - [1.3.2 摩尔定律(Moore's Law)-预测算力发展趋势](#132-摩尔定律moores-law-预测算力发展趋势)
  - [1.3.3 局部性原则(Priciple of Locality)-内存层次结构(Memory Hierarchy)](#133-局部性原则priciple-of-locality-内存层次结构memory-hierarchy)
  - [1.3.4 深度学习负载的线性代数(Linear Algebra)计算与缺陷容忍(Defect Tolerance)特性](#134-深度学习负载的线性代数linear-algebra计算与缺陷容忍defect-tolerance特性)
  - [1.3.5 并行(Parallel)加速与阿姆达尔定律(Amdahl's Law)优化上限](#135-并行parallel加速与阿姆达尔定律amdahls-law优化上限)
  - [1.3.6 冗余(Redundancy)与可靠性(Dependability)](#136-冗余redundancy与可靠性dependability)
  - [小节与讨论](#小节与讨论)
  - [参考文献](#参考文献)

## 1.3.1 抽象-层次化表示与解释

系统会在各个层次抽象不同的表示，在高层方便用户表达算子，在底层则被转换为指令被芯片执行。这样搭积木的方式让整个工具链快速协同发展且能互相利用，大为加速了开发效率与自动化。
我们一般通过Python语言书写和调用深度学习库完成整个机器学习流水线的构建，但其实刚刚接触到其中一层，从上到下系统已经为我们抽象了多个层次。我们通过下面实例可以了解各个层次的抽象与表达，为之后的学习形成立体化的视角。

- 语言：Python语言

在本层可以书写各种控制流，调用库等。

```
for i in range(10):
  # 执行vector add
```
- 框架：TensorFlow, PyTorch等

例如，我们通过Python调用PyTorch实现一个简单的向量加法。
```python
impor torch

a = torch.randn(4)
b = torch.randn(4)
torch.add(a, b)
```

在本层，我们可以编写深度学习模型，通过卷积，池化，全连接，注意力机制，等算子，组合出复杂的数据流图模型。

- 驱动编程库：CUDA, cuDNN等

当向量加法执行到CUDA层，将通过下面实例进行实现
例如，在CUDA层就会实现
```c++
...
// CUDA内核.每个线程执行一个元素的加法
__global__ void vecAdd(double *a, double *b, double *c, int n)
{
    // 获取全局线程Id
    int id = blockIdx.x*blockDim.x+threadIdx.x;
 
    // 确保不越界
    if (id < n)
        c[id] = a[id] + b[id];
}
...
```
- 汇编指令：NVIDIA GPU中有PTX, SASS等

PTX 是一种低级并行线程执行虚拟机(Virtual Machine)和指令集架构(ISA)。 PTX 将GPU暴露为并行计算设备。SASS是编译成二进制微码的低级汇编语言，在 NVIDIA GPU 硬件上本地执行。

例如，读者可以书写导出或者参考[实例](https://docs.nvidia.com/cuda/ptx-compiler-api/index.html#sample-example)。

下面为向量加法的PTX指令实例：
```
   .version 7.0                                           \n \
   .target sm_50                                          \n \
   .address_size 64                                       \n \
   .visible .entry simpleVectorAdd(                       \n \
        .param .u64 simpleVectorAdd_param_0,              \n \
        .param .u64 simpleVectorAdd_param_1,              \n \
        .param .u64 simpleVectorAdd_param_2               \n \
   ) {                                                    \n \
        .reg .f32   %f<4>;                                \n \
        .reg .b32   %r<5>;                                \n \
        .reg .b64   %rd<11>;                              \n \
        ld.param.u64    %rd1, [simpleVectorAdd_param_0];  \n \
        ld.param.u64    %rd2, [simpleVectorAdd_param_1];  \n \
        ld.param.u64    %rd3, [simpleVectorAdd_param_2];  \n \
        cvta.to.global.u64      %rd4, %rd3;               \n \
        cvta.to.global.u64      %rd5, %rd2;               \n \
        cvta.to.global.u64      %rd6, %rd1;               \n \
        mov.u32         %r1, %ctaid.x;                    \n \
        mov.u32         %r2, %ntid.x;                     \n \
        mov.u32         %r3, %tid.x;                      \n \
        mad.lo.s32      %r4, %r2, %r1, %r3;               \n \
        mul.wide.u32    %rd7, %r4, 4;                     \n \
        add.s64         %rd8, %rd6, %rd7;                 \n \
        ld.global.f32   %f1, [%rd8];                      \n \
        add.s64         %rd9, %rd5, %rd7;                 \n \
        ld.global.f32   %f2, [%rd9];                      \n \
        add.f32         %f3, %f1, %f2;                    \n \
        add.s64         %rd10, %rd4, %rd7;                \n \
        st.global.f32   [%rd10], %f3;                     \n \
        ret;                                              \n \
   } 
```

- 机器码

每条汇编指令则会在内存中表示为"01010010"形式的二进制序列，最终被芯片解码(decode)执行。

- 硬件执行单元：ALU，控制单元，寄存器，总线等

例如，在[冯诺依曼架构](https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html)的GPU中分别由指令寄存器和流水线进行指令存储，加载，解码，并执行指令。由数据流水线，通过指令控制，将数据加载到寄存器，放入ALU执行并将结果写回内存。

## 1.3.2 摩尔定律(Moore's Law)-预测算力发展趋势

摩尔定律(Moore's law)是由英特尔（Intel）创始人之一戈登·摩尔提出的。集成电路上可容纳的晶体管数目，约每隔两年便会增加一倍。而英特尔首席执行官大卫·豪斯（David House）提出且经常被引用的是预计18个月会将芯片的性能提高一倍（即更多的晶体管使其更快）。

在GPU领域，[黄氏定律](https://en.wikipedia.org/wiki/Huang%27s_law#:~:text=Moores%20law%20would%20predict%20a,the%20new%20'law'%20possible.)是英伟达创始人黄仁勋提出，即图形处理器(GPU) 的发展速度比传统中央处理单元 (CPU) 的发展速度要快得多。黄氏定律指出，GPU 的性能每两年将翻一番以上。

有多种论调在讨论摩尔定律已死，当然也有一些论调认为摩尔定律还存在，只不过是需要以另一种方式来理解，例如台积电企业研究副总裁Philip Wong 博士在Hotchips 2019上提出摩尔定律未死([Moore's Law is not Dead](https://www.youtube.com/watch?v=O5UQ5OGOsnM))，其出发点在于“处理器速度时钟速度已经饱和并不意味着摩尔定律已经失效，就像摩尔博士很多很多年前预测的那样，密度（例如，晶体管，逻辑门，SRAM等）不断增加是摩尔定律持续存在的驱动力”。当时还在Intel的Jim Keller也层做过"Moore’s Law is Not Dead"的演讲，并提出：“要理解计算领域的这种不减增长，我们需要将摩尔定律晶体管数量指数解构为计算堆栈中众多独立创新的输出——在硅工艺技术、集成电路设计、微处理器架构和软件方面。虽然晶体管性能和功率等某些向量的收益确实在递减，但晶体管架构、微处理器架构、软件和新材料等其他向量的收益却在增加。”所以我们也看到软件系统也扮演越来越重要的角色，软硬件协同设计可以进一步挖掘硬件性能。

随着芯片不断提升算力，但是我们发现系统性能本身还会受到其他短板部分或者约束所限制。例如：

- [功耗墙](http://www.edwardbosworth.com/My5155_Slides/Chapter01/ThePowerWall.htm)约束：
  - 1990 年代末和2000年代初的设计目标是提高时钟频率，这是通过在更小的芯片上添加更多晶体管来实现的。不幸的是，这增加了CPU芯片的功耗超出了廉价冷却技术的能力。所以这种约束让工业界有两种路线，一种是采用更复杂的冷却技术，一种是转向多核设计。我们目前看到的针对人工智能设计的芯片也都是多核或者是众核的。同时我们在NVIDIA GPU中通过nvidia-smi命令也可以看到，当GPU的温度超出一定阈值,GPU会减速或者关闭。
- [内存墙](https://en.wikipedia.org/wiki/Random-access_memory#Memory_wall)约束：
  - [“内存墙”](https://en.wikipedia.org/wiki/Random-access_memory#Memory_wall)是芯片与芯片外的内存之间越来越大的速度差距。造成这种差距的一个重要原因是超出芯片边界的有限通信带宽，也称为带宽墙。例如，从 1986 年到 2000 年，CPU 速度以每年 55% 的速度提高，而内存速度仅提高了 10%。鉴于这些趋势，预计内存延迟将成为计算机性能的压倒性瓶颈。这种现状其实我们在GPU的发展过程中也可以观察到，例如，NVIDIA H100相比A100在FP32 Vector上是60TFLOPS相比19.5TFLOPS的近3倍提升，但是内存带宽只是3TB/sec比2TB/sec的1.5倍提升，访存落后于计算的提升速度。可以认为在人工智能领域内存墙这种情况还存在。

在了解以上趋势后，会启发我们看到为何后面大量的系统设计优化，利用多核和分布式计算，以及为何减少数据搬运的原因，因为单纯依靠硬件提升是有一定天花板，我们还需要协同通过系统设计进一步提升效率和性能。

## 1.3.3 局部性原则(Priciple of Locality)-内存层次结构(Memory Hierarchy)

深度学习的访存特点是：
- 对模型整体的每轮迭代，读取批次数据，这部分会随机采样数据进行读取。
- 对模型中的每个算子计算可以转换为底层的循环执行，这其中对输入输出数据的访问有一定的缓存复用机会。

计算机的内存层次结构很深，不同层级的访存时间，带宽，空间和价格都不同，但遵循一定的递增或递减的关系。

<img src="img/4/MemoryHierarchy.png" ch="500" />

图1-4-1. 内存层次结构(Memory Hierarchy)[图片引用](https://diveintosystems.org/book/C11-MemHierarchy/mem_hierarchy.html)
在图中我们看到传统的内存层次结构问题总结中没有覆盖GPU和AI加速器，我们可以通过PCIe总线将AI加速器与当前已有体系结构互联，让计算机利用AI加速器加速模型的训练与推理。

那么我们可以总结各个内存层次结构结构中的利用局部性的例子和机会：
- GPU L1, L2缓存：
  - 片上缓存较小，对算子内计算负载进行切片以及访存调度优化是减少缓存失效是常见思路。深度学习编译器关注循环(Loop)的块(Tile)优化很大程度上因为缓存较小引起。
  - 缓存预取(Prefetch)指令设计：例如在NVIDIA GPU的PTX指令中，提供缓存预取指令，此例子通过预取指令加载数据到L2缓存["```ld.global.L2::64B.b32   %r0, [gbl]; // Prefetch 64B to L2```"](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html)。

<img src="img/4/gpumem.png" ch="500" />

图1-4-2. NVIDIA GPU架构与内存层次结构[图片引用](https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html)

- GPU显存DRAM（与主存通过PCIe总线互联）：
  - GPU显存和主存之间通常传输的是批次(Batch)数据和模型。可以利用深度学习模型对批次数据的访问时间局部性做相应优化，例如可以通过[vDNN](https://dl.acm.org/doi/10.5555/3195638.3195660)等方式对假设已知访存模式情况下的数据预取与卸载。
<img src="img/4/host_device.png" ch="500" />

图1-4-3. GPU显存和主存关系[图片引用](https://www.telesens.co/2019/02/16/efficient-data-transfer-from-paged-memory-to-gpu-using-multi-threading/)
- 主存：
  - 主存和文件系统之间一般传输的是数据，和模型文件，可以提前下载和准备好数据文件，并做好缓存。
  - 对频繁访问的磁盘数据，也可以将缓存逻辑交给操作系统管理。
- 本机磁盘存储：
  - 高效和统一的顺序文件存储格式能够减少随机读写小文件的问题。
  - 本地磁盘存储也可以充当云存储的缓存，例如，[Azure Blob Fuse](https://github.com/Azure/azure-storage-fuse)。
- 网络与云存储：
  - 缓存文件系统中数据或者预取数据可以减少网络或云存储中文件的读取代价，例如：[Alluxio](https://www.alluxio.io/)。

**[局部性](https://en.wikipedia.org/wiki/Locality_of_reference)原则**：
- 时间局部性(Temporal locality)：
  - 通常我们在执行一些迭代计算（例如，循环）中，会在不远的未来再次访问模块地址空间，如果我们能摸清这个规律，利用这种时间局部性，在当前内存达到上限驱逐(Evict)这块地址空间数据之前再次访问，就能够减少再次加载这块数据的开销，提升整体性能和效率。这种优化常常应用于算子内核内部的循环计算中，或者粒度拓展到训练迭代的周期性加载批次数据中。
- 空间局部性(Spatial locality)：
  - 由于内存和总线处于效率和性能考虑，不同存储层级之间的读写最小单元并不是1个字节（例如，通常主存会设128字节的缓存线，加载数据到缓存。磁盘会设置512字节作为块大小作为最小的读写粒度。PCIe会设置最大有效载荷(Maximum Payload）大小为主存缓存线大小来控制传递的数据粒度和效率。GPU也会设置缓存线。），这样就需要系统跨内存加载数据时，尽可能不空载和不会近期访问的数据，类似让“物流”运输更加高效，塞满货车为近期就要使用的商品，提升效率。这种优化遍布于各个内存层次之间的数据搬运中。

在计算和访存到底在指定的硬件和计算负载下谁会成为瓶颈的分析中，我们还可以通过
**[Roofline](https://en.wikipedia.org/wiki/Roofline_model)性能分析模型**进行分析到底当前任务是计算瓶颈还是内存瓶颈。

## 1.3.4 深度学习负载的线性代数(Linear Algebra)计算与缺陷容忍(Defect Tolerance)特性

**线性代数**：大部分的深度学习算子可以抽象为线性代数运算。其较少的控制流，并且大量的矩阵乘等计算让硬件可以通过单指令多数据流(SIMD)进行指令流水线精简设计，并将更多的片上资源用于计算。或者在更高的层次上通过多卡或分布式计算方式进行加速。这种特性的应用我们将在1.3.5中进行总结。同时我们也看到，由于矩阵计算早在几十年前在科学计算与高性能计算(HPC)领域有过大量的成熟研究，在深度学习系统领域也有很多工作会借鉴并优化传统科学计算与高性能计算领域的系统设计与开源组件。

**缺陷容忍**：在学术界, 神经网络芯片在2010年左右开始萌芽。ISCA 2010上，来自法国国立计算机及自动化研究院(INRIA Saclay)的Olivier Temam教授做了["The Rebirth of Neural Networks"](https://pages.saclay.inria.fr/olivier.temam/homepage/ISCA2010web.pdf)的报告，指出了指出ANN（Artificial neural network）的缺陷容忍(Defect Tolerance)特点，并利用这种特点后续开启了一系列的神经网络加速器工作，其中和中科院计算所合作的DianNao系列加速器工作也曾多次获得奖项并商业化。这种缺陷容忍(Defect Tolerance)特点的利用不仅在芯片领域，目前已经拓展到软件层，在计算框架，编译器等部分也常常被作为更为激进优化方式的动机，例如，稀疏，量化，模型压缩等优化。

例如，我们可以在深度学习系统的以下相关领域关注到这类特性假设的应用。
- 硬件层：
  - 通过线性代数特点和缺陷容忍进行模块精简：例如，GPU或针对人工智能的芯片很多精简指令流水线，采用SIMD的模型，提供更多的算力。
  - 稀疏性：在NVIDIA最新的H100 GPU中，提供硬件层对稀疏性计算的原生支持。
  - 量化：在NVIDIA的H100和其他型号GPU中，提供了FP64，FP32，FP16，INT8等不同精度的支持，在准确度允许的范围下，越低精度浮点运算量越大。H100中的Transformer Engine分析输出张量的统计信息，了解接下来会出现哪种类型的神经网络层以及什么它需要的精度，Transformer Engine决定转换哪种目标格式张量到之前将其存储到内存中。
- 软件层：
  - 稀疏：框架和算法可以根据稀疏性在运行时进行优化，不进行非0计算。
  - 量化：训练完成的模型可以通过量化进一步精简数据精度。
  - 模型压缩：训练完成的模型可以通过模型压缩进一步精简模型，降低浮点运算量与内存。
  - 弹性训练：框架在节点失效后，不阻塞，继续训练。例如：[Torch Elastic](https://pytorch.org/docs/stable/elastic/quickstart.html)。

通过以上我们可以看到在深度学习系统中，计算负载的特点本身启发了很多针对深度学习负载本身的系统优化。

## 1.3.5 并行(Parallel)加速与阿姆达尔定律(Amdahl's Law)优化上限

深度学习的训练和推理负载可以在多核与多机的硬件下通过利用负载的并行性(Parallelism)进行加速。并行的设计思路贯穿于整个技术栈，从最底层的指令，到更高层的跨模型多任务并行，我们在系统设计的各个层次上都能找到并行计算的影子。

例如,在以下的深度学习系统的场景中我们都能找到并行加速：
- 加速器内并行：
  - 指令级并行：例如当前针对深度学习的加速器很多都是单指令流多数据流([SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data))的体系结构，能支持指令级并行与流水线。
  - 线程级并行：例如在NVIDIA的技术栈中对线程和束(Warp)都提供了并行支持。
  - 算子内与算子间并行：例如，NVIDIA的技术栈中对CUDA内核(Kernel)级并行CUDA流(Stream)，CUDA块(Block)级并行都有支持。
- 框架数据加载器并行：
  - [并行和流水线化的数据加载器](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)可以加速深度学习的数据读取。
- 框架执行单模型并行：
  - 数据并行(data parallelism)：例如，框架[Horovod](https://github.com/horovod/horovod)将批次切片，部署多副本模型于各个GPU进行数据并行训练。
  - 模型并行(model parallelism)：例如，框架[DeepSpeed](https://github.com/microsoft/DeepSpeed)等将模型切片通过模型并行方式，将计算分布开来。
  - 流水并行(pipeline parallelism)：例如，[GPipe](https://arxiv.org/abs/1811.06965)将模型执行的各个阶段物理划分到不同的单元，采用类[指令流水线](https://en.wikipedia.org/wiki/Instruction_pipelining)的机制加速执行。
- 超参数搜索并行：各个超参数组合的模型之前没有依赖关系可以并行执行。
- 强化学习训练模式并行：多个Agent可以并行执行，模型本身可以并行执行。
- 推理中的并行：内核内与内核间都有较大的并行执行机会。
  
但是并行计算的天花板到底在哪里？如何提前评估并行加速的上限？[阿姆达尔定律](https://en.wikipedia.org/wiki/Amdahl%27s_law)就是为回答以上问题而产生。阿姆达尔定律为：$
S_{latency}(s) = \frac{1}{(1-p) + \frac{p}{s}}
$：其中$S_{latency}$是整个任务执行的理论加速；$s$是从改进的系统资源中受益的部分任务的加速；$p$是受益于改进资源的部分最初占用的执行时间的比例。我们可以看到并行加速受限于串行部分，之后的深度学习中，并行过程中的阶段与阶段间的同步点，聚合运算等都是需要一定串行并无法并行的部分。

通过以上我们可以看到在深度学习系统中，经典的优化方法和理论依然适用。

## 1.3.6 冗余(Redundancy)与可靠性(Dependability) 

虽然深度学习负载提供一定的缺陷容忍的特点。 但是在一些系统层面为保证系统正确执行，不丢失数据或出于调试模型等考虑，也需要设计一定的数据或模型冗余机制进而保证系统的可靠性，那么在深度学习系统的整个技术栈中，常见的一些系统冗余技术实例如下： 

- 硬件层: 错误检查和纠正-ECC(Error Checking and Correcting)，例如NVIDIA GPU中就支持相应的[内存错误管理](https://docs.nvidia.com/deploy/a100-gpu-mem-error-mgmt/index.html#abstract)以保证缺陷修复与检测。
- 框架层：框架的[模型检查点(checkpoint)](https://pytorch.org/tutorials/beginner/saving_loading_models.html)机制，通过备份模型可以让工程师保证系统损坏情况下恢复模型到最近的状态，定期调试训练中的模型。
- 平台层：
  - 元数据：分布式数据库副本机制。例如，深度学习平台的元数据常常存放于Etcd或者Zookeeper中，这些系统本身是通过多副本机制以及协议保证可靠性的。
  - 存储：分布式文件系统副本机制

当然，除了以上一些经典理论，在体系结构，系统，程序分析，软件工程领域还有大量经典理论值得我们学习和借鉴并在人工智能系统中焕发新的生机。例如，最近的一些工作中，[HiveD](https://www.usenix.org/conference/osdi20/presentation/zhao-hanyu)应用经典的[Buddy memory allocation](https://en.wikipedia.org/wiki/Buddy_memory_allocation)思想减少碎片，[Refty](https://www.microsoft.com/en-us/research/publication/refty-refinement-types-for-valid-deep-learning-models/)应用程序分析中的类型系统理论解决深度学习模型缺陷问题等。面向深度学习设计的新的理论与系统设计也将会产生大量的新兴研究与工程实现机会，是一个令人激动人心和值得投身的领域。同时我们也看到，打好计算机基础对从事人工智能系统方向工作与研究至关重要。

综上所述，人工智能系统本身并不是凭空产生，本身继承了大量经典的系统理论与设计方法，并根据深度学习负载的计算，访存与缺陷容忍等特点进一步开掘新的优化机会。我们将在后面的章节中进一步细节的介绍相关场景下的系统设计与实现。

## 小节与讨论

本章我们主要围绕计算机领域的经典理论和原则在深度学习系统场景的应用，同时我们也发现深度学习自身的新的特点，并出现了新的系统设计需求和机会。

请读者读完后面章节后再回看当前章节，并思考随着技术的演进，哪些技术是在变化的，而哪些计算点并没变？

## 参考文献

- [计算机组成与设计：RISC-V](https://www.bilibili.com/video/BV1tz411z7GN?p=1&share_medium=android&share_plat=android&share_session_id=55501f17-936f-4831-8285-7794c1c4c282&share_source=WEIXIN&share_tag=s_i&timestamp=1649549336&unique_k=DxEcILk)
- https://docs.nvidia.com/gameworks/content/developertools/desktop/ptx_sass_assembly_debugging.htm
- https://zh.wikipedia.org/wiki/%E6%91%A9%E5%B0%94%E5%AE%9A%E5%BE%8B
- https://www.tsmc.com/english/news-events/blog-article-20190814 
- http://www.edwardbosworth.com/My5155_Slides/Chapter01/ThePowerWall.htm
- https://docs.nvidia.com/gameworks/content/developertools/desktop/ptx_sass_assembly_debugging.htm
- https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html
- https://www.mindshare.com/files/resources/PLX_Choosing_PCIe_Packet_Payload_Size.pdf