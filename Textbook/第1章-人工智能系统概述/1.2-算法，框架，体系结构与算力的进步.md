<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 1.2 算法，框架，体系结构与算力的进步

催生这轮人工智能热潮的原因有三个重要因素：大数据的积累、超大规模的计算能力支撑、机器学习尤其是深度学习算法都取得了突破性进展，本章我们将围绕以上重要的三方面因素展开。

本小节将围绕以下内容进行介绍：
- [1.2 算法，框架，体系结构与算力的进步](#12-算法框架体系结构与算力的进步)
  - [1.2.1 大数据和分布式系统](#121-大数据和分布式系统)
  - [1.2.2 深度学习算法的进步](#122-深度学习算法的进步)
  - [1.2.3 计算机体系结构和计算能力的进步](#123-计算机体系结构和计算能力的进步)
  - [1.2.4 计算框架的进步](#124-计算框架的进步)
  - [小结与讨论](#小结与讨论)
  - [参考文献](#参考文献)

## 1.2.1 大数据和分布式系统

随着数字化发展，信息系统和平台不断沉淀了大量数据。人工智能的算法是数据驱动（Data Driven）的方式解决问题，从数据中不断学习出规律和模型，进而完成预测任务。
互联网公司由于有海量的用户，大规模的数据中心，信息系统完善，所以可以较早沉淀出大规模数据，并应用人工智能技术，投入研发创新人工智能技术。

互联网服务和大数据平台给深度学习带来了大量的数据集。例如，以下几种服务中沉淀和形成了相应领域代表性的数据集：
- 搜索引擎(Search Engine)
  - 图像检索(Image Search): ImageNet, Coco等计算机视觉数据集
  - 文本检索(Text Search): Wikipedia等自然语言处理数据集
- 商业网站
  - Amazon, Taobao: 推荐系统数据集, 广告数据集
- 其他互联网服务(Internet Services)
  - 对话机器人服务XiaoIce, Siri, Cortana：问答数据集
- ...
  
互联网公司通过不断爬取互联网数据沉淀了大量数据，同时因为其有海量的用户，这些用户不断使用互联网服务，上传文字，图片，音频等数据，又积累了更为丰富的数据。这些数据随着时间的流逝和新业务功能的推出，数据量越来越大，数据模式越来越丰富。所以互联网公司较早的开发和部署了的大数据管理与处理平台。基于这些海量数据，互联网公司通过数据驱动的方式，训练人工智能模型 ，进而优化和提升业务用户体验（例如，点击率预测让用户获取感兴趣的信息），让更多的用户使用服务，进而形成循环。由于天然的随着业务发展遇到更多需要应用人工智能技术的实际场景和需求，相较于学术界，互联网公司作为工业界的代表，较早地将深度学习的发展推到了更加实用，落地的阶段，并不断投入研发推动人工智能算法与系统的不断演进和发展。

以图中为例，同样是图像分类问题，从最开始数据规模较小的MNIST手写数字识别数据集，到更大规模的ImageNet，再到互联网Web服务中沉淀了数亿量级的图像数据：

<style>table{margin: auto;}</style>
|MNIST|ImageNet|Web Images|
|---|---|---|
|6万样本|1600万样本|10亿量级图像样本|
|10分类|1000分类|开放分类|

<center>表1-2-1. 不同图像分类问题数据集的数据量</center>

这些海量的数据集为深度学习系统的发展产生了以下的影响：
- 推动深度学习算法不断在指定任务上产生更高的准确度与更低的误差。让深度学习有更广泛的应用，进而产生商业价值，让工业界和学术界看到其应用潜力并投入更多资源进行研究。这样产生了针对深度学习的系统与硬件发展的用户基础，应用落地场景驱动力和研发资源投入。
- 海量的数据集让单机越来越难以完成深度学习模型的训练，进而产生了分布式训练和平台的需求，让传统的机器学习库不能满足相应的需求。
- 多样的数据格式和任务，产生了模型结构的复杂性，驱动框架或针对深度学习的程序语言需要有更灵活的表达能力对问题进行表达与映射。 
- 同时伴随着性能等需求得到满足，数据安全与模型安全问题挑战也变的日益突出。

综上所述，深度学习系统本身的设计相较于传统机器学习库有更多样的表达需求，更大规模和多样的数据集和更广泛的用户基础。

## 1.2.2 深度学习算法的进步

除了数据本身不断的沉淀，算法研究员和工程师不断设计新的算法和模型提升预测效果，深度学习算法和模型的预测效果不断取得突破性进展。但是新的算法和模型结构需要前端框架提供编程的表达力和灵活性，对执行层系统优化有可能会改变原有假设，进而产生了系统前端设计和执行过程优化新的挑战。

借下来我们从几个代表性数据集上看算法与模型的进步：

***1. 深度学习在已有数据集（MNIST数据集）上超越机器学习算法***

[MNIST](http://yann.lecun.com/exdb/mnist/)手写数字识别数据库是一个大型手写数字图像数据集，在早期通常用于训练和研究各种图像分类的深度学习模型，由于其样本与数据规模较小，当前也常常用于教学或神经网络结构(NAS)搜索的研究。

我们可以观察图1-2-1，了解不同的机器学习算法取得的效果。

<center><img src="./img/2/2-3-5-mnist.png" ch="500" /></center>
<center>图1-2-1. MNIST数据集上各算法的Test Error (%)</center>

从图中可以观察到这样的趋势：

1998年，一个简单的卷积神经网络可以取得和SVM取得的最好效果接近。

2012年，一个深度卷积神经网络可以将错误率降低到0.23% (2012), 这样的结果已经可以和人所达到的错误率0.2%非常接近。

深度学习模型在MNIST数据集上的表现，让研究者们看到了深度学习模型提升预测效果的潜力，进而不断尝试新的深度学习模型和更复杂的数据集上进行验证。


***2. 深度学习在公开数据集（ImageNet）上取得不断的突破***


随着每年ImageNet数据集上的新模型取得突破，我们看到新的深度学习模型结构和训练方式的潜力。通过图1-2-2，我们观察到，更深的模型结构有潜力提升当前预测的效果。

<center><img src="./img/2/2-3-4-dl-imagenet-improve.png" ch="500" /></center>
<center>图1-2-2. 更深和高效的深度学习模型结构在ImageNet数据集上的效果不断取得突破</center>

我们可以观察到，新的模型不断在以下方面演化进而提升效果：
- 更好的激活函数和层: ReLU, BatchNormalization等。
- 更复杂更深的网络结构和更多的模型权重。 
- 更好的训练技巧: 正则化（Regularization），初始化（Initialization），学习方法（Learning methods）等。

这些可以取得更好的效果的技巧和设计，驱动者算法工程师与研究员不断投入设计新的模型，同时也要求深度学习系统不断提供新的算子（Operator）支持，算子优化，训练算法支持，进而驱动框架和编译器对前端，中间表达，和系统算法协同设计的演进和发展。

***3. 其他领域算法的进步***

除了我们看到的计算机视觉，深度学习在多个领域也取得了不俗的表现，并在当年取得超越原有方案的里程碑式的效果，后续方案不断在深度学习的方法上取得新的突破。

例如，在下面的领域中相关代表性工作： 

- 计算机视觉（Computer Vision）领域
  - 例如：2015年, ImageNet数据集上MSRA研发的Resnet取得了5项第一，并又一次刷新了CNN模型在ImageNet上的历史。
  
- 自然语言处理（Natural Language Processing）领域
  - 例如：2019年,在斯坦福大学举办的SQuAD （Stanford Question Answering Dataset）和 CoQA（Conversational Question Answering）挑战赛中，MSRA的NLP团队通过多阶段(Multi-Stage)，多任务(Multi-Task)学习的方式取得第一。

- 语音识别（Speech Recognition）领域
  - 例如：2016年，MSR提出的Combined模型系统的在NIST 2000数据集上错误率为6.2%，超越之前报告的基准测试结果。

- 强化学习（Reinforcement Learning）领域
  - 2016年, Google DeepMind研发的AlphaGo在围棋比赛中以4:1的高分击败了世界大师级冠军李世石。OpenAI 训练出了名为 OpenAI Five 的 Dota 2 游戏智能体。2019 年 4 月，OpenAI Five 击败了一支 Dota 2 世界冠军战队，这是首个击败电子竞技游戏世界冠军的人工智能系统。

由于不同领域的输入数据格式不同，预测输出结果不同，数据获取方式不同，造成模型结构和训练方式产生非常多样的需求，各家公司和组织不断研发新的针对特定领域的框架或上层接口封装以支持特定领域数据科学家快速验证和实现新的想法，工程化部署和批量训练成熟的模型。所以我们可以看到，由最开始AlexNet是作者直接通过[CUDA](https://code.google.com/archive/p/cuda-convnet)实现深度学习模型，到目前有通过Python语言灵活和轻松调用的框架，到大家习惯使用Hugging Face进行神经网络语言模型训练，背后是系统工程师贴合实际需求不断研发新的工具，并推动深度学习生产力提升的结果。所以即使作为系统工程师，也需要密切关注算法和应用的演进，才能紧跟潮流设计出贴合应用实际的工具与系统。

## 1.2.3 计算机体系结构和计算能力的进步

<center><img src="./img/2/2-3-1-arch-improve.png" ch="500" /></center>
<center>图1-2-3. 计算机体系结构和计算能力的进步</center>

从1960年以来，计算机性能的增长主要来自摩尔定律，到二十世纪初大概增长了10的8次方倍。
但是由于摩尔定律的停滞，性能的增长逐渐放缓了。单纯靠工艺尺寸的进步，无法满足各种应用对性能的要求。

于是，人们就开始为应用定制硬件，通过消除通用处理器中冗余的功能部分，来进一步提高对特定应用的计算性能。
比如，图形图像处理器，GPU就对图像类算法做专用加速。后来出现GPGPU，也就是通用GPU，对适合于抽象为单指令流多数据流（SIMD）的高并行算法与工作负载都能起到不错的加速效果。

为了更高的性能，这些年人工智能芯片也大行其道。其中一个代表就是TPU。通过对矩阵乘法定制硬件，进一步提高了性能。通过定制化硬件，我们又将处理器性能提升了大约10的5次方量级。

然而可惜的是，经过这么多年的发展，虽然处理器性能提升这么多，我们机器的数值运算能力早已是人类望尘莫及了，里面的程序仍然是人类指定的固定代码，智能程度还远远不及生物大脑。从智力程度来说，大约也就只相当于啮齿动物，距离人类还有一定距离。

我们可以看到随着硬件的发展，虽然算力逐渐逼近人脑，让深度学习取得了突破。同时我么也看到，计算力也可能在短期内成为瓶颈，那么人工智能系统的性能下一代的出路在哪？

所以我们在后面会看到，除了单独芯片的不断迭代进行性能放大(Scale up)，系统工程师不断设计更好的分布式计算系统将计算并行开来达到向外扩展(Scale out)，同时发掘深度学习的作业特点，如稀疏性等通过算法，系统硬件协同设计，进一步提升计算效率和性能。


## 1.2.4 计算框架的进步

算法工程师和研究员为了实现深度学习模型，完成训练，并部署执行，抛开其他需求，这其中都离不开深度学习框架的支持，例如：PyTorch，TensorFlow等。
计算框架对用户提供编程接口，隐藏硬件细节，同时将用户书写的深度学习程序进行编译优化并部署在专用硬件上进行执行。其中计算框架属于深度学习系统中的核心，构建了算法工程师和底层硬件之间的桥梁。通过业界的开源社区发展和学术研究进展，我们观察到，深度学习框架大致经过以下的发展脉络。

**第一代框架**：

以[Caffe](https://caffe.berkeleyvision.org/)为代表的第一代框架，其编程范式为通过配置文件进行模型构建，框架将模型翻译成粗粒度的算子（例如，卷积层，池化层），并调用底层硬件提供的优化算子库（如NVIDIA cuDNN，CUDA）等进行高效执行。其特点是简单构建方便，但是灵活性不足，用户容易写出错误的程序，同时底层优化依赖硬件厂商库，框架对运行时调度没有过多优化机会。

**第二代框架**：

以[TenorFlow](https://github.com/tensorflow/tensorflow)和[PyTorch](https://github.com/pytorch/pytorch)为代表的第二代框架，目前是有最为广泛用户基础的计算框架。其中通常业界将框架按照编程范式分类两类：

- 声明式编程(Declarative programming)
  - 代表性框架：TensorFlow, Keras, CNTK, Caffe2
  - 特点：用户只需要表达模型结构和需要执行的任务，无需关注底层的执行流程，框架提供计算图优化，让用户无需关心底层优化细节，但是对用户来说不容易调试。

- 命令式编程(Imperative programming）
  - 代表性框架：PyTorch, Chainer, DyNet
  - 特点：用户不仅表达模型结构，还需要表达执行步骤，并且按照每一步定义进行执行，由于无法像声明式编程获取完整计算图并优化后执行，所以难以提供全面的计算图优化，但是由于其简单易用，灵活性高，在模型研究人员中也有很高的用户基础，并不断在新的研究工作中被广泛使用，从而打下增长势头较好且广泛的用户基础。

<center><img src="./img/2/2-3-2-framework1to2.png" ch="500"></center>
<center>图1-2-2. 第一代框架到第二代框架的进步</center>

但是虽然框架解决了大部分的问题，同时我们也看到第二代框架以Python语言作为前端语言，并结合使用Numpy, Scipy等数据处理库构建深度学习的程序，但是我们也可以看到，数据预处理等其他阶段的执行与深度学习模型执行阶段的库与协同优化的割裂，除深度学习框架之外的库无法利用GPU等专有硬件等现实问题也产生了很多挑战。
同时Python语言其特点是简单，但是不利于优化与错误检测。
由于性能，并发，部署和跨语言调用等问题在不断演化的深度学习研究与工程化对性能和稳定性越来越极致要求的趋势下捉襟见肘。目前也有趋势是提供静态语言前端（例如，Swift等），后端提供编译器（例如，TVM,TensorFlow XLA等）进行优化尝试规避和解决当前框架已有的问题。

**第三代框架**:

我们除了设计框架解决当前的问题，还应该思考关注和设计下一代的框架支持未来的模型趋势。

<center><img src="./img/2/2-3-3-framework-2to3.png" ch="500"></center>
<center>图1-2-3. 第二代框架到第三代框架的发展趋势</center>

- 框架应在有更加全面功能的编程语言前端下构建，并提供灵活性和表达力，例如：控制流的支持，递归和稀疏性的原生表达与支持。这样才能应对大的（Large）、动态（Dynamic）的和自我修改（Self-Modifying）的深度学习模型趋势。我们无法准确预估深度学习模型在多年后会是什么样子，但从现在的趋势看，它们将会更大、更稀疏、结构更松散。下一代框架应该更好地支持像[Pathways](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/)模型这样的动态模型，像预训练模型或专家混合模型这样的大型模型，以及需要与真实或模拟环境频繁交互的强化学习模型等多样的需求。

- 框架同时应该不断跟进并提供针对多样且新的硬件特性下的编译优化与运行时调度的优化支持。例如：SIMD (单指令流多数据流)到MIMD（多指令流多数据流）的支持，稀疏性的硬件内优化，分布式计算等。这样才能利用张量计算单元、稀疏性和混合精度支持，SoW（System on Wafer）和大规模训练集群等专有硬件趋势。
  
## 小结与讨论

本章我们主要围绕深度学习系统的算法，框架与体系结构展开，对系统研究，除了理解上层深度学习算法，也需要理解底层的体系结构，并利用两者之前的巨大的优化空间进行设计和权衡。

请读者思考未来的深度学习框架和系统应该是怎样的？

## 参考文献

- Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caffe: Convolutional Architecture for Fast Feature Embedding. In <i>Proceedings of the 22nd ACM international conference on Multimedia</i> (<i>MM '14</i>). Association for Computing Machinery, New York, NY, USA, 675–678. DOI:https://doi.org/10.1145/2647868.2654889
- Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo,
Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis,
Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow,
Andrew Harp, Geoffrey Irving, Michael Isard, Rafal Jozefowicz, Yangqing Jia,
Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Mike Schuster,
Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Jonathon Shlens,
Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker,
Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas,
Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke,
Yuan Yu, and Xiaoqiang Zheng.
TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org.
- Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Köpf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., & Chintala, S. (2019). PyTorch: An Imperative Style, High-Performance Deep Learning Library. ArXiv, abs/1912.01703.
- Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch, Naveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon. 2017. In-Datacenter Performance Analysis of a Tensor Processing Unit. In <i>Proceedings of the 44th Annual International Symposium on Computer Architecture</i> (<i>ISCA '17</i>). Association for Computing Machinery, New York, NY, USA, 1–12. DOI:https://doi.org/10.1145/3079856.3080246
- https://github.com/BradLarson/swift/blob/main/docs/WhySwiftForTensorFlow.md
- Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., Xiao, T., Xu, B., Zhang, C., & Zhang, Z. (2015). MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems. ArXiv, abs/1512.01274.
- Neubig, G., Dyer, C., Goldberg, Y., Matthews, A., Ammar, W., Anastasopoulos, A., Ballesteros, M., Chiang, D., Clothiaux, D., Cohn, T., Duh, K., Faruqui, M., Gan, C., Garrette, D., Ji, Y., Kong, L., Kuncoro, A., Kumar, M., Malaviya, C., Michel, P., Oda, Y., Richardson, M., Saphra, N., Swayamdipta, S., & Yin, P. (2017). DyNet: The Dynamic Neural Network Toolkit. ArXiv, abs/1701.03980.
- Tokui, S., Okuta, R., Akiba, T., Niitani, Y., Ogawa, T., Saito, S., Suzuki, S., Uenishi, K., Vogel, B.K., & Vincent, H.Y. (2019). Chainer: A Deep Learning Framework for Accelerating the Research Cycle. Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining.
- Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition (pp. 248–255).
- Frank Seide and Amit Agarwal. 2016. CNTK: Microsoft's Open-Source Deep-Learning Toolkit. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '16). Association for Computing Machinery, New York, NY, USA, 2135. DOI:https://doi.org/10.1145/2939672.2945397
- Deng, L. (2012). The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6), 141–142.
- Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Meghan Cowan, Haichen Shen, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. 2018. TVM: an automated end-to-end optimizing compiler for deep learning. In Proceedings of the 13th USENIX conference on Operating Systems Design and Implementation (OSDI'18). USENIX Association, USA, 579–594.
- https://www.tensorflow.org/xla
