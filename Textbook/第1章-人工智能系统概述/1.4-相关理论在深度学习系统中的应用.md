<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 1.4 相关理论在深度学习系统中的应用

人工智能系统内容保罗万象，涵盖传统计算机体系结构，编译器，操作系统，计算机网络的经典的应用与拓展。同时我们也可以观察到经典的计算机系统相关理论和系统优化方法在深度学习中依然在发挥巨大的作用，我们依然可以将当前很多问题映射和抽象并通过经典理论所解决。那么在展开后面内容之前，我们通过几个代表性理论和在深度学习系统中的应用，以及深度学习作业的独有特点所产生的新问题展开，启发之后我们在学习具体系统问题时形成理论指导。

- [1.4 相关理论在深度学习系统中的应用](#14-相关理论在深度学习系统中的应用)
  - [1.3.1 抽象-层次化表示与解释](#131-抽象-层次化表示与解释)
  - [1.3.2 摩尔定律(Moore's Law)-算力发展趋势](#132-摩尔定律moores-law-算力发展趋势)
  - [1.3.3 局部性(Locality)原则-内存层次结构(Memory Hierarchy)](#133-局部性locality原则-内存层次结构memory-hierarchy)
  - [1.3.4 深度学习中的线性代数(Linear Algebra)计算与缺陷容忍(defect tolerance)](#134-深度学习中的线性代数linear-algebra计算与缺陷容忍defect-tolerance)
  - [1.3.5 并行与阿姆达尔定律(Amdahl's Law)-优化上限](#135-并行与阿姆达尔定律amdahls-law-优化上限)
  - [1.3.6 冗余(Redundancy)与可靠性(Dependability)](#136-冗余redundancy与可靠性dependability)
  - [参考文献](#参考文献)

## 1.3.1 抽象-层次化表示与解释

- 语言：Python
- 驱动编程库：CUDA
- 汇编指令：PTX, SASS
- 机器码
- 硬件执行单元

## 1.3.2 摩尔定律(Moore's Law)-算力发展趋势

摩尔定律(Moore's law)是由英特尔（Intel）创始人之一戈登·摩尔提出的。集成电路上可容纳的晶体管数目，约每隔两年便会增加一倍。而英特尔首席执行官大卫·豪斯（David House）提出且经常被引用的是预计18个月会将芯片的性能提高一倍（即更多的晶体管使其更快）。

在GPU领域，[黄氏定律](https://en.wikipedia.org/wiki/Huang%27s_law#:~:text=Moores%20law%20would%20predict%20a,the%20new%20'law'%20possible.)是英伟达创始人黄仁勋提出，即图形处理单元 (GPU) 的发展速度比传统中央处理单元 (CPU) 的发展速度要快得多。黄氏定律指出，GPU 的性能每两年将翻一番以上。

有多种论调在讨论摩尔定律已死，当然也有一些论调认为摩尔定律还存在，只不过是需要以另一种方式来理解，例如台积电企业研究副总裁Philip Wong 博士在Hotchips 2019上提出摩尔定律未死([Moore's Law is not Dead](https://www.youtube.com/watch?v=O5UQ5OGOsnM))，其出发点在于“处理器速度时钟速度已经饱和并不意味着摩尔定律已经失效，就像摩尔博士很多很多年前预测的那样，密度（例如，晶体管，逻辑门，SRAM等）不断增加是摩尔定律持续存在的驱动力”。当时还在Intel的Jim Keller也层做过"Moore’s Law is Not Dead"的演讲，并提出：“要理解计算领域的这种不减增长，我们需要将摩尔定律晶体管数量指数解构为计算堆栈中众多独立创新的输出——在硅工艺技术、集成电路设计、微处理器架构和软件方面。虽然晶体管性能和功率等某些向量的收益确实在递减，但晶体管架构、微处理器架构、软件和新材料等其他向量的收益却在增加。”所以我们也看到软件系统也扮演越来越重要的角色，软硬件协同设计可以进一步挖掘硬件性能。

随着芯片不断提升算力，但是我们发现系统性能本身还会受到其他短板部分或者约束所限制。例如：

- [功耗墙](http://www.edwardbosworth.com/My5155_Slides/Chapter01/ThePowerWall.htm)约束
  - 1990 年代末和2000年代初的设计目标是提高时钟频率，这是通过在更小的芯片上添加更多晶体管来实现的。不幸的是，这增加了CPU芯片的功耗超出了廉价冷却技术的能力。所以这种约束让工业界有两种路线，一种是采用更复杂的冷却技术，一种是转向多核设计。我们目前看到的针对人工智能设计的芯片也都是多核或者是众核的。同时我们在NVIDIA GPU中通过nvidia-smi命令也可以看到，当GPU的温度超出一定阈值,GPU会减速或者关闭。
- [内存墙](https://en.wikipedia.org/wiki/Random-access_memory#Memory_wall)约束
  - [“内存墙”](https://en.wikipedia.org/wiki/Random-access_memory#Memory_wall)是芯片与芯片外的内存之间越来越大的速度差距。造成这种差距的一个重要原因是超出芯片边界的有限通信带宽，也称为带宽墙。例如，从 1986 年到 2000 年，CPU 速度以每年 55% 的速度提高，而内存速度仅提高了 10%。鉴于这些趋势，预计内存延迟将成为计算机性能的压倒性瓶颈。这种现状其实我们在GPU的发展过程中也可以观察到，例如，NVIDIA H100相比A100在FP32 Vector上是60TFLOPS相比19.5TFLOPS的近3倍提升，但是内存带宽只是3TB/sec比2TB/sec的1.5倍提升，访存落后于计算的提升速度。可以认为在人工智能领域内存墙这种情况还存在。

在了解以上趋势后，会启发我们看到为何后面大量的系统设计优化，利用多核和分布式计算，以及为何减少数据搬运的原因，因为单纯依靠硬件提升是有一定天花板，我们还需要协同通过系统设计进一步提升效率和性能。

## 1.3.3 局部性(Locality)原则-内存层次结构(Memory Hierarchy)

深度学习的访存特点是：
- 对模型整体的每轮迭代，读取批次数据，这部分会随机采样数据进行读取。
- 对模型中的每个算子计算可以转换为底层的循环执行，这其中对输入输出数据的访问有一定的缓存复用机会。

那么我们可以总结各个内存层级结构中的利用局部性的例子和机会：
- GPU缓存
  - 片上缓存较小，对算子内计算负载进行切片以及访存调度优化是减少缓存失效是常见思路。深度学习编译器关注循环(loop)的(tile)优化很大程度上因为缓存较小引起。
  - 缓存预取指令设计(pre-fetch)：例如在NVIDIA GPU的PTX指令中，提供缓存预取指令，此例子通过预取指令加载数据到L2缓存["```ld.global.L2::64B.b32   %r0, [gbl]; // Prefetch 64B to L2```"](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html)。
- GPU显存
  - GPU显存和主存之间通常传输的是批次(Batch)数据和模型。可以利用深度学习模型对批次数据的访问时间局部性做相应优化，例如可以通过[vDNN](https://dl.acm.org/doi/10.5555/3195638.3195660)等方式对假设已知访存模式情况下的数据预取与卸载。
- 主存
  - 主存和文件系统之间一般传输的是数据，和模型文件，可以提前下载和准备好数据文件，并做好缓存。
  - 对频繁访问的磁盘数据，也可以将缓存逻辑交给操作系统管理。
- 本机磁盘存储。
  - 高效和统一的顺序文件存储格式能够减少随机读写小文件的问题。
  - 本地磁盘存储也可以充当云存储的缓存，例如，[Azure Blob Fuse](https://github.com/Azure/azure-storage-fuse)。
- 网络与云存储
  - 缓存文件系统中数据或者预取数据可以减少网络或云存储中文件的读取代价，例如：[Alluxio](https://www.alluxio.io/)。


在计算和访存到底在指定的硬件和计算负载下谁会成为瓶颈的分析中，我们还可以通过
**[Roofline](https://en.wikipedia.org/wiki/Roofline_model)性能分析模型**进行分析到底当前任务是计算瓶颈还是内存瓶颈。

## 1.3.4 深度学习中的线性代数(Linear Algebra)计算与缺陷容忍(defect tolerance)

**线性代数**：大部分的深度学习算子可以抽象为线性代数运算。其较少的控制流，并且大量的矩阵乘等计算让硬件可以通过单指令多数据流(SIMD)进行指令流水线精简设计，并将更多的片上资源用于计算。或者在更高的层次上通过多卡或分布式计算方式进行加速。这种特性的应用我们将在1.3.5中进行总结。同时我们也看到，由于矩阵计算早在几十年前在科学计算与高性能计算(HPC)领域有过大量的成熟研究，在深度学习系统领域也有很多工作会借鉴并优化传统科学计算与高性能计算领域的系统设计与开源组件。

**缺陷容忍**：在学术界, 神经网络芯片在2010年左右开始萌芽。ISCA 2010上，来自法国国立计算机及自动化研究院(INRIA Saclay)的Olivier Temam教授做了["The Rebirth of Neural Networks"](https://pages.saclay.inria.fr/olivier.temam/homepage/ISCA2010web.pdf)的报告。在此次报告中，Olivier Temam指出了1990s像Intel和Philips等构建硬件神经网络商用系统的应用场景局限性，SVM在当时有一段时间处于应用负载的主导，造成其无法应对机器学习和神经网络的发展，大规模并行计算打破单机算力局限造成这种风潮没有成功。指出ANN（Artificial neural network）的缺陷容忍(defect tolerance)特点，和深度网络的趋势，指出神经网络加速器设计的方向。之后，在12年的ISCA上，Olivier Temam提出AI加速器的设计["A defect-tolerant accelerator for emerging high-performance applications"](https://dl.acm.org/doi/10.1145/2366231.2337200)，利用ANN的缺陷容忍特性，提出空间扩展网络(spatially expanded network)相比时分复用(time-multiplexed)架构的在缺陷容忍(defect tolerance)，提升能效的优势，并在工作中评估了这个工作的缺陷容忍效果。
这种缺陷容忍特定的利用不仅在芯片领域，目前已经拓展到软件层，在计算框架，编译器等部分也常常被作为更为激进优化方式的动机。

例如，我们可以在深度学习系统的以下相关领域关注到这类特性的应用。
- 硬件层
  - 通过线性代数特点和缺陷容忍进行模块精简：例如，GPU或针对人工智能的芯片很多精简指令流水线，采用SIMD的模型，提供更多的算力。
  - 稀疏性：在NVIDIA最新的H100 GPU中，提供硬件层对稀疏性计算的原生支持。
  - 量化：在NVIDIA的H100和其他型号GPU中，提供了FP64，FP32，FP16，INT8等不同精度的支持，在准确度允许的范围下，越低精度浮点运算量越大。H100中的Transformer Engine分析输出张量的统计信息，了解接下来会出现哪种类型的神经网络层以及什么它需要的精度，Transformer Engine决定转换哪种目标格式张量到之前将其存储到内存中。
- 软件层
  - 稀疏：框架和算法可以根据稀疏性在运行时进行优化，不进行非0计算。
  - 量化：训练完成的模型可以通过量化进一步精简数据精度。
  - 模型压缩：训练完成的模型可以通过模型压缩进一步精简模型，降低浮点运算量与内存。
  - 弹性训练：框架在节点失效后，不阻塞，继续训练。例如：[Torch Elastic](https://pytorch.org/docs/stable/elastic/quickstart.html)。

通过以上我们可以看到在深度学习系统中，计算负载的特点本身启发了很多针对深度学习负载本身的系统优化。

## 1.3.5 并行与阿姆达尔定律(Amdahl's Law)-优化上限

深度学习的训练和推理负载可以在多核与多机的硬件下通过利用负载的并行性(Parallelism)进行加速。并行的设计思路贯穿于整个技术栈，从最底层的指令，到更高层的跨模型多任务并行，我们在系统设计的各个层次上都能找到并行计算的影子。

但是并行计算的天花板到底在哪里？如何提前评估并行加速的上限？阿姆达尔定律就是为回答以上问题而产生。

例如,在以下的深度学习系统的场景中我们都能找到并行加速：
- 硬件
  - 指令级并行：例如当前针对深度学习的加速器很多都是单指令流多数据流([SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data))的体系结构，能支持指令级并行与流水线。
  - 线程级并行：例如在NVIDIA的技术栈中对线程和束(Warp)都提供了并行支持。
  - 算子内与算子间并行：例如，NVIDIA的技术栈中对CUDA内核(Kernel)级并行CUDA Stream，CUDA块(Block)级并行都有支持。
- 框架数据加载器
  - [并行和流水线化的数据加载器](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)可以加速深度学习的数据读取。
- 框架执行单模型并行策略
  - 数据并行(data parallelism)：例如，框架[Horovod](https://github.com/horovod/horovod)将批次切片，部署多副本模型于各个GPU进行数据并行训练。
  - 模型并行(model parallelism)：例如，框架[DeepSpeed](https://github.com/microsoft/DeepSpeed)等将模型切片通过模型并行方式，将计算分布开来。
  - 流水并行(pipeline parallelism)：例如，[GPipe](https://arxiv.org/abs/1811.06965)将模型执行的各个阶段物理划分到不同的单元，采用类[指令流水线](https://en.wikipedia.org/wiki/Instruction_pipelining)的机制加速执行。
- 超参数搜索并行
- 强化学习训练模式并行
- 推理中的并行
  
通过以上我们可以看到在深度学习系统中，经典的优化方法和理论依然适用。

## 1.3.6 冗余(Redundancy)与可靠性(Dependability) 

虽然深度学习负载提供一定的缺陷容忍的特点。 但是在一些系统层面为保证系统正确执行，不丢失数据或出于调试模型等考虑，也需要设计一定的数据或模型冗余机制进而保证系统的可靠性，那么在深度学习系统的整个技术栈中，常见的一些系统冗余技术实例如下： 

- 硬件层: 指令纠错技术ECC(Error Checking and Correcting)，例如NVIDIA GPU中就支持相应的[内存错误管理](https://docs.nvidia.com/deploy/a100-gpu-mem-error-mgmt/index.html#abstract)以保证缺陷修复与检测。
- 框架层：框架的[模型检查点(checkpoint)](https://pytorch.org/tutorials/beginner/saving_loading_models.html)机制，通过备份模型可以让工程师保证系统损坏情况下恢复模型到最近的状态，定期调试训练中的模型。
- 平台层：
  - 元数据：分布式数据库副本机制。例如，深度学习平台的元数据常常存放于Etcd或者Zookeeper中，这些系统本身是通过多副本机制以及协议保证可靠性的。
  - 存储：分布式文件系统副本机制

当然，除了以上一些经典理论，在体系结构，系统，程序分析，软件工程领域还有大量经典理论值得我们学习和借鉴并在人工智能系统中焕发新的生机。例如，最近的一些工作中，[HiveD](https://www.usenix.org/conference/osdi20/presentation/zhao-hanyu)应用经典的[Buddy memory allocation](https://en.wikipedia.org/wiki/Buddy_memory_allocation)思想减少碎片，[Refty](https://www.microsoft.com/en-us/research/publication/refty-refinement-types-for-valid-deep-learning-models/)应用程序分析中的类型系统理论解决深度学习模型缺陷问题等。面向深度学习设计的新的理论与系统设计也将会产生大量的新兴研究与工程实现机会，是一个令人激动人心和值得投身的领域。同时我们也看到，打好计算机基础对从事人工智能系统方向工作与研究至关重要。

综上所述，人工智能系统本身并不是凭空产生，本身继承了大量经典的系统理论与设计方法，并根据深度学习负载的计算，访存与缺陷容忍等特点进一步开掘新的优化机会。我们将在后面的章节中进一步细节的介绍相关场景下的系统设计与实现。



## 参考文献

- [计算机组成与设计：RISC-V](https://www.bilibili.com/video/BV1tz411z7GN?p=1&share_medium=android&share_plat=android&share_session_id=55501f17-936f-4831-8285-7794c1c4c282&share_source=WEIXIN&share_tag=s_i&timestamp=1649549336&unique_k=DxEcILk)
- https://docs.nvidia.com/gameworks/content/developertools/desktop/ptx_sass_assembly_debugging.htm
- https://zh.wikipedia.org/wiki/%E6%91%A9%E5%B0%94%E5%AE%9A%E5%BE%8B
- https://www.tsmc.com/english/news-events/blog-article-20190814 
- http://www.edwardbosworth.com/My5155_Slides/Chapter01/ThePowerWall.htm