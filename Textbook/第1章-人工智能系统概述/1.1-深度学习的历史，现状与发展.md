<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/YanjieGao/AI-System/blob/main/LICENSE)版权许可-->

# 1.1 深度学习的历史，现状与发展

本章将介绍深度学习的由来，现状和趋势，让读者能够了解系统之上的深度学习负载的由来与趋势，为后面理解深度学习系统的设计和权衡形成初步的基础。

- [1.1 深度学习的历史，现状与发展](#11-深度学习的历史现状与发展)
  - [1.1.1 深度学习正在改变世界](#111-深度学习正在改变世界)
  - [1.1.2 深度学习方法](#112-深度学习方法)
  - [1.1.3 神经网络的基本理论在深度学习前已基本奠定](#113-神经网络的基本理论在深度学习前已基本奠定)
  - [1.1.4 深度学习模型现状和趋势](#114-深度学习模型现状和趋势)
  - [参考文献](#参考文献)


## 1.1.1 深度学习正在改变世界

人工智能起源于上世纪五十年代，经历了几次繁荣与低谷。直到2016年谷歌所收购的DeepMind公司的AlphaGo赢得与世界围棋冠军的比赛，大众对人工智能的热情被重新点燃。

我们在媒体中经常看到词汇，人工智能，机器学习和深度学习，那么他们之间的关系是什么？我们可以认为机器学习是实现人工智能的一种方法，而深度学习是一种实现机器学习的技术。我们在之后的内容中主要介绍的是围绕深度学习而衍生和设计的系统，因为深度学习目前是人工智能中应用最为广泛，前沿，系统设计挑战最大的方向。我们在之后也会穿插使用人工智能泛指深度学习。

人工智能逐渐在互联网，安防、医疗、金融等不同领域有大范围的应用。人工智能并不是一个独立的技术，而是结合各个行业的多样性与大规模的数据储备，通过数据驱动的方式应用到各个具体任务（例如，人脸识别，物体检测等）中的一系列技术。

<img src="./img/1/1-2-1-dl-change-world.png" ch="500" />
<center>图1-1-1. 深度学习正改变世界 </center>

在以下为例的行业中已经有越来越多的任务使用人工智能技术提升效果：

- 互联网 
  - 谷歌、百度、微软Bing等公司通过人工智能技术进行更好的文本向量化，提升检索质量，同时人工智能进行点击率预测，获取更高的利润。
- 医疗 
  - IBM Watson从海量的医学文献和病历中提取医生临床诊断经验，通过让人工智能模型学习掌握临床诊断方法，辅助医生进行诊断。
- 安防
  - 通过人脸识别，物体检测等技术，可以提升传统安防场景的检测精度。
- 自动驾驶
  - 通过人工智能能够进行更好的路标检测，道路线检测进而增强自动驾驶方案。
- 游戏
  - 在游戏中我们可以通过增加学习技术进行对战，设计新的策略，提升游戏体验。




## 1.1.2 深度学习方法 

我们将以图中的实例介绍深度学习是如何工作的。
<img src="./img/1/1-2-1-dl-method.png" ch="500" />
<center>图1-1-1. 深度学习方法 </center>

我们假定读者有一定机器学习经验，其中的一些概念我们不再过多解释。

我们将深度神经网络的工作模式抽象为以下几个步骤：

- 确定问题输入与输出：图中所示，本问题我们给深度学习模型输入图片（例如，图片中有狗，猫等），输出是图片的类别。用户需要提前准备好模型的输入输出数据，进而展开后续的模型训练。
- 开发模型结构：开发者通过编程框架开发了图中图中的模型结构，绿色线代表权重与白色圆代表的输入数据发生乘法操作。其中的$w_n$代表权重，也就是可以被学习和不断更新的数值。
- 训练过程：如图中上半部分所示，训练过程就是根据用户给定的带有标签(例如图中的Cat，Dog等输出标签)的数据集，不断通过梯度下降算法，以下面的流程学习出最优的模型权重$w_n$的取值。
  - 前向传播：由输入到输出完成整个模型中各个层的矩阵计算，产生输出并完成损失函数计算。
  - 反向传播：由输出到输入反向完成整个模型中各个层的权重和输出对损失函数的梯度求解。
  - 梯度更新：对模型权重通过梯度下降法完成更新，不断重复以上步骤，直到达到模型收敛或达到终止条件。
- 推理过程
  - 前向传播：如图中下半部分所示，由输入到输出完成整个模型中各个层的矩阵计算，产生输出。例如本例中输入是狗的图片，输出的结果为向量，向量中的各个维度编码了图像的类别可能性，其中够的类别概率最大，判定为狗。

后面章节将要介绍的深度学习系统，就是围绕其中的各个环节，提供良好的开发体验，极致的执行性能，和更大规模的数据与模型的训练和推理。
  
## 1.1.3 神经网络的基本理论在深度学习前已基本奠定

<img src="./img/1/1-3-1-history.jpg" ch="500" />
<center>图1-1-2. 神经网络的基本理论与发展 (图片引用自互联网)</center>

神经网络作为深度学习的前身，经历了以下的发展阶段：

1943年，神经科学家和控制论专家Warren McCulloch和逻辑学家Walter Pitts基于数学和阈值逻辑算法创造了一种神经网络计算模型。并发表文章"A Logical Calculus of the ideas Imminent in Nervous Activity"。

1957年，Frank Rosenblat发明感知机(Perception)。奠定了之后深度学习的基本结构，其计算以矩阵乘加运算为主，进而奠定了后续人工智能芯片和系统的基本算子。

1960年，Widrow和Hoff发明了Adaline/Madaline,首次尝试把线性层叠加整合为多层感知器网络。感知器本质上是一种线性模型，可以对输入的训练集数据进行二分类，且能够在训练集中自动更新权值，其奠定了后续深度学习模型的通用模式。感知器的提出吸引了大量科学家对人工神经网络研究的兴趣，对神经网络的发展具有里程碑式的意义。为之后的多层深度学习的网络结构奠定了基础，进而后期不断衍生更深层的模型，产生大模型和模型并行等系统问题。

1969年，Marvin Minsky和Seymour Papert共同编写了一本书籍《感知器》，在书中他们证明了单层感知器无法解决线性不可分问题（例如：异或问题）。发现了当时的神经网络的两个重大缺陷：第一，基本感知机无法处理异或回路。第二，当时计算机的计算能力不足以用来处理复杂神经网络。神经网络的研究就此停滞不前。这也为后来深度学习的两大驱动力，提升硬件算力和模型增加非线性能力的演进埋下了伏笔。

1974年，Paul Werbos在博士论文中提出了用误差反向传播来训练人工神经网络，有效解决了异或回路问题，使得训练多层神经网络成为可能。这个工作奠定了之后深度学习的训练方式，深度学习训练系统中最为重要的执行步骤就是在不断的进行反向传播训练。

20世纪80年代，符号学习的代表方法是决策树和基于逻辑的学习。

1986年，深度学习一词由 Rina Dechter 于 1986 年引入机器学习社区。

1989年，Yann LeCun提出了一种用反向传导进行更新的卷积神经网络，称为LeNet。启发了后续卷积神经网络的研究与发展，卷积神经网络为深度学习系统的重要负载，大多数的深度学习系统都需要在卷积神经网络上验证性能。

20世纪90年代中期统计学习登场，支持向量机开始成为主流，进入第二个低谷。

2006年，Geoff Hinton、Ruslan Salakhutdinov、Osindero和Teh的论文表明，多层前馈神经网络可以一次有效地预训练一层，依次将每一层视为无监督受限的Boltzmann机，然后使用监督反向传播对其进行微调，其论文主要研究深度信念网络(Deep Belief Nets)的学习。

2012年，Alex Krizhevsky, Ilya Sutskever和Geoffrey Hinton，团队通过设计AlexNet赢得ImageNet竞赛，深度神经网络开始再次流行。首次采用ReLU激活函数，扩展了LeNet5结构，添加Dropout层减小过拟合，LRN层增强泛化能力/减小过拟合，这些新的模型结构和训练方法影响着后续的模型设计和系统优化。采用GPU对计算进行加速，进而形成深度学习系统以GPU等加速器为主要计算单元的架构。

在之后的时间里，以ImageNet等公开的各领域数据集为代表，驱动着卷积神经网络和其他的深度学习模型结构发展。

深度学习模型网络结构越来越深，新结构层出不穷，同时不断驱动深度学习系统的演化。

关注模型结构的变化，能够让系统研究者和工程师把握系统发展的未来趋势，并设计出符合潮流和应对未来变化的系统。

## 1.1.4 深度学习模型现状和趋势

目前深度学习模型有很多种类并在每年不断推出新的模型，我们将其简要归为以下一些代表性的类型。这些代表性的网络结构也是未来人工智能系统进行评测和验证所广泛使用的基准。同时一些新的结构的涌现，也不断推进一些新的系统设计。

基本模型结构：
- 卷积神经网络
  - 以卷积层(Convolution)，池化层(Pool)，全连接层(Fully Connected)的组合形成的在计算机视觉领域取得明显效果的模型结构。
- 循环神经网络
  - 以循环神经元(RNN)，长短时记忆（LSTM）等基本单元组合形成的适合时序数据预测的模型结构。
- 混合结构
  - 组合之前卷积神经网络和循环神经网络，进而解决如光学字符识别(OCR)等复杂的预测任务。

深度学习模型的趋势与发展：
- 更大的模型
  - 以Transformer为基本结构的代表性预训练模型，例如，BERT，GPT-3等，其不断增加的层数和参数量，对底层系统和硬件设计提出了很大的挑战。
- 更灵活的结构和建模能力
  - 图神经网络等网络不断捕捉多样的数据结构，应对更为复杂的建模需求。进而衍生了新的算子与计算框架。
- 更多样的训练方式
  - 自动化机器学习为代表的训练方式，衍生出多作业执行与优化的系统需求。
  - 增强学习为代表的算法有比传统训练方式更为复杂的过程。其衍生出训练，推理，数据处理混合部署与协同优化的系统需求。
  - Mixture of experts (MoE)为代表的融合异构模型的训练方式，衍生出新的系统的开发灵活性与协同优化的挑战。


## 参考文献

- Warren S. McCulloch and Walter Pitts. A logical calculus of the ideas immanent in nervous activity. Bulletin of mathematical biophysics, vol. 5 (1943), pp. 115–133.
- Rosenblatt, Frank (1957). "The Perceptron—a perceiving and recognizing automaton". Report 85-460-1. Cornell Aeronautical Laboratory.
- LeCun, Y.; Boser, B.; Denker, J. S.; Henderson, D.; Howard, R. E.; Hubbard, W. & Jackel, L. D. (1989). Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541-551.
- Rina Dechter (1986). Learning while searching in constraint-satisfaction problems. University of California, Computer Science Department, Cognitive Systems Laboratory.Online Archived 2016-04-19 at the Wayback Machine
- LeCun, Y.; Boser, B.; Denker, J. S.; Henderson, D.; Howard, R. E.; Hubbard, W.; Jackel, L. D. (December 1989). "Backpropagation Applied to Handwritten Zip Code Recognition". Neural Computation. 1 (4): 541–551. doi:10.1162/neco.1989.1.4.541. ISSN 0899-7667. S2CID 41312633
- Widrow, B., & Lehr, M.A. (1993). ARTIFICIAL NEURAL NETWORKS OF THE PERCEPTRON, MADALINE, AND BACKPROPAGATION FAMILY.
- Hinton, G. E.; Osindero, S.; Teh, Y. W. (2006). "A Fast Learning Algorithm for Deep Belief Nets" (PDF). Neural Computation. 18 (7): 1527–1554. doi:10.1162/neco.2006.18.7.1527. 
- Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. ImageNet classification with deep convolutional neural networks. In Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1 (NIPS'12). Curran Associates Inc., Red Hook, NY, USA, 1097–1105.

