<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 1.4 深度学习样例背后的系统问题

算法工程师通过Python和框架书写人工智能程序，而人工智能程序底层的系统问题被当前层的抽象隐藏，到底在每个代码部分具体底层发生了什么？有哪些有意思的系统设计问题？我们将从一个实例启发读者，并和后面各个章节构建起桥梁与练习，给读者构建除目录之外的第二个内容索引。

- [1.4 深度学习样例背后的系统问题](#14-深度学习样例背后的系统问题)
  - [1.4.1 一个深度学习样例与其中的系统问题](#141-一个深度学习样例与其中的系统问题)
  - [1.4.2 隐藏在算子(Operator)实现中的系统问题](#142-隐藏在算子operator实现中的系统问题)
  - [1.4.3 框架执行深度学习模型的生命周期](#143-框架执行深度学习模型的生命周期)
  - [1.4.4 更大范围的系统问题](#144-更大范围的系统问题)
  - [1.4.5 深度学习框架及工具入门实验](#145-深度学习框架及工具入门实验)
    - [1.4.5.1 实验目的](#1451-实验目的)
    - [1.4.5.2 实验环境](#1452-实验环境)
    - [1.4.5.3 实验原理](#1453-实验原理)
    - [1.4.5.4  实验内容](#1454--实验内容)
    - [1.4.5.5  实验计划](#1455--实验计划)
  - [小结与讨论](#小结与讨论)
  - [参考文献](#参考文献)

## 1.4.1 一个深度学习样例与其中的系统问题

如下图所示，我们可以看到一个深度学习模型可以接受输入（例如，当前手写数字图片），产生输出（例如图中为数字分类），这个过程叫前向传播(Forward Propagation)，也叫做推理(Inference)。这其中图片经过深度学习模型的处理产生输出结果，这个过程我们一般称作推理(Inference)。那么如何得到一个针对当前已有的输入输出数据上，预测效果最好的模型呢？我们需要通过训练的过程，训练过程可以抽象为一个优化问题，优化目标为:
$$\theta = argmin_{\theta}\sum[Loss(f_{\theta}(x), y)]$$
其中的$f_{\theta}$代表深度学习模型，例如后面提到的LeNet，$Loss$代表损失函数，$x$代表数据中的输入也就是图像，$y$代表数据中的标签值，也就是输出，训练的过程就是找到最小化$Loss$的$\theta$取值，$\theta$也称作权重。在训练过程中将通过梯度下降算法进行求解，$\theta = \theta - \alpha \delta_{\theta}Loss(\theta)$，其中$\alpha$也叫学习率(Learning Rate)。当训练完成，就可以通过$\hat{y} = f_\theta(x)$进行推理，使用和部署模型。

<center><img src="./img/4/4-1-2.png" ch="500" /></center>
<center>图1-5-1. 深度学习训练过程</center>


如下图所示，左上角的图示中展示的是输入为手写数字图像，输出为分类向量，中间的矩形为各层输出的特征图。我们可以看到深度学习模型就是通过各个层将输入图像处理为类别输出概率向量。用户一般经过两个阶段进行构建: (1)定义网络结构，例如图中和下面代码实例中构建的LeNet网络，其中包含有二维卷积(Conv2D)，最大池化(max_pool2d)，全连接(Linear)层。（2）开始训练，遍历一个批尺寸(Batch Size)数据，设置计算资源，前向传播计算，计算损失(Loss)。

<center><img src="./img/4/4-1-1.png" ch="500" /></center>
<center>图1-5-2. PyTorch训练LeNet实例</center>

下面的实例是PyTorch在MNIST数据集上训练一个卷积神经网络[LeNet](http://yann.lecun.com/exdb/lenet/)的代码实例。

```
...
# 读者可以参考第3章理解深度学习框架的底层原理和设计
import torch
...

# 如果模型层数多，权重多到无法在单GPU显存放置，我们需要通过模型并行方式进行训练，读者可以参考第6章进行了解
class LeNet(nn.Module):
    def __init__(self):
        super(LeNet, self).__init__()
        # 请参考1.4.2小节，通过循环实现卷积理解卷积的执行逻辑并思考其中的潜在系统问题
        self.conv1 = nn.Conv2d(3, 6, 5)
        # 我们能否调整超参数6为64？如何高效的搜索最有的配置？这些内容我们将在第9章展开介绍
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16*5*5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc2 = nn.Linear(84, 10)

    def forward(self, x):
        out = F.relu(self.conv1(x))
        out = F.max_pool2d(out, 2)
        out = F.relu(self.conv2(out))
        out = F.max_pool2d(out, 2)
        out = out.view(out.size(0), -1)
        out = F.relu(self.fc1(out))
        out = F.relu(self.fc2(out))
        out = self.fc3(out)
        return out


def train(args, model, device, train_loader, optimizer, epoch):
    # 如何进行高效的训练，运行时是如何执行的？我们将在第3章进行介绍
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        ...


def test(model, device, test_loader):
    model.eval()
    ... 
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            # 推理系统如何高效进行模型推理？我们将在第8章进行介绍
            output = model(data)
            ...


def main():
    ...
    # 当前语句决定了使用哪种加速器以及体系结构，读者可以通过第4章了解。
    device = torch.device("cuda" if use_cuda else "cpu")
    # 如果batch size过大，造成单GPU内存无法容纳模型及中间激活的张量，读者可以参考第6章进行了解如何分布式训练
    train_kwargs = {'batch_size': args.batch_size}
    test_kwargs = {'batch_size': args.test_batch_size}
    ...
    """
    如何高效的进行数据读取？这些内容我们将在第7章进行介绍。
    如果我们训练的数据集和模型是预测系统优化配置，我们想训练的模型是优化系统配置，那么读者可以参考第13章，
    思考如何将AI应用到系统优化。
    如果我们的数据集没有提前准备好，需要实时和环境交互获取，那么读者可以参考第10章进行理解。
    """
    dataset1 = datasets.MNIST('../data', train=True, download=True, transform=transform)
    dataset2 = datasets.MNIST('../data', train=False, transform=transform)
    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)
    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)
    model = LeNet().to(device)
    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)
    ... 
    for epoch in range(1, args.epochs + 1):
        train(args, model, device, train_loader, optimizer, epoch)
        # 模型如果训练完成需要部署，我们如何压缩和量化后再部署？读者可以参考第11章进行了解
        test(model, device, test_loader)
        ... 
    # 模型进行检查点(checkpoint)保存模型，防止因为环境失效导致模型丢失，同时也可以定期测试模型观察训练效果。
    if args.save_model:
        torch.save(model.state_dict(), "mnist_cnn.pt")

# 如果用户提交多个这样的训练作业，系统如何调度和管理资源？读者可以参考第7章进行了解
if __name__ == '__main__':
    main()
```

## 1.4.2 隐藏在算子(Operator)实现中的系统问题

我们在深度学习中所描述的层(Layer)，一般在深度学习编译器或者算子中也称作操作符(Operator)。底层算子的具体实现时由其对应的矩阵运算翻译为对应的循环（目前我们简化问题，忽略stride等其他超参数影响）。

图1-5-3的卷积层实例中，每次选取输入数据一层的一个窗口（例如和卷积核一样的宽高）然后和对应的卷积核(例如Filter-1中的5$\times$5卷积核)进行矩阵内积计算,最后将所有的计算结果与偏置项$b$相加后输出，产生特征图。输入张量形状(Tensor Shape)为（3x32x32），经过2x3x5x5的卷积（2代表输出通道数，3代表输入通道数，5代表卷积核高，5代表卷积核宽）后，输出张量形状(Tensor Shape)为（2x28x28）。

<center><img src="./img/4/conv3d.png" ch="500" /></center>
<center>图1-5-3. Conv2d计算过程实例</center>

我们以卷积算子为例，卷积的计算可以表达为多层循环，我们以下面代码为例进行分析。
```
# 为简化阐述计算过程，我们简化了维度(Dimension)的形状推导(Shape Inference)。
# Conv2d将被转换为如下的7层循环进行计算:

# 批尺寸维度
for n in range(batch_size):
  # 输出张量通道维度
  for oc in range(output_channel):
    # 输入张量通道维度
    for ic in range(input_channel):
      # 输出张量高度维度
      for h in range(out_height):
        # 输出张量宽度维度
        for w in range(out_width):
          # 卷积核高度维度
          for fh in range(filter_height):
            # 卷积核宽度维度
            for fw in range(filter_width):
              # 乘加运算
              output[h, w, oc] += input[h + fw, w + fh, ic] * kernel[fw, fh, c, oc]  
```

在这其中有很多有趣的问题问题读者可以思考与预估：
- 从算法来说，当前7层循环是否能转换为更加简单的矩阵计算(例如，[cuDNN](https://docs.nvidia.com/deeplearning/cudnn/)库中的卷积就提供了[多种实现算法](https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionForward))。这些算法被封装在库中，有些框架会在运行时动态调优选择不同算法策略，读者可以参考第3章进行更多的了解。

为了利用张量核心的矩阵乘法算子，先进的深度学习加速库，例如cuDNN，通常通过应用[im2col](https://hal.inria.fr/inria-00112631/document)函数将卷积转换为通用矩阵乘法（General Matrix Multiplication）缩写[GEMM](https://en.wikipedia.org/wiki/GEMM)。cuDNN也支持利用其他算法实现卷积，例如,FFT, WINOGRAD等。通用矩阵乘是自然语言处理中的主要的计算原语，同时卷积也可以转换为通用矩阵乘，同时底层GPU和其他专有人工智能芯片也针对矩阵乘作为底层支持（例如张量核（[Tensor Core](https://www.nvidia.com/en-us/data-center/tensor-cores/)）），这样的转换就可以让算子利用底层硬件和软件的优化。

<center> <img src="./img/4/1-4-4-imtocol.png" ch="500" width="500" height="160" /></center>
<center>图1-4-4. 卷积通im2col转换为通用矩阵乘(<a href="https://arxiv.org/pdf/2105.09564.pdf">图片引用</a>)</center>

- 其中参与计算的输入，权重和输出张量能否完全放入GPU缓存(L1，L2)？如果不能放入则需要通过块(Tile)优化进行切片，这些内容将在第5章着重介绍。
- 循环执行的主要计算语句是否有局部性可以利用？空间局部性（缓存线内相邻的空间是否会被连续访问）以及时间局部性（同一块内存多久后还会被继续访问），这样我们可以通过预估后，尽可能的通过编译调度循环执行，这些内容将在第5章着重介绍。
- 如果有些权重为0是否可以不进行计算？读者可以参考第11章稀疏性(Sparsity)部分进行了解。
- 读者可以[预估](https://www.microsoft.com/en-us/research/uploads/prod/2020/09/dnnmem.pdf)各个层的输出(Output)张量，输入(Input)张量，和内核(Kernel)张量大小，进而评估是否需要多卡，内存管理策略设计，以及换入换出策略等。读者可以参考第5，8章相关内存优化与管理内容。
- 那么当算子与算子在运行时按一定调度次序执行，框架如何进行运行时管理，请读者参考第3章相关内容理解

## 1.4.3 框架执行深度学习模型的生命周期

在之前的实例中，我们基本知晓Python和框架代码中的一个算子（例如，卷积）是如何翻译成底层for循环计算的，这类for循环计算通常可以被设备厂商提供的运行时算子库抽象，不需要用户继续书写for循环了，例如cuDNN提供卷积的实现和API。如图1-4-5.所示，相当于我们已经抽象到了cuDNN这层书写，似乎我们已经提升了很多开发效率，我们为什么还需要深度学习框架（例如，TensorFlow，PyTorch）？
因为即使抽象到这层，仍然调用起来十分不方便。那么框架作为至关重要的深度学习系统究竟在其中扮演什么角色和做了其他什么工作呢？用户的Python代码是如何一步步翻译到底层的具体实现呢？我们以一个实例为例介绍。

<center> <img src="./img/4/4-1-4-frompythontolower.png" ch="500" width="500" height="300" /></center>
<center>图1-4-5. 深度学习程序的层次化调用关系</center>
   
首先，我们先对比一下，如果没有深度学习框架，而只将算子for循环抽象提供算子库（例如，cuDNN）的调用，读者将只能通过设备提供的底层API编写作业，例如，通过CUDA + cuDNN库书写卷积神经网络（例如，读者可以对比这个通过[cuDNN书写的卷积神经网络LeNet实例](https://github.com/tbennun/cudnn-training)），代码量大，也极为容易出错。所以好的系统和框架能大幅提升生产力，是会不断抽象和屏蔽底层的计算与资源管理细节。我们通过LeNet实现实例，对比cuDNN这层抽象还不足以让算法工程师非常高效的设计模型和书写算法，如两个实例所示，同样实现LeNet，使用高层框架只需要9行，而通过cuDNN需要上千行代码，而且还需要精心的管理内存分配释放，拼接模型计算图，效率十分低下。

***LeNet实现实例 (1) 不通过框架，直接通过cuDNN编程实现LeNet，需要~1000行实现模型结构和内存管理等逻辑***
[参考文档](https://github.com/tbennun/cudnn-training/blob/master/lenet.cu)
```C++
// 内存分配，如果用深度学习框架此步骤会省略
...
cudaMalloc(&d_data, sizeof(float) * context.m_batchSize * channels * height * width);
cudaMalloc(&d_labels, sizeof(float) * context.m_batchSize * 1  * 1 * 1);
cudaMalloc(&d_conv1, sizeof(float) * context.m_batchSize * conv1.out_channels * conv1.out_height * conv1.out_width);
...
// 前向传播第一个卷积算子（仍需要写其他算子）
...
cudnnConvolutionForward(cudnnHandle, &alpha, dataTensor,
                        data, conv1filterDesc, pconv1, conv1Desc, 
                        conv1algo, workspace, m_workspaceSize, &beta,
                        conv1Tensor, conv1);
...
// 反向传播第一个卷积算子（仍需要写其他算子），如果用深度学习框架此步骤会省略
cudnnConvolutionBackwardBias(cudnnHandle, &alpha, conv1Tensor,
                             dpool1, &beta, conv1BiasTensor, gconv1bias);
        
cudnnConvolutionBackwardFilter(cudnnHandle, &alpha, dataTensor,
                               data, conv1Tensor, dpool1, conv1Desc,
                               conv1bwfalgo, workspace, m_workspaceSize, 
                               &beta, conv1filterDesc, gconv1));
// 第一个卷积权重梯度更新（仍需要写其他算子），如果用深度学习框架此步骤会省略
cublasSaxpy(cublasHandle, static_cast<int>(conv1.pconv.size()),
            &alpha, gconv1, 1, pconv1, 1);
cublasSaxpy(cublasHandle, static_cast<int>(conv1.pbias.size()),
            &alpha, gconv1bias, 1, pconv1bias, 1);
// 内存释放，如果用深度学习框架此步骤会省略
...
cudaFree(d_data);
cudaFree(d_labels);
cudaFree(d_conv1);
...
```
***LeNet实现实例 (2) 通过Keras书写LeNet （TensorFlow Backend），只需要9行构建模型结构***
[参考文档]()
```python
model = keras.Sequential()
model.add(layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(32,32,1)))
model.add(layers.AveragePooling2D())
model.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))
model.add(layers.AveragePooling2D())
model.add(layers.Flatten())
model.add(layers.Dense(units=120, activation='relu'))
model.add(layers.Dense(units=84, activation='relu'))
model.add(layers.Dense(units=10, activation = 'softmax'))
```

从上面我们看到，深度学习框架对算法工程师开发深度学习模型，训练模型非常重要。总结起来，深度学习框架一般会提供以下功能：
1. 以Python API供读者编写复杂的模型计算图（Computation Graph）结构，调用基本算子实现（例如，卷积的cuDNN实现），大幅降低开发代码量。
2. 自动化内存管理，不暴露指针和内存管理给用户。
3. 自动微分（Automatic Differentiation）的功能，并能自动构建反向传播计算图，与前向传播图拼接成统一计算图。
4. 调用或生成运行期优化代码（静态优化）
5. 调度算子在指定设备的执行，并在运行期应用并行算子，提升设备利用率等优化（动态优化）。
  
TensorFlow也是应用非常广泛的框架，相比PyTorch的[命令式执行（Imperative Execution）](https://en.wikipedia.org/wiki/Imperative_programming)方式（运行到算子代码即触发执行，易于调试），TensorFlow采用[符号执行（Symbolic Execution）](https://en.wikipedia.org/wiki/Symbolic_execution) （调用session.run才真正触发执行，并且框架能获取完整计算图进行优化）方式。二者详细区别我们将在后面框架章节进行介绍。我们在下面的图示和实例中以TensorFlow的一个简单程序为例，展示一个深度学习模型是如何被深度学习框架静态（Static）编译与运行时动态（Dynamic）管理的。

如图1-4-6到图1-4-9所示，我们通过划分不同阶段，解释一个TensorFlow程序完成一个精简模型x*y + z的训练全生命周期。

（1）前端程序转换为数据流图：如图1-4-6.所示，这个阶段框架会将用户书写的模型程序，通过预先定义的接口，翻译为中间表达（Intermediate Representation），并且构建算子直接的依赖关系，形成前向计算图。
<center> <img src="./img/4/4-1-5-pythontoforward.png" ch="500" width="1000" height="400" /></center>
<center>图1-4-6. Python + TensorFlow程序解析为中间表达和前向传播数据流图</center>

（2）反向求导：如图1-4-7.所示，这个阶段框架会分析形成前向数据流图，通过算子之前定义的反向传播算子，构建反向传播数据流图，并和前向传播数据流图一起形成整体的数据流图。

<center> <img src="./img/4/4-1-6-backwardgraph.png" ch="500" width="1000" height="300" /></center>
<center>图1-4-7. 反向求导，自动微分(Automatic Differentiation)</center>

（3）产生运行期代码：如图1-4-8.所示，这个阶段框架会分析整体的数据流图，并根据运行时部署所在的设备（CPU，GPU等），将算子中间表达产生为算子运行期的代码，例如图中的C++或者CUDA实现。


<center> <img src="./img/4/4-1-7-genruntime.png" ch="500" width="1000" height="300" /></center>
<center>图1-4-8. 产生运行期代码</center>

（4）调度并运行代码：如图1-4-9.所示，这个阶段框架会将算子及其运行期的代码实现，依次根据依赖关系，调度到计算设备上进行执行。对一些不方便静态做优化的选择，可以通过运行期调度达到，例如重叠计算与IO，如有空闲资源并行执行没有依赖的算子等。

<center> <img src="./img/4/4-1-8-execution.png" ch="500" width="1000" height="300" /></center>
<center>图1-4-9. 调度并运行代码</center>

综上所示，我们通过上面两个小节可以发现，如果没有框架和算子库的支持，算法工程师进行简单的深度学习模型设计与开发都会举步维艰，所以我们看到深度学习算法本身飞速发展的同时，也要看到底层系统对提升整个算法研发的生产力起到了不可或缺的作用。

## 1.4.4 更大范围的系统问题

- 更多的超参数组合与模型结构探索
  - 之前我们看到的实例本身是单个模型的样例，但是深度学习模型可以通过变换其中的超参数和模型结构获取和训练更好的结果，这种探索式的过程也叫做自动化机器学习，读者可以参考第9章-自动化机器学习系统了解相关领域内容与挑战。
- 共享的资源与多租的环境
  - 如果我们现在的GPU等训练资源都是被公司或组织机构集中管理，用户需要共享使用资源进而提升资源整体利用率，那么在这种环境下系统如何提供给算法工程师接近单机的使用环境体验让算法工程师更加简便高效的使用资源？读者可以参考第7章-异构计算集群调度与资源管理系统进行了解平台如何应对当前的挑战。
- 假设数据无法离线提前准备好？
  - 如果数据没有提前准备好，需要系统提供更加多样的训练方式，深度学习系统需要不断与环境或者模拟器交互，通过强化学习方式进行训练，读者可以参考第10章-强化学习系统进行了解，强化学习系统如何在更复杂与多样的场景下进行模型训练以及数据获取。
- 数据和人工智能模型的安全与隐私如何保障？
  - 当前深度学习为数据驱动的方法，同时会产生交付的模型文件，模型泄露，篡改以及本身的缺陷会造成潜在的安全风险。如何保障深度学习整体的安全与隐私相比传统安全领域遇到了新的挑战，读者可以参考第12章-人工智能安全与隐私进行了解。
- 之前我们大部分了解的是针对人工智能负载做系统设计也称作System for AI，反过来我们也可以思考如何通过人工智能这种数据驱动的方法反过来指导系统设计与优化，也就是AI for System，读者可以参考第13章-人工智能优化计算机系统进行了解。

## 1.4.5 深度学习框架及工具入门实验

通过在深度学习框架上调试和运行样例程序，观察不同配置下的运行结果，了解深度学习系统的工作流程。通过实验读者将了解：1）深度学习框架及工作流程（Deep Learning Workload）。2）在不同硬件和批大小（batch_size）条件下，张量运算产生的开销。

具体实现细节请大家参考实验[AI-System Lab1 框架及工具入门示例](https://github.com/microsoft/AI-System/tree/main/Labs/BasicLabs/Lab1)。

实验（Experiment）与遥测（Telemetry）是系统工作必不可少的环节，同时系统研究与工作离不开动手实践。希望读者通过上面实例端到端跑通样例并对相关工具和系统有初步的实践体验。

### 1.4.5.1 实验目的

1. 了解深度学习框架及工作流程（Deep Learning Workload）
2. 了解在不同硬件和批大小（batch_size）条件下，张量运算产生的开销


### 1.4.5.2 实验环境

* PyTorch==1.5.0

* TensorFlow>=1.15.0

* 【可选环境】 单机Nvidia GPU with CUDA 10.0


### 1.4.5.3 实验原理

通过在深度学习框架上调试和运行样例程序，观察不同配置下的运行结果，了解深度学习系统的工作流程。

### 1.4.5.4  实验内容

***实验流程图***

<center> <img src="./img/4/Lab1-flow.png" ch="500" width="400" height="400" /></center>
<center>图1-4-10. 实验流程图</center>


***具体步骤***

1.	安装依赖包。PyTorch==1.5, TensorFlow>=1.15.0

2.	下载并运行PyTorch仓库中提供的MNIST样例程序。

3.	修改样例代码，保存网络信息，并使用TensorBoard画出神经网络数据流图。

4.	继续修改样例代码，记录并保存训练时正确率和损失值，使用TensorBoard画出损失和正确率趋势图。

5.	添加神经网络分析功能（profiler），并截取使用率前十名的操作。

6.	更改批次大小为1，16，64，再执行分析程序，并比较结果。

7.	【可选实验】改变硬件配置（e.g.: 使用/ 不使用GPU），重新执行分析程序，并比较结果。


### 1.4.5.5  实验计划

***实验环境***

||||
|--------|--------------|--------------------------|
|硬件环境|CPU（vCPU数目）|&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |
||GPU(型号，数目)||
|软件环境|OS版本||
||深度学习框架<br>python包名称及版本||
||CUDA版本||
||||

***实验结果***

1. 模型可视化结果截图
   
|||
|---------------|---------------------------|
|<br/>&nbsp;<br/>神经网络数据流图<br/>&nbsp;<br/>&nbsp;|&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |
|<br/>&nbsp;<br/>损失和正确率趋势图<br/>&nbsp;<br/>&nbsp;||
|<br/>&nbsp;<br/>网络分析，使用率前十名的操作<br/>&nbsp;<br/>&nbsp;||
||||


2. 网络分析，不同批大小结果比较

|||
|------|--------------|
|批大小 &nbsp;| &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 结果比较 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |
|<br/>&nbsp;<br/>1<br/>&nbsp;<br/>&nbsp;||
|<br/>&nbsp;<br/>16<br/>&nbsp;<br/>&nbsp;||
|<br/>&nbsp;<br/>64<br/>&nbsp;<br/>&nbsp;||
|||

***参考代码***

1.	MNIST样例程序：

    代码位置：AI-System/Labs/BasicLabs/Lab1/mnist_basic.py

    运行命令：`python mnist_basic.py`

2.	可视化模型结构、正确率、损失值

    代码位置：AI-System/Labs/BasicLabs/Lab1/mnist_tensorboard.py

    运行命令：`python mnist_tensorboard.py`

3.	网络性能分析

    代码位置：AI-System/Labs/BasicLabs/Lab1/mnist_profiler.py

***参考资料***

* 样例代码：[PyTorch-MNIST Code](https://github.com/pytorch/examples/blob/master/mnist/main.py)
* 模型可视化：
  * [PyTorch Tensorboard Tutorial](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html) 
  * [PyTorch TensorBoard Doc](https://pytorch.org/docs/stable/tensorboard.html)
  * [pytorch-tensorboard-tutorial-for-a-beginner](https://medium.com/@rktkek456/pytorch-tensorboard-tutorial-for-a-beginner-b037ee66574a)
* Profiler：[how-to-profiling-layer-by-layer-in-pytroch](https://stackoverflow.com/questions/53736966/how-to-profiling-layer-by-layer-in-pytroch)
  
## 小结与讨论

本章我们主要通过一些实例启发读者建立本书各个章节之间的联系，由于系统的多层抽象造成我们实践人工智能的过程中已经无法感知底层系统的运行机制，希望读者结合后面章节的学习后，能够看到深度学习系统底层的作用和复杂性，并从中指导上层人工智能作业和代码更加高效的书写。

请读者读完后面章节后再回看当前章节，并重新思考当前书写的人工智能Python代码底层发生了什么？

## 参考文献
- http://yann.lecun.com/exdb/lenet/
