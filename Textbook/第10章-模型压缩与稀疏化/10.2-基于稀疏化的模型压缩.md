<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

<!-- Author: Prof. Jianyi Liu at XJTU. -->

# 基于稀疏化的模型压缩

- [基于稀疏化的模型压缩](#基于稀疏化的模型压缩)
  - [一、模型压缩的重要性](#一模型压缩的重要性)
  - [二、人工智能系统与稀疏性](#二人工智能系统与稀疏性)
  - [三、深度神经网络的稀疏化与剪枝](#三深度神经网络的稀疏化与剪枝)
    - [3.1 权重稀疏](#31-权重稀疏)
    - [3.2  激活稀疏](#32--激活稀疏)
    - [3.3  梯度稀疏](#33--梯度稀疏)
  - [四、稀疏与剪枝方法的总结及挑战](#四稀疏与剪枝方法的总结及挑战)

## 一、模型压缩的重要性

​        从2012年AlexNet 赢得 ImageNet 比赛一直到现在，AI 模型变得越来越大。科学家在实验中一次次证明，越大的模型效果越好。今天，增加模型的深度、宽度和数据精度，依然是提高模型性能的主要手段之一。然而，伴随着深度神经网络模型的性能增加，接踵而来的是深度网络模型的高存储高功耗的弊端，严重制约着深度神经网络在资源有限的应用环境和实时在线处理的应用，特别是智能化移动嵌入式设备。例如８层的AlexNet包含600 000个网络节点、0.61亿个网络参数，需要花费240MB内存存储和7.29亿浮点型计算次数(FLOPs)来分类一副分辨率为224×224的彩色图像；16层的VGGNet则拥有1 500 000个网络节点、1.44亿个网络参数，需要花费528MB内存存储和150亿次浮点型计算次数[2]。表1列出了在自然语言、语音、视觉等领域一些著名大模型的资源消耗情况。对于这些业界领先的算法，即使是现在最强的GPU，也需要几天甚至几周训练一轮。

​                                                 **表1. 当前一些代表性深度学习大模型的资源占据情况**

| 深度模型名称 | <img src=".\img\2\img\2\BERT.png" alt="BERT" style="zoom:33%;" /> | <img src=".\img\2\wavenet.png" alt="wavenet" style="zoom:33%;" /> | <img src=".\img\2\deform.png" alt="deform" style="zoom:33%;" /> | <img src=".\img\2\MoE.png" alt="MoE" style="zoom:33%;" /> |
| ------------ | ----------------------------------------------------- | ----------------------------------------------------------- | --------------------------------------------------------- | --------------------------------------------------- |
| 硬件平台需求 | 64 TPUv2                                              | 2 P100                                                      | 8 P100                                                    | 64 K80                                              |
| 训练耗费时间 | 4 Days                                                | 6 Days                                                      | 10 Days                                                   | 6 Days                                              |
| 模型参数规模 | 1000 GB                                               | 16 GB                                                       | 64 GB                                                     | 1500 GB                                             |

​        计算力一直以来都是制约AI发展和应用的限制因素之一。当前，尽管GPU的功耗效率（power-efficiency）远高于CPU，但在面对一些超大的模型时也已开始捉襟见肘了。不是科学家设计不出好的算法，而是服务器无法支持这些算法的高效运行。对于移动嵌入式平台而言，更是难以直接存储和运行上述如此庞大的深度网络。实际上，百万级以上的深度神经网络模型内部存在大量冗余信息，并不是所有的参数和结构都对产生深度神经网络高判别性起作用。利用深度网络的稀疏性进行模型压缩是降低存储和计算开销、同时保持或提升模型精度的一种有效的解决方案。

## 二、人工智能系统与稀疏性

​		生物研究发现人脑是高度稀疏的。例如当人类识别一只猫时，我们不会仔细检查每一个毛发的纹理，仅仅使用简单的几何边缘就足以做出判别。在交通场景中，当人类看到眼前物体时，我们的神经系统不会处理所有像素，因为那样无法在瞬息万变的场景中迅速做出反应。我们此时仅会关注视野中主要的物体，比如图中用颜色分割的交通参与者（人、车辆）。这些人类认知的本能是千百万年进化与自然选择的结果。同样，在语音识别、医疗成像、自动驾驶、社交网络等各行各业的海量数据中，每种数据都有其内在的结构性。自动学习数据内生的结构性是人工智能算法的核心，数据的结构性带来了信息表达中的稀疏性，高效的人工智能系统应该充分利用这种稀疏性。

<img src=".\img\2\real cat.png" alt="real cat" style="zoom:30%;" /><img src=".\img\2\traffic.png" alt="traffic" style="zoom:25%;" />

**图1.人类的视觉系统是稀疏的，不需要处理所有像素即可迅速做出判断** 

​		其实对于AI模型稀疏度的追求早在深度学习风行之前就已得到了广泛的研究，稀疏编码曾经是实现人脸识别主流的技术手段之一[1]。这是因为训练数据的噪声使得模型中包含大量冗余信息，降低了模型的推广（generalization）能力，而通过模型稀疏化则可以消除这部分冗余从而提升模型精度，但是过度的稀疏则会丢失模型中的关键信息而严重损坏精度指标。在深度神经网络中，对模型尺寸、浮点运算量等性能（performance）指标的追求成为主要的关注点，模型稀疏度的增加可以持续提升上述性能。如何在模型精度与性能之间寻求最优的折衷是一个复杂的研究话题，也是神经网络稀疏化的研究目标。

<img src=".\img\2\tradeoff.png" alt="tradeoff" style="zoom:40%;" />

​                                        **图2. 稀疏性是人工智能系统度解决模型精度与性能间折衷的主要手段[3]**

## 三、深度神经网络的稀疏化与剪枝

​          按照深度学习模型中可以被稀疏化的对象，网络稀疏化可以划分为模型稀疏与瞬态（Ephemeral）稀疏两大类。模型稀疏直接改变网络模型中两个最基本的元素——权重与神经元，多应用于网络的推理即前馈运算过程中。权重稀疏对应于非结构化（unstructured）剪枝，这是一种细粒度（fine-grained）的剪枝方式，计算过程中会造成内存访问的不规则性，影响硬件工作效率。也有部分的权重稀疏化工作通过硬件友好型操作，存储连续块的权重从而实现结构化剪枝。神经元稀疏则通过移除整个神经元实现模型压缩，这是一种结构化的（structured)剪枝方式。由于卷积神经网络中权值共享的机制，一个神经元通常也对应着一个卷积核（滤波器）及其卷积计算结果——一个特征图或通道（channel），因此，滤波器剪枝和通道剪枝也可视为结构化剪枝的类型，剪枝后网络可以有效运行于与原始模型相同的硬件结构之上。

​		瞬态稀疏是直接与特定数据样本相关联的，激活稀疏就属于这一类别，特定图像通过激活函数（如ReLU）后将产生特定的稀疏特征图。由于激活函数与单个神经元直接关联，因此激活稀疏也属于结构化稀疏类别。神经网络中的SoftMax操作和dropout操作也可视为激活稀疏，前者属于舍入激活，后者属于随机激活。另一大类的瞬态稀疏是与梯度计算直接相关的，包括梯度、误差及优化器状态，在基于随机梯度下降（SGD）的后向传播算法中通过对梯度值进行稀疏、延迟梯度更新，可以显著减少分布式训练过程中的通信开销。瞬态稀疏中既包括推理时稀疏，如激活稀疏与条件计算，也包括训练时稀疏，主要体现为梯度相关的稀疏。图3总结了按稀疏对象进行分类的树形结构。

<img src=".\img\2\object.png" alt="object" style="zoom:50%;" />

​                                                                **图3.深度神经网络稀疏性的分类[3]** 

​          下文将对权重、激活和梯度三种主要类型的稀疏进行讨论，它们按照不同的视角进行分类的关系如表1所示。结构化剪枝由于对内存访问的规则性，相比非结构化剪枝的到了更多的关注。动态稀疏在训练阶段将剪枝操作与元素再生结合在一起，权重稀疏属于静态稀疏，因为在每次训练迭代过程中，或是整个推理过程中，其连接结构不会发生改变。相比之下，激活及梯度稀疏则属于动态稀疏，无论卷积层的窗口滑动或是输入样本的改变，激活函数都会产生变化的稀疏输出结构。同样，梯度数值也与样本输入直接相关。数据无关（data free）与数据驱动（data driven）类别的划分是依据模型稀疏表现与输入数据的相关性，其与动/静态稀疏是一一对应的。权重稀疏与激活稀疏可以压缩模型推理阶段的网络规模或浮点运算量，梯度稀疏则用于压缩分布式训练情况下的网络通讯带宽。各种分类方式的关系在表2中进行了总结。

​                                             **表2. 深度神经网络稀疏性各种分类方式间的关系** 

|          | 结构/非结构剪枝 | 动态/静态 | 数据驱动方式 | 训练时/推理时 |
| -------- | --------------- | --------- | ------------ | ------------- |
| 权重稀疏 | 非结构化剪枝    | 静态      | 数据无关     | 推理          |
| 激活稀疏 | 结构化剪枝      | 动态      | 数据驱动     | 推理          |
| 梯度稀疏 | N/A             | 动态      | 数据驱动     | 训练          |

### 3.1 权重稀疏

​       在大多数类型的深度神经网络中，通过对各层卷积核元素的数值（即网络权重）进行数值统计，人们发现许多层权重的数值分布很像是正态分布（或者是多正态分布的混合），越接近于0，权重就越多。这就是深度神经网络中的权重稀疏现象，一个典型的网络权重分布直方图如下图（a）所示。舍弃掉其中接近0值的权重，相当于在网络中剪除部分连接，如（b）所示，对网络精度影响并不大，这就是权重剪枝。



<img src=".\img\2\weight Gaussian.png" alt="weight Gaussian" style="zoom:30%;" />

​                                              （a）                                       （b）                                    （c）

**图4. 深度网络中存在权重稀疏性[3]。**（a）剪枝前的权重分布；（b）剪除0值附近权值后的权重分布；（c）网络微调后的权重分布。

​       这么做的道理是因为权重数值的绝对值大小可以看做重要性的一种度量，较大的权重意味着对最终输出的贡献较大，也相对更加重要，反之则相对不重要。不重要的权重删去对精度影响就应该较小。

<img src=".\img\2\weight prune.png" alt="weight prune" style="zoom: 25%;" />

​                            **图5. 全连接层中权重（非结构化）剪枝示意图。**浅色连接表示被剪除的权值连接。

​       这种仅依靠权重绝对值大小来排序的剪枝方法通常会带来非结构化的稀疏连接，如图5所示，在计算过程中会引起不规则的内存获取，相反会影响网络的计算效率。所以权重剪枝通常以剪除整个滤波器的方式来实现，图6介绍了一种具体的滤波器剪枝实现算法[4]。假设网络中第i个卷积层中共包含$n_i$个通道（channel），即$n_i$个特征图。$\cal F_{i,j}$ 表示第$i$层后的第$j$个卷积核（滤波器），剪除该滤波器将直接导致第$i+1$层的对应通道（特征图）也被剪除，第$i+1$层之后所有滤波器张量中的对应切片（slice）也将被剪除，如图3中三块蓝色区域标识。假设每个滤波器切片$\cal K$的大小为$k\times k$，则上述剪枝操作将直接减少$k^2\times n_i + k^2\times n_{i+2}$个滤波器权重浮点数的存储空间，并减少$k^2\times n_{i+2}\times h_{i+2}\times w_{i+2} + k^2\times n_i\times h_{i+1}\times w_{i+1}$次浮点数乘法运算开销，达到了网络压缩与加快推理速度的目的。

<img src=".\img\2\filter prune.png" alt="filter prune" style="zoom:40%;" /> 

**图6. 权重剪枝算法示例一[4]。**剪除一个完整滤波器对应于剪除一个通道的特征图。

​        如果经过预训练的网络其滤波器参数的稀疏度不足，表现为0值附近的参数数量较少，则上述算法中可供剪除的滤波器数目也将随之较小，从而达不到预期的模型压缩比率。在这种情况下，模型参数正则化（Regularization）就是一种常用的手段来“迫使”模型训练出更加稀疏的滤波器参数分布。如下述公式所示，正则项与训练数据误差项一般共同构成损失函数，并通过计算其梯度实现后向传播的训练过程。

<img src=".\img\2\loss.png" alt="loss" style="zoom:20%;" />

​        上式中$R(W)$即为正则项，正则因子$\lambda$用以调节两项之间的折衷。常用的正则函数包括$L_1$范数、$L_2$范数、$L_{1,2}$范数等，其定义如下表所示：

​                                                                  **表3. 常用的正则项函数**

|               | 正则项定义                                                |
| ------------- | --------------------------------------------------------- |
| $L_1$正则     | <img src=".\img\2\L1norm.png" alt="L1norm" style="zoom:25%;" /> |
| $L_2$正则     | <img src=".\img\2\L2.png" alt="L2" style="zoom:25%;" />         |
| $L_{1,2}$正则 | <img src=".\img\2\L12.png" alt="L12" style="zoom:25%;" />       |

​        文献[5]提出了一种利用批归一化因子（BN）来识别不重要通道的算法，如图8所示。在卷积层之后插入BN层是许多深度网络的通用做法，用以提高训练收敛速度及模型推广能力。BN层中的尺度因子$\gamma $如果接近于0，则其所对应的通道在后续卷积运算中的作用将可以忽略。基于这一考虑，该算法将$L_1$正则项施加于$\gamma$之上，从而迫使BN的尺度因子分布稀疏化，并相应剪除这些接近0值尺度因子所对应的通道、以及对应的滤波器权值连接，从而达到模型压缩目的。

**<img src=".\img\2\BN.png" alt="BN" style="zoom:50%;" />**

**图7. 权重剪枝算法示例二[5]。**利用BN层中的尺度因子稀疏化计算通道的重要性。 

​        即使是移除绝对值接近于0的权重（滤波器）也会带来推理精度的损失。为了恢复网络精度，通常在剪枝之后需要进行再次的训练，这个过程称为微调（fine-tuning）。微调之后的权重分布将部分地恢复高斯分布的特性，如图4（c）所示，同时网络精度也会达到或接近剪枝前的水平。大多数的权重剪枝算法都遵循这一“正则化-剪枝-微调”反复迭代的流程，如图8所示，直到网络规模和精度的折衷达到预设的目标为止。

<img src=".\img\2\three step.png" alt="three step" style="zoom:40%;" />

​                                                       **图8. 剪枝算法常用的迭代计算流程**

### 3.2  激活稀疏

​      <img src=".\img\2\ReLU.jpeg" alt="ReLU" style="zoom:30%;" />

（a）Sigmoid函数                        （b）ReLU函数

**图9. 两种常用的激活函数**

​        神经网络模型中的非线性激活单元（activation)是对人类神经元细胞中轴突末梢（输出）的一种功能模拟。早期的神经网络模型——多层感知机（MLP）中，多采用Sigmoid函数作为激活单元，如图9.（a）所示。然而随着网络层数的加深，Sigmoid函数引起的梯度消失和梯度爆炸问题严重影响了后向传播算法的实用性。为解决上述问题，多种多样新的非线性激活单元被提出，其中ReLU函数是目前应用最为广泛的激活函数，“2D卷积-ReLU激活函数-池化”三个算子相串接而成的基本单元就构成了CNN网络的一个完整层，如下述TensorFlow代码片段所示：

```python
L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')
L1 = tf.nn.relu(L1)
L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],
         strides=[1, 2, 2, 1], padding='SAME')
```

​         ReLU激活函数如图9.（b）所示，其定义为：
$$
\phi(x)=max(0,x)
$$
​         <img src=".\img\2\ReLU-example.png" alt="ReLU-example" style="zoom:40%;" />

**图10. 激活稀疏效果示意图**

​         该函数使得负半轴的输入都产生0值的输出，图10（a）中的特征图经过非线性激活后，产生（b）中的输出，可以看出激活函数给网络带了另一种类型的稀疏性，红圈标识了特征图中被稀疏化的元素。为了利用上述稀疏特性来压缩模型，文献[6]提出了一种神经元剪枝算法。首先，定义网络中每个神经元经ReLU映射后输出的零值平均百分比（APoZ）指标为：

<img src=".\img\2\APoZ.png" alt="APoZ" style="zoom:40%;" />

​       这里，$O_c^{(i)}$表示网络第$i$层中第$c$个通道（特征图）的结果，$N$与$M$分别表示用于验证的图像样本个数、及每个特征图的维度，$f\left(  \cdot  \right)$对真的表达式输出1，反之输出0。由于每个特征图均来自一个滤波器（神经元）的卷积及激活映射结果，因此上式衡量了该神经元对一组特定图像的计算结果中0值输出的平均比例。图11给出了在VGG-16网络的CONV5-3层中，利用50,000张ImageNet图像样本计算得到的所有512个神经元的APoZ指标分布图。可以看出大多数神经元的该项指标都分布在93%附近。实际上，该网络中共有631个神经元的APoZ值超过90%。激活函数的引入反映出VGG网络存在着大量的稀疏与冗余性。

<img src=".\img\2\histo-APoZ.png" alt="histo-APoZ" style="zoom:40%;" />

**图11. 激活稀疏算法示例。**ReLU激活函数输出结果中存在高度的稀疏性。

​       对于那些激活输出以0为主的神经元，即APoZ指标接近于1，可以认为其对后续层的计算以及最终推理结果贡献很小，因此可以将这些神经元移除而不会显著影响整个网络的精度。这被称为神经元剪枝，其示意图如图12 所示，由于每个神经元均对应一个滤波器及下一层中的一个通道，因此神经元剪枝可以被视为滤波器剪枝的另一种实现方式，而区别仅仅在于采用了不同的重要度评价方式。为了保持剪枝后的网络的较高精度，神经元剪枝仍沿用了和权重剪枝类似的“预训练-剪枝-微调”迭代循环基本模式。

<img src=".\img\2\neuro prune.png" alt="neuro prune" style="zoom:25%;" />

​                                        **图12. 神经网络全连接层中神经元（结构化）剪枝原理示意图**

​         激活稀疏同样可以在空间域来实现，体现为结构化特征图稀疏。许多视觉任务如目标检测、语义分割中，希望卷积运算仅在感兴趣区域（ROI）来执行。相比整个图像的尺寸，ROI的占比一般是高度稀疏的，因此仅在此区域卷积可以显著减少浮点运算量，并避免背景噪声对识别的影响。结构化特征图稀疏与注意力（attention）机制也是高度关联的，ROI本质上就体现了图像中具有视觉显著性的部分，通常可以借助某些低计算代价的方式获取，如先验知识或低分辨率分割网络，并以掩膜（mask）的形式和图像本身一起作为网络的输入。文献[7]提出了一种SBNet算法，如图13所示。该算法首先将掩膜的空间位置编码为元组（tuple），然后据此从输入图像张量中抽取出对应的切片并重组（Gather）为一个新的张量作为卷积的输入。经过与滤波器的标准卷积运算后，输出张量将再次依据元组列表重新分配（Scatter）回特征图的对应空间位置，完成一次稀疏卷积运算。由于ROI跨层的空间位置不变性，各层的稀疏卷积运算均可以依据该元组实现“张量重组-标准卷积-重新分配”这一计算过程。

<img src=".\img\2\sbnet.png" alt="sbnet" style="zoom:30%;" />

​       **图13. 结构化特征图稀疏算法SB-Net示意图。**借助外部信息给特征图施加方形掩码以实现空间域稀疏。

###   3.3  梯度稀疏

​        在第五章中我们已经看到，大模型（如BERT）由于参数量庞大，单台主机难以满足其训练时的计算资源需求，往往需要借助分布式训练的方式在多台节点（worker）上协作完成。采用分布式随机梯度下降（Distributed SGD）算法可以允许$N$台节点共同完成梯度更新的后向传播训练任务。其中每台主机均保存一份完整的参数拷贝，并负责其中$1/N$参数的更新计算任务。按照一定时间间隔，节点在网络上发布自身更新的梯度，并获取其他$N-1$台节点发布的梯度计算结果，从而更新本地的参数拷贝。整个协作过程如图14所示。

<img src=".\img\2\SGD.png" alt="SGD" style="zoom:30%;" />

​                                                     **图14. 分布式SGD训练中的参数共享机制示意图[8]。**

​        可以看出，随着参与训练任务节点数目的增多，网络上传输的模型梯度数据量也急剧增加，网络通信所占据的资源开销将逐渐超过梯度计算本身所消耗的资源，从而严重影响大规模分布式训练的效率。另一方面，大多数深度网络模型参数的梯度是高度稀疏的，研究表明在分布式SGD算法中，99.9%的梯度交换都是冗余的。图15显示了在AlexNet的训练早期，各层参数梯度的幅值还是较高的。但随着训练周期的增加，参数梯度的稀疏度显著增大，大约30个训练周期后，各层梯度稀疏度都趋于饱和。显然，将这些0值附近的梯度进行交换，对网络带宽资源是一种极大的浪费。

<img src=".\img\2\Alex grad sparse.png" alt="Alex grad sparse" style="zoom:35%;" />

​                                            **图15. 深度神经网络训练中的各层梯度值存在高度稀疏特性**

​       梯度稀疏的目的在于压缩分布式训练时被传输的梯度数据，减少通信资源开销。由于SGD算法产生的梯度数值是高度噪声的，移除其中并不重要的部分并不会显著影响网络收敛过程，与之相反，有时还会带来正则化的效果，从而提升网络精度。梯度稀疏实现的途径包括：1）预设阈值：在网络上仅仅传输那些幅度超过预设阈值的梯度；2）预设比例：在网络上传输根据一定比例选出的一部分正、负梯度更新值；3）梯度丢弃：在各层梯度完成归一化后，按照预设阈值丢弃掉绝大多数幅值较低的梯度。一些梯度稀疏算法在机器翻译任务中可以节省99%的梯度交换，而仅带来0.3%的模型精度损失；可以将ResNet-50模型训练的梯度交换参数量从97MB压缩为0.35MB而并不损失训练精度[9]。

<img src=".\img\2\gradient sparse.png" alt="gradient sparse" style="zoom:50%;" />    

​            **图16. 通过梯度稀疏可以在分布式训练任务中大幅减少通信时间开销从而提升模型训练效率[9]。**

## 四、稀疏与剪枝方法的总结及挑战

​		神经网络稀疏化尽管近年来已经取得了丰富的研究成果，但是作为一个新的研究方向，并没有完全成熟的知识体系，许多固有结论不断地被打破和重建，深度网络模型的稀疏与压缩仍然具有巨大的潜力和研究空间。下面对现有剪枝方法进行小结，并指出未来该领域的部分挑战性问题[10]。

- 早期的剪枝工作多针对非结构化剪枝及启发式方法，当前结构化剪枝及自动化剪枝受到越来越多的关注，因为其更易获得实际的模型加速机会及更高的模型压缩率。
- 卷积层相比全连接层由于其冗余性更小，因而剪枝方法的设计更具挑战性。因此，那些没有大规模全连接结构的神经网络，如ResNet、GoogLeNet、DenseNet等，就要比拥有较多全连接结构的网络，如VGG、AlexNet等更加难于压缩。
- 神经元剪枝相比权重剪枝更易损失模型精度，训练阶段的梯度则拥有最多的稀疏度。如何优化模型稀疏度与剪枝后精度间的折衷仍是当前该领域的研究重点。
- 图8所示的网络剪枝一般流程也并不是一成不变的，最新的研究表明，对于随机初始化网络先进行剪枝操作再进行训练，有可能会比剪枝预训练网络获得更高的稀疏度和精度。因此，究竟剪枝后的残余连接结构与残余权重值两者哪个更为关键，就成为一个开放的研究问题。



**参考文献：**

1. Wright J, Yang A Y, Ganesh A, et al. Robust face recognition via sparse representation. IEEE transactions on pattern analysis and machine intelligence, 2008, 31(2): 210-227.
2. 纪荣嵘,林绍辉,晁飞,吴永坚,黄飞跃.深度神经网络压缩与加速综述.计算机研究与发展,2018,55(09):1871-1888.
3. Hoefler T, Alistarh D, Ben-Nun T, et al. Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks. Journal of Machine Learning Research, 2021, 22(241): 1-124.
4. Li H, Kadav A, Durdanovic I, et al. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016.
5. Liu Z, Li J, Shen Z, et al. Learning efficient convolutional networks through network slimming. Proceedings of the IEEE international conference on computer vision. 2017: 2736-2744.
6. Hu H, Peng R, Tai Y W, et al. Network trimming: A data-driven neuron pruning approach towards efficient deep architectures. arXiv preprint arXiv:1607.03250, 2016.
7. Ren M, Pokrovsky A, Yang B, et al. Sbnet: Sparse blocks network for fast inference. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 8711-8720.
8. Aji A F, Heafield K. Sparse communication for distributed gradient descent. arXiv preprint arXiv:1704.05021, 2017.
9. Lin Y, Han S, Mao H, et al. Deep gradient compression: Reducing the communication bandwidth for distributed training. arXiv preprint arXiv:1712.01887, 2017.
10. Deng L, Li G, Han S, et al. Model compression and hardware acceleration for neural networks: A comprehensive survey. Proceedings of the IEEE, 2020, 108(4): 485-532.













