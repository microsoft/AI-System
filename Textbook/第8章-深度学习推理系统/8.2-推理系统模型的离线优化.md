<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 8.2 推理系统模型的离线优化

本章将围绕推理系统或库针对模型的离线优化，以及低延迟优化策略展开相应的介绍。

- [8.2 推理系统模型的离线优化](#82-推理系统模型的离线优化)
  - [8.2.1 通过程序理解推理优化动机](#821-通过程序理解推理优化动机)
  - [8.2.2 推理(Inference)延迟(Latency)](#822-推理inference延迟latency)
  - [8.2.3 层(Layer)间与张量(Tensor)融合](#823-层layer间与张量tensor融合)
  - [8.2.4 目标后端自动调优](#824-目标后端自动调优)
  - [8.2.5 低精度推理与精度校准](#825-低精度推理与精度校准)
  - [8.2.6 模型压缩](#826-模型压缩)
  - [小结与讨论](#小结与讨论)
  - [参考文献](#参考文献)

推理系统类似传统的Web服务，需要响应用户请求，并保证一定的服务等级协议(例如，响应时间低于100ms)，进而需要通过一定的策略和优化，保持一定的低延迟(Low Latency)，本小节将围绕部署过程中涉及到的延迟问题和相应的优化进行展开。

## 8.2.1 通过程序理解推理优化动机

请读者在开始后面的内容前，通过以下实例思考相应问题：

我们通过一个矩阵乘法实例进行介绍，下面的代码段中，A和B进行矩阵乘计算，结果存储到result矩阵中。

```python
# 本程序通过循环实现矩阵乘 

# 3x3 矩阵
A = [[0.0,0.0,0.0],
    [4.0,5.0,6.0],
    [7.0,8.0,9.0]]

# 3x4 矩阵
B = [[5.0,8.0,1.0,2.0],
    [6.0,0.0,3.0,0.0],
    [4.0,0.0,0.0,1.0]]

# 3x4 结果矩阵 
result = [[0.0,0.0,0.0,0.0],
         [0.0,0.0,0.0,0.0],
         [0.0,0.0,0.0,0.0]]

# 通过X矩阵的行进行迭代 
for i in range(len(A)):
   # 通过Y矩阵的列进行迭代
   for j in range(len(B[0])):
       # 通过Y矩阵的行进行迭代
       for k in range(len(B)):
           result[i][j] += A[i][k] * B[k][j]
```

1. 程序基准运算量预估：预估整体的[MAC](https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation)运算量。
2. 程序基准内存消耗预估：假设A,B,Result的元素数据类型为Float32 (32比特浮点数)，预估程序整体的内存占用(Byte字节为单位)。
3. 思考如果元素数据类型换为Int8 (8比特整数)，内存占用会为多少？MAC会降低为多少？
   - 本问题启示大家思考模型量化的作用和价值
4. 如果底层系统支持元素为0的部分不需要计算直接得到结果为0，那么预估整体的MAC为多少？相比1有多少倍的加速？
   - 本问题启示大家思考模型压缩的作用和价值
5. 如果矩阵乘的循环在函数(func)中如下图所示，设备执行完成后，缓存失效，再执行下一个函数(func)时仍需要加载result和B一次。思考和预估内存到缓存的额外加载代价?
   - 本例启发大家思考内核融合技术优化的作用

```python 
# 模拟未内核融合
def func1(A, B, result1):
    for i in range(len(A)):
        for j in range(len(B[0])):
            for k in range(len(B)):
                result1[i][j] += A[i][k] * B[k][j]
    return result1

def func2(result1, B, result2):
    for i in range(len(B)):
        for j in range(len(B[0])):
                result2[i][j] += result1[i][j] + B[i][j]
    return result2
```

```python 
# 模拟内核融合
def func1(A, B, result1):
    for i in range(len(A)):
        for j in range(len(B[0])):
            for k in range(len(B)):
                result1[i][j] += A[i][k] * B[k][j]

    for i in range(len(B)):
        for j in range(len(B[0])):
                result2[i][j] += result1[i][j] + B[i][j]
    return result2
```
6. 请思考，如果执行此程序的设备缓存线([Cache Line](https://stackoverflow.com/questions/3928995/how-do-cache-lines-work))为3，有2个缓存线，预估缓存未命中率([Cache Misses Rate](https://en.wikipedia.org/wiki/Cache_performance_measurement_and_metric#Introduction_to_types_of_cache_misses))?思考设计缓存失效率最低的访问方式?如果有两个core可以同时启动两个线程计算，之后会如何设计并行计算的策略？
   - 本问题启发读者思考后端代码生成等编译优化策略的作用。

我们对5中func2进行两种访问方式的模拟。
   
```python
# 方式1
# 列访问result1, B
for i in range(len(B)):
    for j in range(len(B[0])):
        result2[i][j] += result1[i][j] + B[i][j]
# 方式2
# 行访问result1，B
for j in range(len(B[0])):
    for i in range(len(B)):
        result2[i][j] += result1[i][j] + B[i][j]
``` 
7. 由于移动端常常使用电池供电，如果我们的功耗和MAC运算量以及时间正相关，思考有什么策略可以减少移动端模型的功耗？
   
那么接下来的内容，我们将围绕代表性的优化策略进行展开。

## 8.2.2 推理(Inference)延迟(Latency)

延迟(Latency)是在客户端给出查询后，推理系统呈现给客户端推理结果所花费的时间。推理服务通常位于关键路径上，因此预测必须既快速同时满足有限的[尾部延迟(Tail Latency)](https://www.cs.utexas.edu/users/mckinley/talks/ds-strangeloop-2017.pdf)才能满足服务水平协议(Service Level Agreement)。

例如：某互联网公司的在线服务等级要求(Service Level Agreement): 次秒(Sub-Second)级别延迟服务水平协议。

为了达到低延迟，推理系统面临以下的挑战：
- 交互式应用程序的低延迟需求通常与离线批处理训练框架设计的目标(例如，大规模，高吞吐，大批次等)不一致
- 简单的模型速度快，复杂的模型更加准确，但是浮点运算量更大
- 次秒(Sub-Second)级别延迟约束制了批尺寸(Batch Size)
- 模型融合或多租容易引起长尾延迟(Long Tail Traffic)现象

推理系统常常可以通过以下几个方向进行模型推理的延迟优化：

- 模型优化，降低访存开销：
  - 层(Layer)间融合(Fusion)或张量(Tensor)融合
  - 目标后端自动调优
  - 内存分配策略调优
- 降低一定的准确度，进而降低计算量，最终降低延迟：
  - 低精度推理与精度校准(Precision Calibration)
  - 模型压缩(Model Compression)
- 自适应批尺寸(Batch Size)：动态调整需要进行推理的输入数据数量
- 缓存(Caching)结果：复用已推理的结果或特征



## 8.2.3 层(Layer)间与张量(Tensor)融合

我们将模型抽象为计算图(Computation Graph)，其中在设备执行的层(Layer)在一些框架内也称作内核(Kernel)或算子(Operator)。我们可以从下图中看到，设备中执行一个内核，一般需要以下几个步骤和时间开销，启动内核，读取数据到设备，执行内核，结果数据写回主存。

<center> <img src="./img/2/8-2-1-fusion.png" width="400" height="140" /></center>
<center>图8-2-1. 内核执行时间线</center>

我们从上面的实例可以观察到需要对模型的层间和张量融合的原因：
- 相对于内核启动开销和每个层的张量数据读写成本，内核(Kernel)计算通常非常快
- 导致内存带宽瓶颈和可用设备(Device)资源的利用不足

我们可以将融合问题抽象为一个优化问题：
- 目标：最小化设备(Device)访存和最大化设备利用率
- 策略：搜索计算图的最优融合策略

如下图所示，左图为未进行融合的深度学习模型网络结构，右图为经过TensorRT优化和融合之后的深度学习模型网络结构。优化后将很多小的层融合为大的层，这样减少了大量的数据跨设备读写的开销，提升性能。

<center> <img src="./img/2/8-2-2-fusionopt.png" width="700" height="500" /></center>
<center>图8-2-2. TensorRT使用内核融合进行模型执行优化</center>

代表性的开源内核融合系统还有很多，例如，微软开源的[NNFusion](https://github.com/microsoft/nnfusion)。

## 8.2.4 目标后端自动调优

由于深度学习模型中的层的计算逻辑可以转为矩阵运算，而矩阵计算又可以通过循环进行实现。对于循环的实现，由于在不同的推理CPU和硬件设备中，有不同的缓存和内存大小以及访存带宽，进而如何针对不同的设备进行循环的并行化和考虑数据的局部性降低访存开销，可以抽象为一个搜索空间巨大的优化问题。目前有很多代表性的工作在围绕这个问题展开，通过基于专家经验的规则，构建代价模型，抽象为优化问题或者通过机器学习自动化学习和预测是常用的方式。由于在编译优化章节对此有较为详细的介绍，本章不再展开相应的内容，读者可以参考编译优化章节内容理解。

代表性的开源后端自动调优编译器或工具有：[TVM](https://tvm.apache.org/),[Hadlide](https://dl.acm.org/doi/10.1145/2491956.2462176),[Ansor](https://tvm.apache.org/2021/03/03/intro-auto-scheduler)等。


## 8.2.5 低精度推理与精度校准

推理阶段相比训练阶段可以适当降低精度，进而降低推理延迟:
- 大多数深度学习框架都以完整的32位精度（FP32）训练神经网络。
- 使用较低的精度会导致较小的模型大小，较低的内存利用率和延迟以及较高的吞吐量。
- 对模型进行充分训练后，由于不需要进行反向传播求梯度，因此推理计算相比训练常常使用的FP32，推理可以使用半精度FP16甚至INT8张量运算降低计算量和内存占用。

如图所示，不同的精度对应的比特数和取值范围不同，进而产生了不同学习性能和系统性能的权衡。此小节内容将在后续稀疏性章节有更详细的的介绍。

<style>table{margin: auto;}</style>


$$
\begin{array}{|c|c|c|}
    \hline
    & 比特数 & 取值范围 \\
    \hline 
    FP32& 32 & -3.4 \times 10^{38} \sim + 3.4 \times 10^{38} \\
    \hline
    FP16& 16 & -65504 \sim +65504 \\
    \hline 
    INT8 & 8 & -128 \sim +127 \\
    \hline
\end{array}
$$

<center>图8-2-3. 精度取值范围</center>


## 8.2.6 模型压缩

模型压缩(Model Compression)是通过一定的算法和策略，在保证模型预测效果满足一定要求的前提下，尽可能地降低模型权重的大小，进而降低模型的推理计算量，内存开销和模型文件的空间占用，最终降低模型推理延迟。因为其可观的收益和一定的预测效果保证，在模型部署和推理之前，通过模型压缩进行模型精简是常常使用的技术。

我们可以将模型压缩抽象为有约束的优化问题：

优化目标： 在所选择的策略下，最小化模型大小
$$min_{policy_i}\{Model\_Size(Policy_i)\}$$

约束：在所选择的策略进行模型压缩后，保证预测精度满足SLA

$$Accuracy(Policy_i) \geq Accuracy\_SLA$$

常常使用的模型压缩技术有如下几种：

- 参数裁剪(Parameter Pruning)和共享(Sharing）
  - 剪枝(Pruning)
  - 量化(Quantization)
  - 编码(Encoding)
- 低秩分解(Low-Rank Factorization）
- 知识精炼(Knowledge Distillation）
- …

在后面章节中我们将有更为细节的模型压缩内容叙述，本小节只介绍与推理相关的模型压缩技术和问题。

## 小结与讨论

本小节主要围绕推理系统的延迟展开讨论，我们总结了低延迟的推理需求，以及围绕这个设计目标，推理系统常常使用的优化策略。优化延迟的目标，受到空间与准确度的约束，深度学习的推理过程相比训练过程在适当情况下可以牺牲一定准确度进而提升延迟。

看完本章内容后，我们可以思考以下几点问题：
层间与张量融合受到哪些约束?
推理和训练优化内存分配策略的侧重点是否有不同？

## 参考文献 

- [Park, Jongsoo et al. “Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications.” ArXiv abs/1811.09886 (2018): n. pag.](https://arxiv.org/abs/1811.09886)
- [Crankshaw, Daniel et al. “Clipper: A Low-Latency Online Prediction Serving System.” NSDI (2017).](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf)
- [Denis Baylor, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, Mustafa Ispir, Vihan Jain, Levent Koc, Chiu Yuen Koo, Lukasz Lew, Clemens Mewald, Akshay Naresh Modi, Neoklis Polyzotis, Sukriti Ramesh, Sudip Roy, Steven Euijong Whang, Martin Wicke, Jarek Wilkiewicz, Xin Zhang, and Martin Zinkevich. 2017. TFX: A TensorFlow-Based Production-Scale Machine Learning Platform. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '17). Association for Computing Machinery, New York, NY, USA, 1387–1395. DOI:https://doi.org/10.1145/3097983.3098021](https://research.google/pubs/pub46484/)
- [Olston, Christopher et al. “TensorFlow-Serving: Flexible, High-Performance ML Serving.” ArXiv abs/1712.06139 (2017): n. pag.](https://arxiv.org/abs/1712.06139)
- [Jeong-Min Yun, Yuxiong He, Sameh Elnikety, and Shaolei Ren. 2015. Optimal Aggregation Policy for Reducing Tail Latency of Web Search. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '15). Association for Computing Machinery, New York, NY, USA, 63–72. DOI:https://doi.org/10.1145/2766462.2767708](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/samehe-2015sigir.optimalaggregation.pdf)
- [Cheng, Yu et al. “A Survey of Model Compression and Acceleration for Deep Neural Networks.” ArXiv abs/1710.09282 (2017): n. pag.](https://arxiv.org/abs/1710.09282)
- [CSE 599W: System for ML - Model Serving](https://dlsys.cs.washington.edu/)
- https://developer.nvidia.com/deep-learning-performance-training-inference 
- [Han, Song et al. “Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding.” arXiv: Computer Vision and Pattern Recognition (2016): n. pag.](https://arxiv.org/abs/1510.00149) 
- [Song Han, Jeff Pool, John Tran, and William J. Dally. 2015. Learning both weights and connections for efficient neural networks. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1 (NIPS'15). MIT Press, Cambridge, MA, USA, 1135–1143.](https://arxiv.org/abs/1506.02626)
- [DEEP LEARNING DEPLOYMENT WITH NVIDIA TENSORRT](https://developer.nvidia.com/blog/deploying-deep-learning-nvidia-tensorrt/)
- [Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Frédo Durand, and Saman Amarasinghe. 2013. Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines. SIGPLAN Not. 48, 6 (June 2013), 519–530. DOI:https://doi.org/10.1145/2499370.2462176](https://people.csail.mit.edu/jrk/halide-pldi13.pdf)
- [Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Meghan Cowan, Haichen Shen, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. 2018. TVM: an automated end-to-end optimizing compiler for deep learning. In Proceedings of the 13th USENIX conference on Operating Systems Design and Implementation (OSDI'18). USENIX Association, USA, 579–594.](https://arxiv.org/abs/1802.04799)
- [8-bit Inference with TensorRT](https://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf)
- [BLASX: A High Performance Level-3 BLAS Library for Heterogeneous Multi-GPU Computing](https://arxiv.org/abs/1510.05041)