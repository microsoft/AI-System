<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->


# 8.3 推理系统的高吞吐与高效率优化

- [8.3 推理系统的高吞吐与高效率优化](#83-推理系统的高吞吐与高效率优化)
  - [8.3.1 推理系统的吞吐量](#831-推理系统的吞吐量)
  - [8.3.2 加速器模型并发执行](#832-加速器模型并发执行)
  - [8.3.3 动态批尺寸](#833-动态批尺寸)
  - [8.3.4 多模型装箱(Packing)](#834-多模型装箱packing)
  - [小结与讨论](#小结与讨论)
  - [参考文献](#参考文献)

推理系统类似传统的Web服务，需要应对不断增加的用户请求数量，提高吞吐量(throughput)，提升资源利用率(utilization)，本小节将围绕推理系统中涉及到的吞吐和效率问题进行展开。

## 8.3.1 推理系统的吞吐量

推理系统不仅要追求低延迟，在服务客户端请求的过程中，要提供高吞吐量请求服务的支持。推理系统需要高吞吐的目的：
- 应对突发的请求数量暴增
- 不断扩展的用户和设备的需求

推理系统达到高吞吐的常见优化策略有：
- 利用加速器并行
  - 批处理请求(request)
  - 利用优化的BLAS矩阵运算库, SIMD指令和GPU等加速器加速，提升利用率
- 自适应批尺寸(Batch Size)
- 多模型装箱使用加速器
- 扩展到多模型副本(Replica)部署

在接下来的内容中，我们将介绍增加模型推理吞吐量的常用策略。

## 8.3.2 加速器模型并发执行

加速器的低效率使用常常由于所执行的负载的运算量不够高或者由于等待请求或IO等造成资源空置和浪费。如图所示，如果设备(device)中只部署了单个模型，由于等待批处理请求，可能造成GPU空闲。

<center> <img src="./img/3/8-3-2-lowutil.png" width="400" height="180" /></center>
<center>图8-3-2. 加速器的低效率使用</center>

为了应对，单加速器运行多模型，推理系统可以通过时分复用策略，并发(concurrent)执行模型，将等待时的计算资源分配给其他模型进行执行，提升整体的推理吞吐量。

<center> <img src="./img/3/8-3-3-multimodel.png" width="400" height="120" /></center>
<center>图8-3-1. 加速器的低效率使用</center>

## 8.3.3 动态批尺寸

例如，如下图所示，Nvidia所做的在V100上的推理性能基准测试，从图中可以看到，随着批次(batch size)不断增加，模型推理的吞吐量在不断上升，但同时推理延迟(latency)在下降。

<center> <img src="./img/3/8-3-1-throughput.png" width="400" height="200" /></center>
<center>图8-3-2. Nvidia深度学习推理性能测试</center>

由于通过提升批次(batch size)可以提升吞吐量，对于较高请求数量和频率的场景，通过大的批次可以提升吞吐量。但是推理系统要注意，没有免费的午餐，随着吞吐量上升的还有延迟，推理系统推理在动态调整批尺寸时需要满足一定的延迟约束。

定义优化问题：

$$max_{batch\_size}\{Throughput(batch\_size)\}$$

约束：

$$latency(batch\_size) + overhead(batch\_size) \leq latency\_SLA $$

动态批处理尺寸(Batch Size)的尺寸增长和减少，可以借鉴[Additive Increase Multiplicative Decrease (AIMD)](https://en.wikipedia.org/wiki/Additive_increase/multiplicative_decrease) 策略。加法增加/乘法减少 (AIMD) 算法是一种反馈控制算法，其被应用在 TCP 拥塞控制中。 AIMD 将没有拥塞时拥塞窗口的线性增长与检测到拥塞时的指数减少相结合。使用 AIMD 拥塞控制的多个流最终将收敛到均衡使用共享链路。

AIMD在动态批尺寸中使用的策略：
- 加性增加(Addictive Increase): 
  - 将批次大小累加增加固定数量，直到处理批次的延迟超过目标延迟为止。
- 乘性减少(Multiplicative Decrease): 
  - 当达到后，执行一个小的乘法回退。例如，将批次大小减少10％。
  - 因为最佳批次大小不会大幅波动，所以使用的退避常数要比其他应用场景使用的AIMD方案小得多。


## 8.3.4 多模型装箱(Packing)


<center> <img src="./img/3/8-3-4-fragments.png" width="400" height="180" /></center>
<center>图8-3-3. 空闲GPU资源</center>

加速器的低效率使用

在延迟SLA约束下，模型在指定的GPU下按最大吞吐量进行分配，但是可能仍有空闲资源

装箱

Best-fit策略装箱(bin-packing)碎片化的模型请求

<center> <img src="./img/3/8-3-5-packing.png" width="400" height="300" /></center>
<center>图8-3-3. 空闲GPU资源</center>


## 小结与讨论

本小节主要围绕推理系统的高吞吐与高效率的优化展开讨论，我们总结了高吞吐和高效率需求，以及围绕这个设计目标，推理系统常常使用的优化策略。

看完本章内容后，我们可以思考以下几点问题：
当前吞吐量和效率的优化策略是否会对延迟产生影响？
设计其他策略进行吞吐量或使用效率的优化？

## 参考文献

- [Park, Jongsoo et al. “Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications.” ArXiv abs/1811.09886 (2018): n. pag.](https://arxiv.org/abs/1811.09886)
- [Crankshaw, Daniel et al. “Clipper: A Low-Latency Online Prediction Serving System.” NSDI (2017).](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf)
- [Denis Baylor, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, Mustafa Ispir, Vihan Jain, Levent Koc, Chiu Yuen Koo, Lukasz Lew, Clemens Mewald, Akshay Naresh Modi, Neoklis Polyzotis, Sukriti Ramesh, Sudip Roy, Steven Euijong Whang, Martin Wicke, Jarek Wilkiewicz, Xin Zhang, and Martin Zinkevich. 2017. TFX: A TensorFlow-Based Production-Scale Machine Learning Platform. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '17). Association for Computing Machinery, New York, NY, USA, 1387–1395. DOI:https://doi.org/10.1145/3097983.3098021](https://research.google/pubs/pub46484/)
- [Olston, Christopher et al. “TensorFlow-Serving: Flexible, High-Performance ML Serving.” ArXiv abs/1712.06139 (2017): n. pag.](https://arxiv.org/abs/1712.06139)
- [Jeong-Min Yun, Yuxiong He, Sameh Elnikety, and Shaolei Ren. 2015. Optimal Aggregation Policy for Reducing Tail Latency of Web Search. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '15). Association for Computing Machinery, New York, NY, USA, 63–72. DOI:https://doi.org/10.1145/2766462.2767708](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/samehe-2015sigir.optimalaggregation.pdf)
- [Cheng, Yu et al. “A Survey of Model Compression and Acceleration for Deep Neural Networks.” ArXiv abs/1710.09282 (2017): n. pag.](https://arxiv.org/abs/1710.09282)
- [CSE 599W: System for ML - Model Serving](https://dlsys.cs.washington.edu/)
- https://developer.nvidia.com/deep-learning-performance-training-inference 
- [Han, Song et al. “Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding.” arXiv: Computer Vision and Pattern Recognition (2016): n. pag.](https://arxiv.org/abs/1510.00149) 
- [Song Han, Jeff Pool, John Tran, and William J. Dally. 2015. Learning both weights and connections for efficient neural networks. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1 (NIPS'15). MIT Press, Cambridge, MA, USA, 1135–1143.](https://arxiv.org/abs/1506.02626)
- [DEEP LEARNING DEPLOYMENT WITH NVIDIA TENSORRT](https://developer.nvidia.com/blog/deploying-deep-learning-nvidia-tensorrt/)
- [Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Frédo Durand, and Saman Amarasinghe. 2013. Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines. SIGPLAN Not. 48, 6 (June 2013), 519–530. DOI:https://doi.org/10.1145/2499370.2462176](https://people.csail.mit.edu/jrk/halide-pldi13.pdf)
- [Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Meghan Cowan, Haichen Shen, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. 2018. TVM: an automated end-to-end optimizing compiler for deep learning. In Proceedings of the 13th USENIX conference on Operating Systems Design and Implementation (OSDI'18). USENIX Association, USA, 579–594.](https://arxiv.org/abs/1802.04799)
- [8-bit Inference with TensorRT](https://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf)
