<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 8.5 开发、训练与部署的全生命周期管理-MLOps

MLOps 是一种用于人工智能（包含机器学习与深度学习）全生命周期的工程化方法，它借鉴 DevOps 思想将机器学习（例如，模型的训练与推理）开发（Dev 部分）与机器学习系统（深度学习框架，自动化机器学习系统）统一起来操作与维护（Ops 部分）。MLOps 的过程希望标准化和自动化机器学习全生命周期的关键步骤。MLOps 提供了一套标准化的流程和工具，用于构建、部署、快速可靠地运行机器学习全流程和机器学习系统。相比于传统的 DevOps 不同之处在于，当用户部署 Web 服务时，用户关心的是每秒查询数（QPS）、负载均衡（Load Balance）等。在部署机器学习模型时，用户还需要关注模型准确度，数据的变化、模型的变化等。这些是 MLOps 所要解决的新的挑战。 

- [8.5 开发、训练与部署的全生命周期管理-MLOps](#85-开发训练与部署的全生命周期管理-mlops)
  - [8.5.1 MLOps的生命周期](#851-mlops的生命周期)
  - [8.5.2 MLOps工具链（Toolchain）](#852-mlops工具链toolchain)
  - [8.5.3 线上发布与回滚策略](#853-线上发布与回滚策略)
  - [8.6.4 MLOps持续集成，持续交付（CI/CD）](#864-mlops持续集成持续交付cicd)
    - [数据流](#数据流)
  - [8.6.5 MLOps 工具与服务](#865-mlops-工具与服务)
  - [小结与讨论](#小结与讨论)
  - [参考文献](#参考文献)

## 8.5.1 MLOps的生命周期

推理系统本身就像传统 Web 服务发布代码包一样，需要定期发布模型，提供新功能或更好的效果。
类似于传统软件工程中应用程序开发团队在创建和管理应用程序。借鉴传统的软件工程最佳实践，业界使用了 DevOps，这是管理应用程序开发周期操作的行业标准。为了应对深度学习全生命周期的挑战，如图 8-5-1 所示，组织需要一种将 DevOps 的敏捷性带入机器学习生命周期的方法，业界称这种方法为[MLOps](https://docs.microsoft.com/en-us/learn/modules/start-ml-lifecycle-mlops/2-mlops-introduction)

<center> <img src="./img/4/8-4-5-mlops.png" width="500" height="300" /></center>
<center>图 8-5-1. 模型构建与部署 </center>

如图 8-5-2 所示的时序图，其中训练模型的循环我们已经在第 7 章介绍。那么训练完成的模型如何和推理系统建立联系呢，我们可以看到一般训练完成后经过以下一些步骤进行模型部署与推理：
1. 模型测试阶段，一般此阶段含有功能性测试（离线测试和在线 [A/B测试](https://en.wikipedia.org/wiki/A/B_testing)等）和非功能性测试（性能测试等）。一般在互联网在线服务中需要先进行离线测试数据测试，测试达标后进行在线 A/B 测试，分配真实流量进行功能性测试，一旦模型达标则可以进行模型部署阶段。同时本阶段需要做一定的非功能性测试，例如测试性能等。一旦不达标针对非功能性指标，可以通过模型压缩，量化，编译优化等进行优化，如果对功能性指标不达标，则需要重新训练与调优。
2. 进行到模型部署阶段，需要用户进行针对的平台编译与代码生成，模型打包或者针对服务端制作镜像。一般可以部署到服务端或者移动端（边缘端）。一旦服务中出现问题，模型还可以通过一定策略进行回滚使用原来的模型。
3. 模型部署后则可以对用户请求进行服务，也就是我们所说的模型推理（Inference）过程，响应用户请求。例如，如果用户请求的是物体检测模型，则用户提交图片作为请求，模型推理进行物体检测，将物体检测的结果作为请求的响应再返回给用户。

整个过程不断随着开发者研发训练新的模型进而不断更新线上模型，提升应用的服务水准。
<center> <img src="./img/5/8-5-6-fulllifecyclemlops.png" ch="500" width="2000" height="800" /></center>
<center>图 8-5-2. 深度学习的全生命周期图 （点击图片查看大图） (<a href="">图片来源</a>)</center>

接下来，我们以 MLOps 中的代表性的问题进行介绍。

***相关概念解释***

A/B 测试：通常用于在线推理服务中，是模型上线前的在线测试环节。A/B 测试一般采用对用户流量进行分桶，即将其分成实验组和对照组用户，对实验组的用户部署新模型返回请求应答，对照组的用户沿用旧模型，在分桶的过程中，需要注意样本的独立性和无偏性，保证用户每次只能分到同一个桶中，最终验证新模型和旧模型的效果优势，如果有优势可以上线新模型。对需要在线部署的模型，A/B 测试有以下的好处：1. 离线评估测试无法完全消除过拟合的影响。2. 离线评估无法完全还原线上的工程环境。例如，数据延迟、缺失等情况。3. 线上系统的某些评测指标在离线评估中无法计算，例如点击率等。

## 8.5.2 MLOps工具链（Toolchain）

在系统（System），软件工程（Software Engineering）社区，常常会看到有相应的深度学习模型管理的工作，我们也可以将其拓展为覆盖整个 MLOps 的工具链。我们整理了一些有趣且实用的工具和相关工作，读者可以对感兴趣的工具进行尝试和使用。

- ***模型动物园（Model Zoo）***
模型动物园（Model Zoo）是开源框架和公司开源与组织机器学习和深度学习预训练模型的常用方式。当前很多场景下由于使用预训练模型可以大幅减少训练代价，或者很多开发者没有足够资源从头训练造成集成预训练模型并进行微调的方式。例如图 8-5-2 所示，[Hugging Face](https://huggingface.co/)就通过语言模型的Model Zoo不断拓展社区，用户不断在其模型动物园下载和微调（Fine-tuning）预训练模型，并形成语言模型场景应用广泛的工具和生态，使得当前很多自然语言场景下，用户常常使用 Hugging Face 库而不是底层的 PyTorch 或者 TensorFlow 构建模型。

<center> <img src="./img/5/8-5-4-modelzoo.png" width="1000" height="500" /></center>
<center>图 8-5-3. Hugging Face 语言预训练模型动物园 （2022/5/11 日数据） </center>

- ***模型和工作流可视化***

深度学习模型可视化能够让开发者更好的调试，设计和理解深度学习模型，并在平时的开发过程中被得以越来越多的使用。例如，我们可以通过 [Netron](https://github.com/lutzroeder/netron)，[MMdnn](https://github.com/microsoft/MMdnn/tree/master/mmdnn/visualization) 等工具进行模型结构可视化。读者可以参考如下实例体验可视化过程。
1. 准备模型文件，可以导出或者从模型动物园下载
2. 安装 Netron
3. 将模型文件拖拽到 Netron
4. 可视化与交互，读者可以从可视化界面中观察到，模型数据流图，算子类型，超参数和张量尺寸等信息。
如图 8-5-4，我们下载 Keras MobileNet 并通过 Netron 可视化，我们点击一个 Conv2D 可以看到其超参数和输入输出张量尺寸。
<center> <img src="./img/5/8-5-10-netron.png" width="1000" height="700" /></center>
<center>图 8-5-4. Netron可视化Keras MobileNet</center>

工作流可视化的方式可以以低代码的形式极大提升生产力，同时促进模块化复用组件。同时对整个开发的流程，很多公司也开发了可视化低代码的开发接口，进行工作流拖拽式编程，让用户可以快速搭建机器学习流水线，复用模块。例如 [Azure Machine Learning designer](https://docs.microsoft.com/en-us/azure/machine-learning/concept-designer)，如下图实例，用户可以配置一个工作流中的模块，进行数据清洗，整个过程无需书写代码。

<center> <img src="./img/5/8-5-9-visualizeworkflow.png" width="1000" height="500" /></center>
<center>图 8-5-5. 可视化机器学习流水线工作流 (<a href="https://docs.microsoft.com/en-us/azure/machine-learning/concept-designer">图片引用</a>) </center>

- ***模型转换***

由于框架之间互用预训练模型以及训练到部署的运行时环境不同，产生了模型转换的需求，这其中 [MMdnn](https://github.com/microsoft/MMdnn) 和 [ONNX](https://onnx.ai/) 为代表的工具正是为解决这个需求而产生。

- ***模型和搜索空间合法性（Validity）验证***

模型本身容易产生各种类型错误（Type Errors）。例如，超参数不合法（Illegal Hyperparameter），张量不匹配（Tensor Mismatch）等。有相关工作如 [Refty](https://www.microsoft.com/en-us/research/publication/refty-refinement-types-for-valid-deep-learning-models/)，[Pythia](https://github.com/plast-lab/doop-mirror/blob/master/docs/pythia.md) 等通过静态检测的方式在训练前提前验证和规避相应的错误，保证模型的结构正确性。

- ***模型调试器***

由于模型训练过程中，无论研究工作还是实际工作中充满了大量的技巧（Trick），对于小白用户希望像专家一下进行模型的训练优化保证交付模型，为了应对这种需求，一些工具和云服务提供模型和训练过程的调试，这样让即使没有相关经验的用户也能训练出效果好的模型。例如，[Amazon SageMaker Debugger](https://docs.aws.amazon.com/sagemaker/latest/dg/train-debugger.html) 是亚马逊推出的针对机器学习的调试器，实时调试、监控和分析训练作业、检测非收敛条件、通过消除瓶颈优化资源利用率、缩短训练时间并降低机器学习模型的成本。学术界中软工社区的工作 [DeepDiagnosis ICSE 22](https://arxiv.org/pdf/2112.04036.pdf) 提出自动化诊断的思路，通过在框架注册回调函数（Call Back）收集信息，自动化的分析当前的训练指标，给出用户错误位置定位，并给出修改建议。例如，其会分析，权重变化，激活问题，精确度没有升高，损失没有降低等问题，并定位问题位置并给出建议，例如调整学习率，不合适的数据等。

<center> <img src="./img/5/8-5-5-deepDiagnosis.png" width="800" height="200" /></center>
<center>图 8-5-4. DeepDiagnosis 自动诊断和推荐修复 (<a href="https://arxiv.org/pdf/2112.04036.pdf">图片引用</a>) </center>

- ***模型与数据监控***

当前由于超参数变的越来越多，实验也越来越多，对模型和实验的监控与管理也变的越发重要。同时由于很多实际场景中，数据不断的增长，造成原有的数据分布产生了变化，而人工智能模型又对数据分布的变化较为敏感，所以不断监测数据分布变化，并在一定条件下触发模型重新训练，也是整个生命周期中非常重要的环节。

例如，用户在大规模 MLOps 实践过程中，可以对以下环节构建监控体系。
  - 模型性能：查看模型的预测有多准确。看看模型性能是否会随着时间的推移而衰减，进而决定是否需要重新训练。
  - 模型训练和再训练：在训练和再训练期间查看学习曲线（Learning Curve）、训练模型预测分布或混淆矩阵（Confusion Matrix）。
  - 输入/输出分布：查看模型中输入数据和特征的分布是否发生了变化。预测的类分布是否随时间变化、这些东西可能与数据漂移（Drift）有关。
  - 硬件指标：查看模型在训练和推理期间使用了多少 GPU，内存等硬件资源，甚至是多少费用。
  - CI/CD 流水线：查看来自 CI/CD 管道作业的评估并直观地比较它们。这样用户和机构能对当前整个流程和环节健康有清晰认识。

例如，Neptune 提供了一个展示[实例界面](https://app.neptune.ai/common/example-project-tensorflow-keras/experiments)，用户可以观察当前这类监控产品的监控项。我们可以观察到每个实验，模型训练效果，超参数配置等信息，综合管理实验。

<center> <img src="./img/5/8-5-8netptune.png" width="1000" height="500" /></center>
<center>图 8-5-5. Neptune模型与实验监控 (<a href="https://app.neptune.ai/common/example-project-tensorflow-keras/experiments?split=tbl&dash=charts&viewId=44675986-88f9-4182-843f-49b9cfa48599">图片引用</a>) </center>

一些开源系统中，例如 NNI，MLflow 也提供模型及实验监控追踪功能。通过 NNI 的界面，我们可以观察到实验，模型训练效果，超参数配置等信息，综合管理实验，还可以看到超参数的相关性等针对超参数搜索场景的信息。

<center> <img src="./img/5/nnimonitor.gif" width="1000" height="500" /></center>
<center>图 8-5-6. NNI 模型与实验监控 (<a href="https://github.com/microsoft/nni/blob/master/docs/static/img/webui.gif">图片引用</a>) </center>

- ***模型版本管理***

由于模型上线使用后并不是一劳永逸，随着数据分布变化，或算法工程师设计新的模型效果更好，需
要不断上线新模型替代原有模型。这就要求当前的推理系统，模型部署的服务器文件系统或者网络文件系统做好相应的模型版本管理和策略，方便模型更新与回滚。

我们以 NVIDIA [Triton 推理服务器的版本管理](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md#version-policy)为例：每个模型都有一个或多个版本。在模型配置文件中，模型版本策略属性可以设置以下的策略：

- 全部（All)：模型存储库中可用的所有模型版本都可用于推理。版本策略：
  ```version_policy: { all: {}}```

- 最新（Latest）: 只有存储库中模型的最新 “n” 个版本可用于推理。模型的最新版本是数字最大的版本号。版本策略：
  ```version_policy: { latest: { num_versions: 2}}```

- 特定（Specific）: 特定：只有模型的特别列出的版本可用于推理。版本策略：
  ```version_policy: { specific: { versions: [1,3]}}```

如果没有配置模型版本策略 *Latest* (配置 n=1) 作为默认配置，代表推理系统会使用最新的版本模型。

如下实例（[实例来源](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md#version-policy))），Triton规定，最小模型配置必须指定平台和/或后端属性 platform 、最大批尺寸 max_batch_size 属性以及模型的输入和输出张量 input，output。
```
  # 平台类型
  platform: "tensorrt_plan"
  # 最大推理批尺寸
  max_batch_size: 8
  # 每个模型输入和输出都必须指定名称、数据类型和形状。 为输入或输出张量指定的名称必须与模型预期的名称相匹配。
  input [
    {
      name: "input0"
      data_type: TYPE_FP32
      dims: [ 16 ]
    },
    {
      name: "input1"
      data_type: TYPE_FP32
      dims: [ 16 ]
    }
  ]
  output [
    {
      name: "output0"
      data_type: TYPE_FP32
      dims: [ 16 ]
    }
  ]
  # 配置模型版本管理策略
  version_policy: { all { }}
```

## 8.5.3 线上发布与回滚策略

在 MLOps 中，其中较为重要的一个问题是模型的版本管理：线上发布，回滚等策略。因为近些年，软件逐渐由客户端软件演化为在线部署的服务，而模型最终也是部署于一定的软件系统中，所以越来越多的模型部署于在线服务（Online Service）中。在线服务每隔一段时间训练出的新版本模型替换线上模型，但是可能存在缺陷（Defect），另外如果新版本模型发现缺陷需要回滚。同时在整个模型的生命周期中，还有很多代表性的服务，工具和系统也越来越充当着非常重要的角色。

<center> <img src="./img/5/8-5-2-modelmanagement.png" width="700" height="600" /></center>
<center>图 8-5-7. 推理系统中的模型生命周期管理管理(<a href="https://developer.nvidia.com/tensorrt">图片引用</a>) </center>

如图 8-5-7 所示，训练完成的模型被保存在模型库，并被推理系统所管理与加载，上线后还有遵循一定的策略保证正确性和可回滚。

模型生命周期管理的具体策略实例还有有：[TensorFlow-Serving 中提出的金丝雀（Canary）策略，回滚（Roll Back）策略](https://arxiv.org/pdf/1712.06139.pdf)。
- 金丝雀策略
  - 当获得一个新训练的模型版本时，当前服务的模型成为第二新版本时，用户可以选择同时保持这两个版本
  - 将所有推理请求流量发送到当前两个版本，比较它们的效果
  - 一旦对最新版本达标，用户就可以切换到仅使用最新版本
  - 该策略需要更多的高峰资源，避免将用户暴露于缺陷模型
- 回滚策略
  - 如果在当前的主要服务版本上检测到缺陷，则用户可以请求切换到特定的较旧版本
  - 卸载和装载的顺序应该是可配置的
  - 当问题解决并且获取到新的安全版本模型时，从而结束回滚

当然业界每家公司也会根据自身在线服务特点设计和定制个性化的策略和线上管理流程，其目标都是朝着敏捷不断迭代模型，可回滚，可靠等角度去设计和迭代优化的。

## 8.6.4 MLOps持续集成，持续交付（CI/CD）

<center> <img src="./img/5/8-5-7-cicdmlops.png" width="2000" height="700"/></center>
<center>图 8-5-8. MLOps持续集成，持续交付（CI/CD）的时序图实例 （点击图片查看大图）</center>

### 数据流

如图 8-5-8.本实例涉及一个ML算法工程师开发一个新模型并上线到推理系统服务的 DevOps 流水线（Pipeline）。 图中流程如下：

1. 算法工程师更改设计新模型，并更改当前项目程序源代码。
2. 所做的代码更改提交到源代码管理存储库，例如 GitHub。代码管理库触发一定测试，例如单元测试等。如果不通过，则算法工程师修复，并重走刚才的流程。
3. 为了启动持续集成 (CI) 过程，Webhook 会触发一个 Jenkins 项目生成。
4. Jenkins 触发一定的测试，构建镜像，并将镜像推送到镜像中心。如果不通过，则算法工程师修复，并重走刚才的流程。
5. Jenkins 通过之前定义好的流程，再触发系统到训练平台进行模型训练，拉取镜像，下载数据模型，启动训练作业，离线测试与验证，达到要求的模型存储到文件系统，并触发新的镜像构建。如果不通过，则算法工程师修复或优化模型，并重走刚才的流程。
6. Jenkins 通过持续部署 (CD) 将这个更新的容器映像部署到生产环境推理系统。如果不通过，则开发人员修复，并重走刚才的流程。
7. 终端用户不断向推理系统发起请求，返回响应，平台运维工程师进行线上推理系统服务运维。如果需要热修复推理系统性能和安全性问题，则运维工程师或者推理系统工程师服务修复。如果是模型缺陷或者效果不好则算法工程师修复，也可能触发自动回滚旧模型的策略，并重走刚才的流程。
8. 重复1-8的过程。

同时，请读者思考相比传统软件工程中的 CI/CD 的 DevOps 流程，在 MLOps 中我们遇到了哪些新的变化与挑战呢？
- 被管理的资产（Assets）：模型和数据相比以往代码更多。
- 大量的实验管理：传统程序需要程序员进行程序开发，而当前以实验驱动模型开发。
- 可解释性差：模型可解释性差，难以调试与问题诊断，更需要模型监控覆盖度更高。
- 进度管理：由于模型训练有很大非确定性，造成很难像传统软件工程进行工作量及交付时间预估。

***经典回顾***

我们仍可以参考 DevOps 领域的经典原则并将其用于MLOps中进而达到更敏捷和高质量的MLOps。
例如，Atlassian 总结了一份 [DevOps 原则](https://www.atlassian.com/devops/what-is-devops)，我们可以参考应用到MLOps：
- 合作（Collaboration）：开发，运维和算法团队紧密协作。
- 自动化（Automation）：尽可能让MLOps全生命周期自动化。
- 持续提升（Continuous Improvement）：构建实验交付流水线，持续提升交付的模型效果。
- 以客户为中心的行动（Customer-Centric Action）：构建与终端使用模型的客户循环反馈，以客户需求为中心设计和优化流水线。
- 以终为始进行创作（Create With The End In Mind）：理解用户对模型的需求，构建解决实际需求的模型。

## 8.6.5 MLOps 工具与服务

- 开源 MLOps 工具：
  - [MLflow](https://mlflow.org/)：是由 UCB 开源的 MLOps 工具，其提供标准化的 API，Python 社区友好。MLflow 能够跟踪指标、参数和工件，打包模型和可重现的机器学习项目，并将模型部署到批处理或实时服务平台。基于这些现有功能，MLflow 模型库供了一个中央存储库来管理模型部署生命周期。同时整个生命周期通过 CI/CD 进行管理，达到持续集成，持续部署。

- 公有云 MLOps 服务：
  - [Amazon SageMaker MLOps](https://aws.amazon.com/sagemaker/mlops/)：此服务为亚马逊云服务机器学习平台 SageMaker 提供的 MLOps 服务，其提供全流程的机器学习声明周期管理，并提供特色的模型调试与诊断功能，将专家级调试经验自动化。
  - [Azure Machine Learning](https://azure.microsoft.com/en-us/services/machine-learning/mlops/)：微软 Azure Machine Learning 服务也提供了 MLOps 的功能，并持续构建全套解决方法，与微软云集成度最佳。
  - [Google Cloud MLOps](https://cloud.google.com/resources/mlops-whitepaper)：Google Cloud 的 MLOps 功能不仅较早提出，同时向业界发布白皮书，布道相关概念与技术。

## 小结与讨论

本章我们主要介绍目前比较火的概念 MLOps，其实在人工智能开始工程化的时间点，MLOps 已经发生了，我们借鉴软件工程和 DevOps 的成熟理论，让人工智能生命周期工程化，流程化，进而达到提升人工智能模型和产品研发的生产力。

## 参考文献

- https://docs.microsoft.com/en-us/learn/modules/start-ml-lifecycle-mlops/2-mlops-introduction
- https://en.wikipedia.org/wiki/A/B_testing
- https://huggingface.co/
- https://github.com/lutzroeder/netron
- https://github.com/microsoft/MMdnn/tree/master/mmdnn/visualization
- https://docs.microsoft.com/en-us/azure/machine-learning/concept-designer
- https://github.com/microsoft/MMdnn
- https://onnx.ai/
- https://www.microsoft.com/en-us/research/publication/refty-refinement-types-for-valid-deep-learning-models/
- https://github.com/plast-lab/doop-mirror/blob/master/docs/pythia.md
- https://docs.aws.amazon.com/sagemaker/latest/dg/train-debugger.html
- [Wardat, Mohammad et al. “DeepDiagnosis: Automatically Diagnosing Faults and Recommending Actionable Fixes in Deep Learning Programs.” ArXiv abs/2112.04036 (2021): n. pag.](https://arxiv.org/abs/2112.04036)
- https://app.neptune.ai/common/example-project-tensorflow-keras/experiments
- https://app.neptune.ai/common/example-project-tensorflow-keras/experiments?split=tbl&dash=charts&viewId=44675986-88f9-4182-843f-49b9cfa48599
- https://github.com/microsoft/nni
- https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md#version-policy
- https://developer.nvidia.com/tensorrt
- [Olston, Christopher et al. “TensorFlow-Serving: Flexible, High-Performance ML Serving.” ArXiv abs/1712.06139 (2017): n. pag.](https://arxiv.org/abs/1712.06139)
- https://mlflow.org/
- https://aws.amazon.com/sagemaker/mlops/
- https://azure.microsoft.com/en-us/services/machine-learning/mlops/
- https://cloud.google.com/resources/mlops-whitepaper
- [Park, Jongsoo et al. “Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications.” ArXiv abs/1811.09886 (2018): n. pag.](https://arxiv.org/abs/1811.09886)
- [Crankshaw, Daniel et al. “Clipper: A Low-Latency Online Prediction Serving System.” NSDI (2017).](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf)
- [Denis Baylor, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, Mustafa Ispir, Vihan Jain, Levent Koc, Chiu Yuen Koo, Lukasz Lew, Clemens Mewald, Akshay Naresh Modi, Neoklis Polyzotis, Sukriti Ramesh, Sudip Roy, Steven Euijong Whang, Martin Wicke, Jarek Wilkiewicz, Xin Zhang, and Martin Zinkevich. 2017. TFX: A TensorFlow-Based Production-Scale Machine Learning Platform. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '17). Association for Computing Machinery, New York, NY, USA, 1387–1395. DOI:https://doi.org/10.1145/3097983.3098021](https://research.google/pubs/pub46484/)
- https://www.microsoft.com/en-us/research/project/ai-tooling-and-mlops/
- https://zhuanlan.zhihu.com/p/59685112
- https://docs.microsoft.com/en-us/learn/modules/start-ml-lifecycle-mlops/2-mlops-introduction
- https://aws.amazon.com/sagemaker/mlops/
- https://azure.microsoft.com/en-us/services/machine-learning/mlops/
- https://cloud.google.com/resources/mlops-whitepaper
- [Industrializing AI for the Enterprise with 
NVIDIA DGX Systems and MLOps](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/dgx-ready-software/dgx-mlops-whitepaper.pdf)
- https://databricks.com/blog/2020/04/15/databricks-extends-mlflow-model-registry-with-enterprise-features.html
