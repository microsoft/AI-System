<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->


# 8.4 推理系统的运行期优化


推理系统类似传统的Web服务，需要应对不断增加的用户请求数量，提高吞吐量(Throughput)，提升资源利用率(Utilization)，本小节将围绕推理系统中涉及到的运行期（Runtime）优化，以及吞吐和效率问题进行展开。

- [8.4 推理系统的运行期优化](#84-推理系统的运行期优化)
  - [8.4.1 推理系统的吞吐量](#841-推理系统的吞吐量)
  - [8.4.2 加速器模型并发执行](#842-加速器模型并发执行)
  - [8.4.3 动态批尺寸](#843-动态批尺寸)
  - [8.4.4 多模型装箱(Bin Packing)](#844-多模型装箱bin-packing)
  - [8.4.5 内存分配策略调优](#845-内存分配策略调优)
  - [8.4.6 深度学习模型内存分配算法实验与模拟研究](#846-深度学习模型内存分配算法实验与模拟研究)
    - [(1) 数据获取](#1-数据获取)
    - [(2) 评测指标设定](#2-评测指标设定)
    - [(3) 算法实现与评测](#3-算法实现与评测)
  - [小结与讨论](#小结与讨论)
  - [参考文献](#参考文献)

## 8.4.1 推理系统的吞吐量

推理系统不仅要追求低延迟，在服务客户端请求的过程中，要提供高吞吐量请求服务的支持。推理系统需要高吞吐的目的：
- 应对突发的请求数量暴增
- 不断扩展的用户和设备的需求

推理系统达到高吞吐的常见优化策略有：
- 利用加速器并行
  - 批处理请求(Request)
  - 利用优化的BLAS矩阵运算库, SIMD指令和GPU等加速器加速，提升利用率
- 自适应批尺寸(Batch Size)
- 多模型装箱使用加速器
- 扩展到多模型副本(Replica)部署

在接下来的内容中，我们将介绍增加模型推理吞吐量的常用策略。

## 8.4.2 加速器模型并发执行

加速器的低效率使用常常由于所执行的负载的运算量不够高或者由于等待请求或IO等造成资源空置和浪费。如图所示，如果设备(Device)中只部署了单个模型，由于等待批处理请求，可能造成GPU空闲。

<center> <img src="./img/3/8-3-2-lowutil.png" width="600" height="230" /></center>
<center>图8-3-2. 加速器的低效率使用</center>

为了应对，单加速器运行多模型，推理系统可以通过时分复用策略，并发(Concurrent)执行模型，将等待时的计算资源分配给其他模型进行执行，提升整体的推理吞吐量(Throughput)和设备利用率(Utilization)。

<center> <img src="./img/3/8-3-3-multimodel.png" width="600" height="180" /></center>
<center>图8-3-1. 加速器的低效率使用</center>

## 8.4.3 动态批尺寸

例如，如下图所示，NVIDIA所做的在V100上的推理性能基准测试，从图中可以看到，随着批尺寸(Batch Size)不断增加，模型推理的吞吐量在不断上升，但同时推理延迟(Latency)在下降。

<center> <img src="./img/3/8-3-1-throughput.png" width="700" height="400" /></center>
<center>图8-3-2. NVIDIA深度学习推理性能测试 <a href="https://developer.nvidia.com/deep-learning-performance-training-inference">图片来源</a> </center>

由于通过提升批尺寸(Batch Size)可以提升吞吐量，对于较高请求数量和频率的场景，通过大的批次可以提升吞吐量。但是推理系统要注意，没有免费的午餐，随着吞吐量上升的还有延迟，推理系统推理在动态调整批尺寸时需要满足一定的延迟约束。

定义优化问题：

$$max_{batch\_size}\{Throughput(batch\_size)\}$$

约束：

$$latency(batch\_size) + overhead(batch\_size) \leq latency\_SLA $$

动态批处理尺寸(Batch Size)的尺寸增长和减少，在相关工作[Clipper](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf)中有借鉴[Additive Increase Multiplicative Decrease (AIMD)](https://en.wikipedia.org/wiki/Additive_increase/multiplicative_decrease) 策略。加法增加/乘法减少(AIMD) 算法是一种反馈控制算法，其被应用在TCP拥塞控制中。AIMD 将没有拥塞时拥塞窗口的线性增长与检测到拥塞时的指数减少相结合。使用 AIMD 拥塞控制的多个流最终将收敛到均衡使用共享链路。

AIMD在动态批尺寸中使用的策略：
- 加性增加(Addictive Increase): 
  - 将批次大小累加增加固定数量，直到处理批次的延迟超过目标延迟为止。
- 乘性减少(Multiplicative Decrease): 
  - 当达到后，执行一个小的乘法回退。例如，将批次大小减少10％。
  - 因为最佳批次大小不会大幅波动，所以使用的退避常数要比其他应用场景使用的AIMD方案小得多。

接下来我们以NVIDIA Triton推理服务器为例看实际系统中支持的动态批尺寸功能。NVIDIA Triton支持[动态批尺寸器（Dynamic Batcher）](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md#dynamic-batcher)，动态批处理允许服务器组合推理请求，从而动态创建批处理。创建一批请求通常会增加吞吐量。动态批处理器应该用于无状态模型。动态创建的批次分布到为模型配置的所有模型实例。一般可以通过下面的流程实施：

以下步骤是为每个模型调整动态批处理器的推荐过程。其还推荐，可以使用[模型分析器](https://github.com/triton-inference-server/server/blob/main/docs/model_analyzer.md)自动搜索不同的动态批处理器配置。

- 确定模型的最大批量大小max_batch_size 。

- 将以下内容添加到模型配置中以启用动态批处理器。默认策略是：动态批处理器将创建尽可能大的批次并且成批时不会耽误（和下面介绍的此配置相关max_queue_delay_microseconds），直到最大批次大小（max_batch_size） 。

```yml
    dynamic_batching { }
```

- 使用性能分析器确定默认动态批处理器配置提供的延迟和吞吐量，相比于上面提到的AIMD策略，当前为人工介入，读者也可以将AIMD的思路应用于配置批尺寸的实验尝试中。根据性能约束，确定合适的批尺寸，之后再进行配置。

如果默认配置导致延迟值在您的延迟预算范围内，请尝试以下一种或两种方法来权衡增加的延迟以增加吞吐量：

  - 增加最大批量大小。max_batch_size 属性表示模型支持的最大批量大小，可用于 Triton 可以利用的批处理类型。

  - 将批处理延迟（Batch Delay）max_queue_delay_microseconds 设置为非零值，动态批处理器可以配置为允许请求在调度器中延迟有限的时间，以允许其他请求加入动态批处理。尝试增加延迟值直到超过延迟预算以查看对吞吐量的影响。

- 大多数模型不应使用首选批尺寸（Preferred Batch Sizes）。仅当该批大小导致性能明显高于其他批大小时，才应配置首选批大小。
preferred_batch_size 属性指示动态批处理器应尝试创建的批处理大小。

下面的[代码实例](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md#preferred-batch-sizes)配置Triton应用动态批尺寸，并且配置首选批尺寸（Preferred Batch Sizes）为4或者8。

```yml
  dynamic_batching {
    preferred_batch_size: [ 4, 8 ]
  }
```

## 8.4.4 多模型装箱(Bin Packing)

在延迟服务等级协议(SLA)约束下，模型在指定的GPU下按最大吞吐量进行分配，但是可能仍有空闲资源，造成加速器的低效率使用。

如图所示，有些设备(Device)上的算力较高，部署的模型运算量又较小，使得设备上可以装箱(Bin Packing)多个模型，共享使用设备。

<center> <img src="./img/3/8-3-4-fragments.png" width="600" height="300" /></center>
<center>图8-3-3. 空闲GPU资源</center>

如图所示，推理系统可以通过[最佳匹配(best fit)](https://en.wikipedia.org/wiki/Best-fit_bin_packing)策略装箱(Bin Packing)模型，将碎片化的模型(例如，model1和model2)请求由共享的设备进行推理。这样不仅提升了推理系统的吞吐(Throughput)，也提升了设备的利用率(Utilization)。

<center> <img src="./img/3/8-3-5-packing.png" width="700" height="500" /></center>
<center>图8-3-4. 空闲GPU资源</center>

## 8.4.5 内存分配策略调优

由于设备或服务端内存是紧缺资源，推理系统常常也需要考虑做内存的分配策略的优化，进而能够服务更大的模型。

- 目标：最小化内存占用和内存分配调用开销
- 约束：保证延迟服务等级协议(SLA)
- 优化策略：
  - 缓存分配器(Cached Allocator)：推理系统预先申请设备内存，构建推理系统的内存管理器，减少设备内存分配释放等API调用代价(例如，[cudaFree调用可能阻塞它的调用者，直到所有GPU上所有先前排队的工作完成](https://arxiv.org/abs/1912.01703))。
  - [预取(Prefetch)和卸载(Off-loading)](https://arxiv.org/abs/1602.08124)：异步地将设备内存数据在读取前和产生后和主存进行换入换出，减少设备内存的数据压力。
  - [算子融合(Fusion)](https://github.com/microsoft/nnfusion)：将中间结果在缓存层给下一阶段的内核使用，减少中间结果回写吞吐和延迟更低的设备内存或者主存的开销。

读者也可以参考相关工作（例如，[DNNMem](https://dl.acm.org/doi/abs/10.1145/3368089.3417050)，[vDNN](https://dl.acm.org/doi/10.5555/3195638.3195660)等）进一步了解深度学习模型内存的分配占用分类和优化策略。


## 8.4.6 深度学习模型内存分配算法实验与模拟研究

此实验需要读者自行参考下面方式收集数据集。

### (1) 数据获取
读者可以参考库中提供的脚本读取数据并了解数据模式。

- 日志收集

我们通过下面的实例或者脚本进行深度学习作业的日志收集，进而获取张量尺寸信息，分配(Allocation)和释放(Deallocation)信息。
```shell
# 假设TensorFlow 1.13版本
export TF_CPP_MIN_VLOG_LEVEL=2 
python tf_infer.py # 程序中为常见的TensorFlow推理或训练程序
```
- 张量分配与释放相关日志抽取，进而获取张量的大小与分配释放顺序

例如，其中一条张量分配日志如下
```
20XX-XX-XX 12:20:44.472769: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorAllocation { step_id: 2 kernel_name: "vgg_16/pool3/MaxPool" tensor { dtype: DT_FLOAT shape { dim { size: 64 } dim { size: 256 } dim { size: 28 } dim { size: 28 } } allocation_description { requested_bytes: 51380224 allocated_bytes: 51380224 allocator_name: "GPU_0_bfc" allocation_id: 101 has_single_reference: true ptr: 140615920648192 } } }
```

分配张量日志实例与关键字
```
MemoryLogTensorAllocation
```

释放张量日志实例与关键字
```
MemoryLogTensorDeallocation
```

[参考文档](https://stackoverflow.com/questions/36331419/tensorflow-how-to-measure-how-much-gpu-memory-each-tensor-takes/43189934#43189934)

### (2) 评测指标设定

读者可以设计以下评测指标，进行算法策略设计：

- 最小化时间开销
  - malloc()、free() 例程在一般情况下应该尽可能快。
- 最小化空间(Space)占用
  - 分配器不应该浪费空间，它应该从系统中获取尽可能少的内存，并且应该以最小化碎片(Fragmentation)的方式维护内存。碎片是程序不使用的连续内存块中的无法再分配的内存空闲浪费区域。
- 最小化局部性(Locality)
  - 分配通常在彼此附近一起使用的内存块。 这有助于在程序执行期间最大限度地减少页面和缓存未命中。
- 其他

### (3) 算法实现与评测

- 内存分配器模拟与算法设计
  - 假设设计的内存分配器依赖调用底层NVIDIA [cudaFree](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1ga042655cbbf3408f01061652a075e094)和[cudaMalloc](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g37d37965bfb4803b6d4e59ff26856356)原语获取设备原始内存，但是此API调用有一定的时间代价。参考本文或其他测试数据中进行量化模拟API调用时间开销：[当使用cudaMalloc和cudaFree时，增加矩阵大小时性能退化分析](https://arxiv.org/pdf/1510.05041.pdf)。
- 设计实现malloc(), free()接口并实现内部算法
  
读者可以选用以经典算法作为基准测试（例如，[DNNMem](https://dl.acm.org/doi/10.1145/3368089.3417050)中介绍的主流框架内存分配算法，或者传统[内存分配器策略](https://www.cs.tufts.edu/~nr/cs257/archive/doug-lea/malloc.html)），设计新的算法，并通过收集的数据模拟，看能否提升当前目标，超越基准算法，并进行结果分析，形成分析报告或论文。



## 小结与讨论

本小节主要围绕推理系统的高吞吐与高效率的优化展开讨论，我们总结了推理系统高吞吐和高效率需求，以及围绕这个设计目标，推理系统常常使用的优化策略。

看完本章内容后，我们可以思考以下几点问题：
当前吞吐量和效率的优化策略是否会对延迟产生影响？
设计其他策略进行吞吐量或使用效率的优化？

## 参考文献

- [Park, Jongsoo et al. “Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications.” ArXiv abs/1811.09886 (2018): n. pag.](https://arxiv.org/abs/1811.09886)
- [Crankshaw, Daniel et al. “Clipper: A Low-Latency Online Prediction Serving System.” NSDI (2017).](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf)
- [Denis Baylor, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, Mustafa Ispir, Vihan Jain, Levent Koc, Chiu Yuen Koo, Lukasz Lew, Clemens Mewald, Akshay Naresh Modi, Neoklis Polyzotis, Sukriti Ramesh, Sudip Roy, Steven Euijong Whang, Martin Wicke, Jarek Wilkiewicz, Xin Zhang, and Martin Zinkevich. 2017. TFX: A TensorFlow-Based Production-Scale Machine Learning Platform. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '17). Association for Computing Machinery, New York, NY, USA, 1387–1395. DOI:https://doi.org/10.1145/3097983.3098021](https://research.google/pubs/pub46484/)
- [Olston, Christopher et al. “TensorFlow-Serving: Flexible, High-Performance ML Serving.” ArXiv abs/1712.06139 (2017): n. pag.](https://arxiv.org/abs/1712.06139)
- [Jeong-Min Yun, Yuxiong He, Sameh Elnikety, and Shaolei Ren. 2015. Optimal Aggregation Policy for Reducing Tail Latency of Web Search. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '15). Association for Computing Machinery, New York, NY, USA, 63–72. DOI:https://doi.org/10.1145/2766462.2767708](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/samehe-2015sigir.optimalaggregation.pdf)
- [Cheng, Yu et al. “A Survey of Model Compression and Acceleration for Deep Neural Networks.” ArXiv abs/1710.09282 (2017): n. pag.](https://arxiv.org/abs/1710.09282)
- [CSE 599W: System for ML - Model Serving](https://dlsys.cs.washington.edu/)
- https://developer.nvidia.com/deep-learning-performance-training-inference 
- [Han, Song et al. “Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding.” arXiv: Computer Vision and Pattern Recognition (2016): n. pag.](https://arxiv.org/abs/1510.00149) 
- [Song Han, Jeff Pool, John Tran, and William J. Dally. 2015. Learning both weights and connections for efficient neural networks. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1 (NIPS'15). MIT Press, Cambridge, MA, USA, 1135–1143.](https://arxiv.org/abs/1506.02626)
- [DEEP LEARNING DEPLOYMENT WITH NVIDIA TENSORRT](https://developer.nvidia.com/blog/deploying-deep-learning-nvidia-tensorrt/)
- [Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Frédo Durand, and Saman Amarasinghe. 2013. Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines. SIGPLAN Not. 48, 6 (June 2013), 519–530. DOI:https://doi.org/10.1145/2499370.2462176](https://people.csail.mit.edu/jrk/halide-pldi13.pdf)
- [Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Meghan Cowan, Haichen Shen, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. 2018. TVM: an automated end-to-end optimizing compiler for deep learning. In Proceedings of the 13th USENIX conference on Operating Systems Design and Implementation (OSDI'18). USENIX Association, USA, 579–594.](https://arxiv.org/abs/1802.04799)
- [8-bit Inference with TensorRT](https://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf)
