<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

## 2.1 神经网络的基本工作原理简介

- [2.1 神经网络的基本工作原理简介](#21-神经网络的基本工作原理简介)
  - [2.1.1 神经元细胞的数学模型](#211-神经元细胞的数学模型)
    - [输入 input](#输入-input)
    - [权重 weights](#权重-weights)
    - [偏移 bias](#偏移-bias)
    - [求和计算 sum](#求和计算-sum)
    - [激活函数 activation](#激活函数-activation)
    - [小结](#小结)
  - [2.1.2 神经网络的主要功能](#212-神经网络的主要功能)
    - [回归（Regression）或者叫做拟合（Fitting）](#回归regression或者叫做拟合fitting)
    - [分类（Classification）](#分类classification)
  - [2.1.3 为什么需要激活函数](#213-为什么需要激活函数)
    - [生理学上的例子](#生理学上的例子)
    - [激活函数的作用](#激活函数的作用)
    - [常用激活函数](#常用激活函数)
- [小结与讨论](#小结与讨论)
- [参考文献](#参考文献)

### 2.1.1 神经元细胞的数学模型

神经网络由基本的神经元组成，图 2.1.1 就是一个神经元的数学/计算模型，便于我们用程序来实现。

<img src="./img/NeuranCell.png" width="400"/>

图 2.1.1 神经元计算模型

#### 输入 input

$(x_1,x_2,x_3)$ 是外界输入信号，一般是一个训练数据样本的多个属性，比如，我们要预测一套房子的价格，那么在房屋价格数据样本中，$x_1$ 可能代表了面积，$x_2$ 可能代表地理位置，$x_3$ 可能代表朝向。另外一个例子是，$(x_1,x_2,x_3)$ 分别代表了(红,绿,蓝)三种颜色，而此神经元用于识别输入的信号是暖色还是冷色。

#### 权重 weights

$(w_1,w_2,w_3)$ 是每个输入信号的权重值，以上面的 $(x_1,x_2,x_3)$ 的例子来说，$x_1$ 的权重可能是 $0.92$，$x_2$ 的权重可能是 $0.2$，$x_3$ 的权重可能是 $0.03$。当然权重值相加之后可以不是 $1$。

#### 偏移 bias

还有个 $b$ 是怎么来的？一般的书或者博客上会告诉你那是因为 $y=wx+b$，$b$ 是偏移值，使得直线能够沿 $Y$ 轴上下移动。这是用结果来解释原因，并非 $b$ 存在的真实原因。从生物学上解释，在脑神经细胞中，一定是输入信号的电平/电流大于某个临界值时，神经元细胞才会处于兴奋状态，这个 $b$ 实际就是那个临界值。亦即当：

$$w_1 \cdot x_1 + w_2 \cdot x_2 + w_3 \cdot x_3 \geq t$$

时，该神经元细胞才会兴奋。我们把t挪到等式左侧来，变成$(-t)$，然后把它写成 $b$，变成了：

$$w_1 \cdot x_1 + w_2 \cdot x_2 + w_3 \cdot x_3 + b \geq 0$$

于是 $b$ 诞生了！

#### 求和计算 sum

$$
\begin{aligned}
Z &= w_1 \cdot x_1 + w_2 \cdot x_2 + w_3 \cdot x_3 + b \\\\
&= \sum_{i=1}^m(w_i \cdot x_i) + b
\end{aligned}
$$

在上面的例子中 $m=3$。我们把$w_i \cdot x_i$变成矩阵运算的话，就变成了：

$$Z = W \cdot X + b$$

#### 激活函数 activation

求和之后，神经细胞已经处于兴奋状态了，已经决定要向下一个神经元传递信号了，但是要传递多强烈的信号，要由激活函数来确定：

$$A=\sigma{(Z)}$$

如果激活函数是一个阶跃信号的话，会像继电器开合一样咔咔的开启和闭合，在生物体中是不可能有这种装置的，而是一个渐渐变化的过程。所以一般激活函数都是有一个渐变的过程，也就是说是个曲线，如图 2.1.2 所示。

<img src="./img/activation.png" />

图 2.1.2 激活函数图像

可以看到，无论 Z(input) 有多大，都可以映射到 A(output) 的 (0,1) 区间内。至此，一个神经元的工作过程就结束了。

#### 小结

- 一个神经元可以有多个输入。
- 一个神经元只能有一个输出，这个输出可以同时输入给多个神经元。
- 一个神经元的 $w$ 的数量和输入的数量一致。
- 一个神经元只有一个 $b$。
- $w$ 和 $b$ 有人为的初始值，在训练过程中被不断修改。
- $A$ 可以等于 $Z$，即激活函数不是必须有的。
- 一层神经网络中的所有神经元的激活函数必须一致。


### 2.1.2 神经网络的主要功能

#### 回归（Regression）或者叫做拟合（Fitting）

单层的神经网络能够模拟一条二维平面上的直线，从而可以完成线性分割任务。而理论证明，两层神经网络可以无限逼近任意连续函数。图 2.1.3 所示就是一个两层神经网络拟合复杂曲线的实例。

<img src="./img/regression.png">

图 2.1.3 回归/拟合示意图

所谓回归或者拟合，其实就是给出 x 值输出 y 值的过程，并且让 y 值与样本数据形成的曲线的距离尽量小，可以理解为是对样本数据的一种骨架式的抽象。

以图 2.1.3 为例，蓝色的点是样本点，从中可以大致地看出一个轮廓或骨架，而红色的点所连成的线就是神经网络的学习结果，它可以“穿过”样本点群形成中心线，尽量让所有的样本点到中心线的距离的和最近。

#### 分类（Classification）

如图 2.1.4，二维平面中有两类点，红色的和蓝色的，用一条直线肯定不能把两者分开了。

<img src="./img/classification_sample.png">

图 2.1.4 分类示意图

我们使用一个两层的神经网络可以得到一个非常近似的结果，使得分类误差在满意的范围之内。图2-4中那条淡蓝色的曲线，本来并不存在，是通过神经网络训练出来的分界线，可以比较完美地把两类样本分开，所以分类可以理解为是对两类或多类样本数据的边界的抽象。

图 2.1.3 和图 2.1.4 的曲线形态实际上是一个真实的函数在 $[0,1]$ 区间内的形状，其原型是：

$$f(x)=0.4x^2 + 0.3x\sin(15x) + 0.01\cos(50x)-0.3$$

这么复杂的函数，一个两层的神经网络是如何做到的呢？其实从输入层到隐藏层的矩阵计算，就是对输入数据进行了空间变换，使其可以被线性可分，然后在输出层画出一个分界线。而训练的过程，就是确定那个空间变换矩阵的过程。因此，多层神经网络的本质就是对复杂函数的拟合。我们可以在后面的试验中来学习如何拟合上述的复杂函数的。

神经网络的训练结果，是一大堆的权重组成的数组（近似解），并不能得到上面那种精确的数学表达式（数学解析解）。

### 2.1.3 为什么需要激活函数

#### 生理学上的例子

人体骨关节是动物界里最复杂的生理结构，一共有8个重要的大关节：肩关节、
肘关节、腕关节、髋关节、膝关节、踝关节、颈关节、腰关节。

人的臂骨，腿骨等，都是一根直线，人体直立时，也是一根直线。但是人在骨关节和肌肉组织的配合下，可以做很多复杂的动作，原因就是关节本身不是线性结构，而是一个在有限范围内可以任意活动的结构，有一定的柔韧性。

比如肘关节，可以完成小臂在一个二维平面上的活动。加上肩关节，就可以完成胳膊在三维空间的活动。再加上其它关节，就可以扩展胳膊活动的三维空间的范围。

用表 2.1.1 来对比人体运动组织和神经网络组织。

表 2.1.1 人体运动组织和神经网络组织的对比

|人体运动组织|神经网络组织|
|---|---|
|支撑骨骼|网络层次|
|关节|激活函数|
|肌肉韧带|权重参数|
|学习各种运动的动作|前向+反向训练过程|

激活函数就相当于关节。

#### 激活函数的作用

看以下的例子：

$$Z1 = X \cdot W1 + B1$$

$$Z2 = Z1 \cdot W2 + B2$$

$$Z3 = Z2 \cdot W3 + B3$$

把 $Z1,Z2$ 带入 $Z3$ 的表达式：

$$
\begin{aligned}
Z3&=Z2 \cdot W3 + B3 \\\\
&=(Z1 \cdot W2 + B2) \cdot W3 + B3 \\\\
&=((X \cdot W1 + B1) \cdot W2 + B2) \cdot W3 + B3 \\\\
&=X \cdot (W1\cdot W2 \cdot W3) + (B1 \cdot W2 \cdot W3+B2 \cdot W2+B3) \\\\
&=X \cdot W+B
\end{aligned}
$$

$Z1,Z2,Z3$ 分别代表三层神经网络的计算结果。最后可以看到，不管有多少层，总可以归结到 $XW+B$ 的形式，这和单层神经网络没有区别。

如果我们不运用激活函数的话，则输出信号将仅仅是一个简单的线性函数。线性函数一个一级多项式。线性方程是很容易解决的，但是它们的复杂性有限，并且从数据中学习复杂函数映射的能力更小。一个没有激活函数的神经网络将只不过是一个线性回归模型罢了，不能解决现实世界中的大多数非线性问题。

没有激活函数，我们的神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。这就是为什么我们要使用人工神经网络技术，诸如深度学习，来理解一些复杂的事情，一些相互之间具有很多隐藏层的非线性问题。

<img src="./img/LinearvsActivation.png"/>

图 2.1.5 从简单到复杂的拟合

图 2.1.5 展示了几种拟合方式，最左侧的是线性拟合，中间的是分段线性拟合，右侧的是曲线拟合，只有当使用激活函数时，才能做到完美的曲线拟合。

#### 常用激活函数

常用激活函数分为两大类，挤压型函数（俗称Sigmoid函数）和半线性函数（又可以叫非饱和型激活函数）。

- Sigmoid 对数几率函数

对数几率函数（Logistic Function，简称对率函数），也就是常说的 Sigmoid 函数。

公式：

$$Sigmoid(z) = \frac{1}{1 + e^{-z}} \rightarrow a$$

导数：

$$Sigmoid'(z) = a(1 - a)$$

注意，如果是矩阵运算的话，需要在公式中使用$\odot$符号表示按元素的矩阵相乘：$a\odot (1-a)$，后面不再强调。


函数图像：

<img src="./img/sigmoid.png" width="500" />

图 2.1.6 Sigmoid函数图像

优点：

从函数图像来看，Sigmoid函数的作用是将输入压缩到 $(0,1)$ 这个区间范围内，这种输出在0~1之间的函数可以用来模拟一些概率分布的情况。它还是一个连续函数，导数简单易求。  

从数学上来看，Sigmoid函数对中央区的信号增益较大，对两侧区的信号增益小，在信号的特征空间映射上，有很好的效果。 

从神经科学上来看，中央区酷似神经元的兴奋态，两侧区酷似神经元的抑制态，因而在神经网络学习方面，可以将重点特征推向中央区，
将非重点特征推向两侧区。

缺点：

指数计算代价大。

反向传播时梯度消失：从梯度图像中可以看到，Sigmoid的梯度在两端都会接近于0，根据链式法则，如果传回的误差是$\delta$，那么梯度传递函数是$\delta \cdot a'$，而$a'$这时接近零，也就是说整体的梯度也接近零。这就出现梯度消失的问题，并且这个问题可能导致网络收敛速度比较慢。


- Tanh函数

TanHyperbolic，即双曲正切函数。

公式：

$$
Tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} = (\frac{2}{1 + e^{-2z}}-1) \rightarrow a \tag{3}
$$

即

$$
Tanh(z) = 2 \cdot Sigmoid(2z) - 1 \tag{4}
$$

导数：

$$
Tanh'(z) = (1 + a)(1 - a)
$$


函数图像：

图 2.1.7 是双曲正切的函数图像。

<img src="./img/tanh.png" width="500" />

图 2.1.7 双曲正切函数图像

优点：

具有Sigmoid的所有优点。

无论从理论公式还是函数图像，这个函数都是一个和Sigmoid非常相像的激活函数，他们的性质也确实如此。但是比起Sigmoid，Tanh减少了一个缺点，就是他本身是零均值的，也就是说，在传递过程中，输入数据的均值并不会发生改变，这就使他在很多应用中能表现出比Sigmoid优异一些的效果。

缺点：

exp指数计算代价大。梯度消失问题仍然存在。

- 其它挤压型函数

图 2.1.8 展示了其它S型函数，除了$Tanh(x)$以外，其它的基本不怎么使用，目的是告诉大家这类函数有很多，但是常用的只有Sigmoid和Tanh两个。

<img src="./img/others.png" />

图 2.1.8 其它S型函数

- ReLU函数 

Rectified Linear Unit，修正线性单元，线性整流函数，斜坡函数。

公式：

$$ReLU(z) = max(0,z) = \begin{cases} 
  z, & z \geq 0 \\\\ 
  0, & z < 0 
\end{cases}$$

导数：

$$ReLU'(z) = \begin{cases} 1 & z \geq 0 \\\\ 0 & z < 0 \end{cases}$$

<img src="./img/relu.png" width="500"/>

图 2.1.9 线性整流函数ReLU

优点：

- 反向导数恒等于1，更加有效率的反向传播梯度值，收敛速度快；
- 避免梯度消失问题；
- 计算简单，速度快；
- 活跃度的分散性使得神经网络的整体计算成本下降。

缺点：

无界。

梯度很大的时候可能导致的神经元“死”掉。

这个死掉的原因是什么呢？是因为很大的梯度导致更新之后的网络传递过来的输入是小于零的，从而导致ReLU的输出是0，计算所得的梯度是零，然后对应的神经元不更新，从而使ReLU输出恒为零，对应的神经元恒定不更新，等于这个ReLU失去了作为一个激活函数的作用。问题的关键点就在于输入小于零时，ReLU回传的梯度是零，从而导致了后面的不更新。在学习率设置不恰当的情况下，很有可能网络中大部分神经元“死”掉，也就是说不起作用了。

- Leaky ReLU函数

LReLU，带泄露的线性整流函数。

公式：

$$LReLU(z) = \begin{cases} z & z \geq 0 \\\\ \alpha \cdot z & z < 0 \end{cases}$$

导数：

$$LReLU'(z) = \begin{cases} 1 & z \geq 0 \\\\ \alpha & z < 0 \end{cases}$$

函数图像：

函数图像如图 2.1.10 所示。

<img src="./img/leakyRelu.png" width="500"/>

图 2.1.10 LeakyReLU的函数图像

优点：

继承了ReLU函数的优点。

Leaky ReLU同样有收敛快速和运算复杂度低的优点，而且由于给了$z<0$时一个比较小的梯度$\alpha$,使得$z<0$时依旧可以进行梯度传递和更新，可以在一定程度上避免神经元“死”掉的问题。

- Softplus函数

公式：

$$Softplus(z) = \ln (1 + e^z)$$

导数：

$$Softplus'(z) = \frac{e^z}{1 + e^z}$$

函数图像：

Softplus的函数图像如图 2.1.11 所示。

<img src="./img/softplus.png" width="500"/>

图 2.1.11 Softplus 的函数图像

- ELU函数

公式：

$$ELU(z) = \begin{cases} z & z \geq 0 \\ \alpha (e^z-1) & z < 0 \end{cases}$$

导数：

$$ELU'(z) = \begin{cases} 1 & z \geq 0 \\ \alpha e^z & z < 0 \end{cases}$$

函数图像：

ELU的函数图像如图 2.1.12 所示。

<img src="./img/elu.png" width="500"/>

图 2.1.12 ELU 的函数图像

## 小结与讨论

本小节主要围绕神经网络基本概念展开，介绍了神经元细胞的数学模型，神经网络的主要功能，以及为什么需要激活函数。

读者思考这些数学模型如何通过代码进行实现？

## 参考文献

- [1]《智能之门》，胡晓武等著，高等教育出版社
- [2] Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul), 2121-2159.
- [3] Zeiler, M. D. (2012). ADADELTA: an adaptive learning rate method. arXiv preprint arXiv:1212.5701.
- [4] Tieleman, T., & Hinton, G. (2012). Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2), 26-31.
- [5] Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
- [6] 周志华老师的西瓜书《机器学习》
- [7] Chawla N V, Bowyer K W, Hall L O, et al. SMOTE: synthetic minority over-sampling technique[J]. Journal of Artificial Intelligence Research, 2002, 16(1):321-357.
- [8] Inoue H. Data Augmentation by Pairing Samples for Images Classification[J]. 2018.
- [9] Zhang H, Cisse M, Dauphin Y N, et al. mixup: Beyond Empirical Risk Minimization[J]. 2017.
- [10] 《深度学习》- 伊恩·古德费洛
- [11] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun, Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. Link: https://arxiv.org/pdf/1506.01497v3.pdf
