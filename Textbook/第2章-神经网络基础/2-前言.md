<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 第2章 神经网络基础
  
## 简介

在本章中，我们将会简要叙述当前无论在学术界还是工业界都非常热门的深度学习的基础 —— 神经网络 —— 的基本知识。

根据读者的不同情况，可能会有以下三种情况：

- 熟悉神经网络的读者可以跳过本章的学习。
- 没有神经网络知识的读者，通过对以下内容的学习，可以了解神经网络的基本脉络。
- 想进一步深入了解更多、更细的知识，请参考《智能之门》或其他参考书。

后续章节的所有内容，都是以本章作为基础的，所以希望读者牢固掌握本章的知识。


## 内容概览

- [神经网络基本概念](2.1-神经网络基本概念.md)
- [神经网络的训练](2.2-神经网络的训练.md)
- [用神经网络解决回归问题](2.3-解决回归问题.md)
- [用神经网络解决分类问题](2.4-解决分类问题.md)
- [深度神经网络基础知识](2.5-深度神经网络.md)
- [梯度下降的优化算法](2.6-梯度下降的优化算法.md)
- [卷积神经网络基础知识](2.7-卷积神经网络.md)
- [循环神经网络基础知识](2.8-循环神经网络.md)


### 参考

- [1]《智能之门》，胡晓武等著，高等教育出版社
- [2] Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul), 2121-2159.
- [3] Zeiler, M. D. (2012). ADADELTA: an adaptive learning rate method. arXiv preprint arXiv:1212.5701.
- [4] Tieleman, T., & Hinton, G. (2012). Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2), 26-31.
- [5] Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
- [6] 周志华老师的西瓜书《机器学习》
- [7] Chawla N V, Bowyer K W, Hall L O, et al. SMOTE: synthetic minority over-sampling technique[J]. Journal of Artificial Intelligence Research, 2002, 16(1):321-357.
- [8] Inoue H. Data Augmentation by Pairing Samples for Images Classification[J]. 2018.
- [9] Zhang H, Cisse M, Dauphin Y N, et al. mixup: Beyond Empirical Risk Minimization[J]. 2017.
- [10] 《深度学习》- 伊恩·古德费洛
- [11] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun, Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. Link: https://arxiv.org/pdf/1506.01497v3.pdf
