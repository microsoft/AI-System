<!--Copyright Â© Microsoft Corporation. All rights reserved.
  é€‚ç”¨äº[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)ç‰ˆæƒè®¸å¯-->

  # 13.1 å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰

 ## å¼ºåŒ–å­¦ä¹ æ˜¯ä»€ä¹ˆï¼Ÿå¼ºåŒ–å­¦ä¹ æ˜¯æ€ä¹ˆå‘å±•çš„å‘¢ï¼Ÿ

å¼ºåŒ–å­¦ä¹ é€šè¿‡ä¸æ–­è¯•é”™å’Œå°è¯•çš„è¿›è¡Œå­¦ä¹ ï¼Œå¹¶ä»¥åšæŸä»¶äº‹å¸¦æ¥çš„å¥–åŠ±ä½œä¸ºæŒ‡å¯¼å…¶è¡Œä¸ºæ”¹å–„çš„åŸºç¡€è¿›è¡Œå­¦ä¹ ã€‚

å¼ºåŒ–å­¦ä¹ èµ·æºäº1954å¹´ï¼ŒMinskyé¦–æ¬¡æå‡ºâ€œå¼ºåŒ–â€å’Œâ€œå¼ºåŒ–å­¦ä¹ â€çš„æ¦‚å¿µå’Œæœ¯è¯­ã€‚
ä»æ­¤åçš„ä¸€äº›å¹´é‡Œï¼Œå¼ºåŒ–å­¦ä¹ é€æ¸è¢«å®Œå–„å’Œå‘å±•ã€‚
1965å¹´Waltzå’Œå‚…äº¬å­™æè¿°é€šè¿‡å¥–æƒ©çš„æ‰‹æ®µè¿›è¡Œå­¦ä¹ çš„åŸºæœ¬æ€æƒ³ã€‚ä»–ä»¬éƒ½æ˜ç¡®äº†â€œè¯•é”™â€æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒæœºåˆ¶ã€‚
1957å¹´ï¼ŒBellmanæå‡ºäº†æ±‚è§£æœ€ä¼˜æ§åˆ¶é—®é¢˜ä»¥åŠæœ€ä¼˜æ§åˆ¶é—®é¢˜çš„éšæœºç¦»æ•£ç‰ˆæœ¬é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMarkov Decision Processï¼ŒMDPï¼‰çš„åŠ¨æ€è§„åˆ’ï¼ˆDynamic Programmingï¼‰æ–¹æ³•ï¼Œè€Œè¯¥æ–¹æ³•çš„æ±‚è§£é‡‡ç”¨äº†ç±»ä¼¼å¼ºåŒ–å­¦ä¹ è¯•é”™è¿­ä»£æ±‚è§£çš„æœºåˆ¶ã€‚
1989å¹´ï¼ŒWatkinsæå‡ºçš„Qå­¦ä¹ ï¼ˆQ Learningï¼‰[3]ä½¿å¾—å¼ºåŒ–å­¦ä¹ ä¸å†ä¾èµ–äºé—®é¢˜æ¨¡å‹ã€‚è‡³ä»Šï¼ŒQå­¦ä¹ å·²ç»æˆä¸ºæœ€å¹¿æ³›ä½¿ç”¨çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚

æ­¤åä¸€æ®µæ—¶é—´ï¼Œå¼ºåŒ–å­¦ä¹ è¢«æœ‰ç›‘ç£å­¦ä¹ ï¼ˆsupervised learningï¼‰çš„å…‰èŠ’æ‰€é®æ©ï¼Œè¿™ç§å­¦ä¹ æ˜¯é€šè¿‡å¤–éƒ¨æœ‰çŸ¥è¯†çš„ç›‘ç£è€…æä¾›çš„ç›‘ç£ä¿¡å·ï¼ˆlabelï¼‰æ¥è¿›è¡Œå­¦ä¹ çš„ï¼Œä½†è¿™ç§å­¦ä¹ å·²ç»å®Œå…¨è¿èƒŒäº†å¼ºåŒ–å­¦ä¹ çš„å®—æ—¨ï¼Œå› ä¸ºç›‘ç£å­¦ä¹ æœ‰æ ‡å‡†çš„ç›‘ç£ä¿¡å·è€Œä¸éœ€è¦è¯•é”™ï¼Œè¿™ä¹Ÿæ˜¯å¼ºåŒ–å­¦ä¹ å‘å±•ç¼“æ…¢çš„åŸå› ã€‚

ç›´åˆ°ï¼Œåœ¨2013å¹´ï¼ŒDeepMindå‘è¡¨äº†åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç©Atariæ¸¸æˆçš„è®ºæ–‡[4]ï¼Œè‡³æ­¤å¼ºåŒ–å­¦ä¹ å¼€å§‹äº†æ–°çš„åå¹´ã€‚
2016å¹´3æœˆï¼Œé€šè¿‡è‡ªæˆ‘å¯¹å¼ˆæ•°ä»¥ä¸‡è®¡ç›˜è¿›è¡Œç»ƒä¹ å¼ºåŒ–ï¼Œç”±å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¾—åˆ°çš„AlphaGo[1]åœ¨ä¸€åœºäº”ç•ªæ£‹æ¯”èµ›ä¸­4:1å‡»è´¥é¡¶å°–èŒä¸šæ£‹æ‰‹æä¸–çŸ³ã€‚
åœ¨æ·±åº¦å­¦ä¹ å·²ç»å–å¾—äº†å¾ˆå¤§çš„è¿›æ­¥çš„åŸºç¡€ä¸Šï¼Œæ·±åº¦å¼ºåŒ–å­¦ä¹ çœŸæ­£çš„å‘å±•å½’åŠŸäºç¥ç»ç½‘ç»œã€æ·±åº¦å­¦ä¹ ä»¥åŠè®¡ç®—åŠ›çš„æå‡ã€‚çºµè§‚è¿‘å¹´çš„é¡¶çº§ä¼šè®®è®ºæ–‡ï¼Œå¼ºåŒ–å­¦ä¹ çš„ç†è®ºè¿›æ­¥ï¼Œåº”ç”¨é¢†åŸŸé€æ¸çˆ†å‘å¼å¢å¹¿ã€‚

 ## å¼ºåŒ–å­¦ä¹ ç”¨æ¥è§£å†³ä»€ä¹ˆé—®é¢˜å‘¢ï¼Ÿ

çœŸå®ä¸–ç•Œå¾ˆå¤šé—®é¢˜éœ€è¦åœ¨åŠ¨æ€å˜åŒ–çš„æƒ…å†µä¸‹åšå‡ºæ­£ç¡®çš„åºåˆ—å†³ç­–ï¼Œè€Œå¼ºåŒ–å­¦ä¹ éå¸¸æ“…é•¿è§£å†³è¿™ç±»é—®é¢˜ï¼Œå¹¶åœ¨ä»¥ä¸‹é¢†åŸŸæœ‰å¹¿æ³›çš„åº”ç”¨ï¼š

1ï¼‰æ¸¸æˆï¼ˆä¾‹å¦‚ï¼š å›´æ£‹[1], Atariå°æ¸¸æˆ[4], Dota2[5], éº»å°†[6], æ˜Ÿé™…äº‰éœ¸[17]ç­‰ï¼‰

2ï¼‰è‡ªåŠ¨é©¾é©¶ (ä¾‹å¦‚ï¼š[13], [14], [15]ç­‰ï¼‰

3ï¼‰è·¯å¾„è§„åˆ’ ï¼ˆä¾‹å¦‚ï¼š[16]ç­‰ï¼‰

4ï¼‰æ¨èç³»ç»Ÿ ï¼ˆä¾‹å¦‚: [8], [9], [10]ç­‰ï¼‰

5ï¼‰é‡‘èäº¤æ˜“ ï¼ˆä¾‹å¦‚ï¼š[7]ï¼‰

6ï¼‰æ§åˆ¶ ï¼ˆä¾‹å¦‚ï¼š[11], [12]ç­‰ï¼‰

ç­‰ç”šè‡³ç‰©ç§çš„è¿›åŒ–ï¼Œä¹Ÿå¯ä»¥çœ‹æˆæ˜¯ä¸€ä¸ªåœ¨å˜åŒ–çš„ç¯å¢ƒé‡Œæ¢ç´¢æ­£ç¡®ç”Ÿå­˜ä¹‹é“çš„é—®é¢˜ã€‚


## 13.1.1 å¼ºåŒ–å­¦ä¹ æ¦‚å¿µ

ä¸‹é¢ä»‹ç»ä¸€äº›å¼ºåŒ–å­¦ä¹ é‡Œçš„å…³é”®è¦ç´ ï¼šç¯å¢ƒï¼ˆenvironmentï¼‰, ä»£ç†ï¼ˆagentï¼‰, å¥–åŠ±ï¼ˆrewardï¼‰, åŠ¨ä½œï¼ˆaction) ,çŠ¶æ€ï¼ˆstate)å’Œç­–ç•¥ï¼ˆpolicyï¼‰ã€‚æœ‰äº†è¿™äº›è¦ç´ æˆ‘ä»¬å°±èƒ½å»ºç«‹ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¨¡å‹ã€‚
å¼ºåŒ–å­¦ä¹ éœ€è¦å»ç¯å¢ƒï¼ˆenvironmentï¼‰é‡Œæ¢ç´¢å’Œå­¦ä¹ ä¸€ä¸ªæœ€ä¼˜çš„ç­–ç•¥ï¼ˆpolicyï¼‰ï¼Œä½¿å¾—åœ¨è¯¥ç­–ç•¥ä¸‹è·å¾—çš„å¥–åŠ±ï¼ˆrewardï¼‰æœ€å¤§ï¼Œ ä»¥è§£å†³å½“å‰çš„é—®é¢˜ã€‚

<div align="center">
<img src="./img/concept_of_rl.png" ch="500" width=60%/>
</div>
<center>å›¾13.1.1. å¼ºåŒ–å­¦ä¹ é‡Œçš„åŸºæœ¬æ¦‚å¿µ </center>

ç”±äºçœŸå®çš„ç¯å¢ƒå¤æ‚åº¦è¾ƒé«˜ï¼Œä¸”å­˜åœ¨å¾ˆå¤šä¸å½“å‰é—®é¢˜æ— å…³çš„å†—ä½™ä¿¡æ¯ã€‚å¦‚å›¾13.1.1æ‰€ç¤ºï¼Œä¸ºäº†èƒ½å‡å°‘å†—ä½™å’ŒåŠ é€Ÿå­¦ä¹ ï¼Œé€šå¸¸æ„å»ºæ¨¡æ‹Ÿå™¨ï¼ˆsimulatorï¼‰æ¥æ¨¡æ‹ŸçœŸå®ç¯å¢ƒçš„ã€‚ä»£ç†ï¼ˆagentï¼‰é€šå¸¸æŒ‡åšå‡ºå†³ç­–çš„æ¨¡å‹ï¼Œå³å¼ºåŒ–å­¦ä¹ ç®—æ³•æœ¬èº«ã€‚ä»£ç†ä¼šæ ¹æ®å½“å‰è¿™ä¸€æ—¶åˆ»çš„çŠ¶æ€ $s_t$ æ‰§è¡ŒåŠ¨ä½œ $a_t$ æ¥å’Œç¯å¢ƒï¼ˆenvorimentï¼‰äº¤äº’ï¼›åŒæ—¶ï¼Œç¯å¢ƒä¼šå°†ä»£ç†æ‰§è¡ŒåŠ¨ä½œååˆ°è¾¾çš„ä¸‹ä¸€ä¸ªæ—¶åˆ»çš„çŠ¶æ€ $s_{t+1}$ å’Œè¿™ä¸ªåŠ¨ä½œæ‹¿åˆ°çš„å¥–åŠ± $r_t$ è¿”å›ç»™ä»£ç†ã€‚ä»£ç†å¯ä»¥æ”¶é›†ä¸€ä¸ªæ—¶é—´æ®µå†…çš„çŠ¶æ€ï¼ŒåŠ¨ä½œå’Œå¥–åŠ± $(ğ‘_1, ğ‘ _1, ğ‘Ÿ_1, . . . , ğ‘_ğ‘¡, ğ‘ _ğ‘¡, ğ‘Ÿ_ğ‘¡)$ ç­‰å†å²ä¿¡æ¯ï¼Œå¹¶å°†å…¶ç”¨æ¥è®­ç»ƒè‡ªèº«çš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹ã€‚

å¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡(goal)æ˜¯å¸Œæœ›èƒ½åœ¨ä¸€æ®µæ—¶é—´å†…è·å¾—çš„å¥–åŠ±æœ€å¤§:
$$G_{t}=\sum_{k=0}^{T} \gamma^{k} r_{t+k+1}$$
å…¶ä¸­, $\gamma \in[0,1)$, è¡¨ç¤ºä¸€ä¸ªæŠ˜æ‰£å› å­ã€‚ 
$\gamma$è¶Šå¤§ï¼Œè¡¨ç¤ºä»£ç†ï¼ˆagentï¼‰æ›´å…³å¿ƒé•¿æœŸå¥–åŠ±ï¼›è€Œ$\gamma$è¶Šå°ï¼Œè¡¨ç¤ºä»£ç†æ›´å…³å¿ƒçŸ­æœŸå¥–åŠ±ã€‚$T$è¡¨ç¤ºä¸€æ®µæ—¶é—´ã€‚

<div align="center">
<img src="./img/rl_snake.gif" ch="1000" width=60%/>
</div>
<center>å›¾13.1.2. è´ªåƒè›‡æ¸¸æˆ </center>

ä¸¾ä¾‹æ¥è¯´ï¼Œåœ¨è´ªåƒè›‡æ¸¸æˆé‡Œï¼Œä»£ç†å°±æ˜¯æŒ‡åƒè±†äººï¼Œå®ƒçš„ç›®æ ‡æ˜¯åƒæ‰ç½‘æ ¼ä¸­çš„é£Ÿç‰©ï¼ŒåŒæ—¶é¿å¼€é€”ä¸­çš„é¬¼é­‚ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç¯å¢ƒå°±æ˜¯ç½‘æ ¼ä¸–ç•Œï¼ŒçŠ¶æ€æ˜¯ä»£ç†åœ¨ç½‘æ ¼ä¸–ç•Œä¸­çš„ä½ç½®ï¼ŒåŠ¨ä½œæ˜¯ä¸Šä¸‹å·¦å³çš„æ“ä½œï¼Œå¥–åŠ±æ˜¯ä»£ç†åƒåˆ°äº†è±†å­ï¼Œé•¿æœŸå¥–åŠ±æ˜¯æŒ‡è¢«é¬¼é­‚åƒæ‰è€Œè¾“æ‰äº†æ¯”èµ›æˆ–è€…æœ€ç»ˆèµ¢å¾—äº†æ¯”èµ›ã€‚

ä¸ºäº†å»ºç«‹æœ€ä¼˜ç­–ç•¥ï¼Œä»£ç†é¢ä¸´ç€æ¢ç´¢æ–°çŠ¶æ€åŒæ—¶æœ€å¤§åŒ–å…¶æ•´ä½“å¥–åŠ±çš„ä¸¤éš¾å¢ƒåœ°ã€‚è¿™å°±æ˜¯æ‰€è°“çš„æ¢ç´¢ï¼ˆexplorationï¼‰ä¸åˆ©ç”¨ï¼ˆexploitï¼‰çš„å¹³è¡¡ã€‚ä¸ºäº†å¹³è¡¡ä¸¤è€…ï¼Œæœ€å¥½çš„ç­–ç•¥å¯èƒ½ç‰ºç‰²çŸ­æœŸçš„å¥–åŠ±ã€‚å› æ­¤ï¼Œä»£ç†åº”è¯¥æ”¶é›†è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œä»¥ä¾¿åœ¨æœªæ¥åšå‡ºæœ€ä½³çš„æ•´ä½“å†³ç­–ã€‚

å¼ºåŒ–å­¦ä¹ æ¨¡å‹åŒ…æ‹¬ç­–ç•¥ï¼ˆpolicyï¼‰å’Œä»·å€¼å‡½æ•°ï¼ˆvalue functionï¼‰ã€‚ä»£ç†éœ€è¦æ ¹æ®ç­–ç•¥å»å†³å®šåšå‡ºä»€ä¹ˆæ ·çš„åŠ¨ä½œã€‚ç­–ç•¥å¯ä»¥åˆ†ä¸ºï¼š

1ï¼‰å†³å®šæ€§çš„ç­–ç•¥ï¼ˆDeterministic policyï¼‰
$$\pi (s)=a$$
2ï¼‰éå†³å®šæ€§çš„ç­–ç•¥
$$\pi (a|s)=p(a_t=a|s_t=s)$$

ä»·å€¼å‡½æ•°æŒ‡çš„æ˜¯åœ¨ç­–ç•¥$\pi$ä¸‹èƒ½å¾—åˆ°çš„æœªæ¥çš„å¥–åŠ±çš„åŠ æƒæœŸæœ›å€¼ï¼š
$$V^{\pi}\left(s_{t}=s\right)=\mathbb{E}_{\pi}\left[r_{t}+\gamma r_{t+1}+\gamma^{2} r_{t+2}+\cdots \mid s_{t}=s\right]$$
å…¶ä¸­$\gamma$æ˜¯æŒ‡è¡°å‡å› å­ã€‚

é™¤æ­¤ä¹‹å¤–ï¼Œå¼ºåŒ–å­¦ä¹ é‡Œè¿˜æ¶‰åŠå¾ˆå¤šå…¶ä»–çš„æ¦‚å¿µï¼Œä½†æ˜¯åœ¨è¿™é‡Œä¸ä¸€ä¸€è®¨è®ºï¼Œæœ¬ç« åªä»‹ç»ä¸€äº›åé¢ç« èŠ‚å¯èƒ½è®¾è®¡åˆ°çš„ç®€å•çš„æ¦‚å¿µã€‚

## å‚è€ƒæ–‡çŒ®
- [1] Silver D, Huang A, Maddison C J, et al. Mastering the game of Go with deep neural networks and tree search[J]. nature, 2016, 529(7587): 484-489.
- [2] Pham H, Guan M, Zoph B, et al. Efficient neural architecture search via parameters sharing[C]//International Conference on Machine Learning. PMLR, 2018: 4095-4104.
- [3] Watkins C J C H, Dayan P. Q-learning[J]. Machine learning, 1992, 8(3): 279-292.
- [4] Mnih V, Kavukcuoglu K, Silver D, et al. Playing atari with deep reinforcement learning[J]. arXiv preprint arXiv:1312.5602, 2013.
- [5] Berner C, Brockman G, Chan B, et al. Dota 2 with large scale deep reinforcement learning[J]. arXiv preprint arXiv:1912.06680, 2019.
- [6] Li J, Koyamada S, Ye Q, et al. Suphx: Mastering mahjong with deep reinforcement learning[J]. arXiv preprint arXiv:2003.13590, 2020.
- [7] Liu X Y, Yang H, Chen Q, et al. Finrl: A deep reinforcement learning library for automated stock trading in quantitative finance[J]. arXiv preprint arXiv:2011.09607, 2020.
- [8] Zheng G, Zhang F, Zheng Z, et al. DRN: A deep reinforcement learning framework for news recommendation[C]//Proceedings of the 2018 World Wide Web Conference. 2018: 167-176.
- [9] Chen S Y, Yu Y, Da Q, et al. Stabilizing reinforcement learning in dynamic environment with application to online recommendation[C]//Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2018: 1187-1196.
- [10] Wang X, Chen Y, Yang J, et al. A reinforcement learning framework for explainable recommendation[C]//2018 IEEE international conference on data mining (ICDM). IEEE, 2018: 587-596.
- [11] Johannink T, Bahl S, Nair A, et al. Residual reinforcement learning for robot control[C]//2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019: 6023-6029.
- [12] Kober J, Bagnell J A, Peters J. Reinforcement learning in robotics: A survey[J]. The International Journal of Robotics Research, 2013, 32(11): 1238-1274.
- [13] Sallab A E L, Abdou M, Perot E, et al. Deep reinforcement learning framework for autonomous driving[J]. Electronic Imaging, 2017, 2017(19): 70-76.
- [14] Wang S, Jia D, Weng X. Deep reinforcement learning for autonomous driving[J]. arXiv preprint arXiv:1811.11329, 2018.
- [15] Kiran B R, Sobh I, Talpaert V, et al. Deep reinforcement learning for autonomous driving: A survey[J]. IEEE Transactions on Intelligent Transportation Systems, 2021.
- [16] Zhang B, Mao Z, Liu W, et al. Geometric reinforcement learning for path planning of UAVs[J]. Journal of Intelligent & Robotic Systems, 2015, 77(2): 391-409.
- [17] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev and others, "Grandmaster level in StarCraft II using multi-agent reinforcement learning," Nature, vol. 575, p. 350â€“354, 2019.